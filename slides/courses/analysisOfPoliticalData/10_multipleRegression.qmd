---
title: "Multiple Regression"
subtitle: "Large N & Leeuwenhoek (70700173)"

author: "Yue Hu"
institute: "Tsinghua University" 

knitr: 
    opts_chunk: 
      echo: false

format: 
  revealjs:
    css: style_basic.css
    theme: goldenBlack.scss
    # logo: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_zzxx.png?inline=true
    slide-number: true
    incremental: true
    preview-links: false # open an iframe for a link
    link-external-newwindow: true
    self-contained: false
    chalkboard: true # allwoing chalk board B, notes canvas C
    callout-icon: false
    
    show-slide-number: all # `speaker` only print in pdf, `all` shows all the time
    title-slide-attributes:
      data-background-image: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_THPS.png?inline=true
      data-background-size: 250px   
      data-background-position: top 10% right 5%
---

```{r setup, include = FALSE}
if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  tidyverse,
  patchwork,
  drhutools
) 


# Functions preload
set.seed(114)

theme_set(
  theme_minimal(base_size = 18)
)

theme_update(
  plot.title = element_text(size = 18), 
  axis.title = element_text(size = 22), 
  axis.text = element_text(size = 18)
)

```

## Overview

+ Fitting Multiple regression
    + Goodness of fit: R<sup>2</sup>
+ Post-estimation inferences


# Fitting Multiple regression

![](images/mr_fit.gif){.r-stretch fig-align="center"}

## Expression

:::{style="text-align:center"}
Multivariate regression/analysis or multiple regression? 
:::

:::{.fragment}
Population Regression Function (PRF): $$Y_i = \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + \epsilon_i.$$
:::


:::{.fragment style="text-align:center"}
According to CLRM: $E[\epsilon_i|\boldsymbol{X}] = 0.$
:::


:::{.fragment}
Sample Regression Function (SRF): $$Y_i = \hat\beta_0 + \hat\beta_1X_{1i} + \hat\beta_2X_{2i} + \hat \epsilon_i$$
:::


## Meaning of &beta;<sub>1</sub>

:::: {.columns}

::: {.column .fragment width="20%"}
To [BLUE]{.blue},

$min_{\hat\beta_0,\hat\beta_1,\hat\beta_2}\sum \hat \epsilon_i^2$

\begin{align}
\Rightarrow\frac{\partial \sum \hat \epsilon_i^2}{\partial\hat\beta_0}\to& 0\\
\frac{\partial \sum \hat \epsilon_i^2}{\partial\hat\beta_1}\to& 0\\
\frac{\partial \sum \hat \epsilon_i^2}{\partial\hat\beta_2}\to& 0
\end{align}
:::

::: {.column .fragment width="80%"}
When [BLUE]{.blue}:

\begin{align}
\hat\beta_0 =& \bar Y - (\bar\beta_1X_{1i} + \bar\beta_2X_{2i}),\\
\hat\beta_1 =& \frac{[\sum(Y_i - \bar Y)(X_{1i} - \bar X_1)][\sum(X_{2i} - \bar X_2)^2-\sum(X_{1i} - \bar X_1)(X_{2i} - \bar X_2)]}{\sum(X_{1i} - \bar X_1)^2\sum(X_{2i} - \bar X_2)^2 - [\sum(X_{1i} - \bar X_1)(X_{2i} - \bar X_2)]^2}\\
            =& \frac{\sum\hat r_{1i}(Y_i - \bar Y)}{\hat r_{1i}^2},
\end{align}

where $\hat r_{1i}$ are the errors from the regression of $X_{1i}$ on $X_{2i}$ (i.e., $X_{1i} = \hat\delta_0 + \hat\delta_1X_{2i} + \hat r_{1i}$), **the proportion that $X_2$cannot explain.**

$\hat\sigma^2 = \frac{\sum\hat \epsilon_i^2}{n - 3}.$
:::

::::

:::{.fragment}
:::{.callout-important}
## Interpretation

Every unit change in X<sub>1</sub> leads to &beta;<sub>1</sub> changes in Y [on average]{.red}, [holding everything else constant]{.red}.
:::
:::


- Change the same amount?
- Constant how?


:::{.notes}
On average
hold on mean or median
:::


# Goodness of Fit
## R<sup>2</sup>: Multiple Coefficient of Determination

:::: {.columns}

::: {.column width="30%"}
$$R^2 = \frac{\sum(\hat{Y} - \bar Y)^2}{\sum(Y - \bar Y)^2}$$

SST: Sum of Squares Total

SSR: Sum of Squares Regression

SSE: Sum of Squares Error.
:::

::: {.column .fragment width="70%"}
From the SRF, 

\begin{align}
Y_i =& \hat Y_i + \hat \epsilon_i\\
Y_i - \bar Y =& \hat Y_i  - \bar Y + \hat \epsilon_i\\
\Rightarrow (Y_i - \bar Y)^2 =& (\hat Y_i  - \bar Y + \hat \epsilon_i)^2\\
                         =& (\hat Y_i  - \bar Y)^2 + \hat \epsilon_i^2 + 2\hat\epsilon_i(\hat Y_i  - \bar Y)\\
\text{Sum up, } \Rightarrow \sum(Y_i - \bar Y)^2 =& \sum(\hat Y_i  - \bar Y)^2 + \sum\hat\epsilon_i^2\\
SST =& SSR + SSE\\
1 =& \frac{SSR}{SST} + \frac{SSE}{SST}\\
\text{In which, } R^2 =& \frac{SSR}{SST} = \frac{\sum(\hat Y_i  - \bar Y)^2}{SST}\\
                      =& \frac{\sum[\hat\beta_0 + (\hat\beta_1X_1 +\cdots +\hat\beta_nX_n)  - \bar Y]^2}{SST}
\end{align}

:::

::::

## My Attitude towards R<sup>2</sup>

![](images/mr_bad4ya.jpg){.r-stretch fig-align="center"}


## Why Is R<sup>2</sup> Bad? Very [low]{.red} for a correct model

$$R^2 = \frac{SSR}{SST} = \frac{\sum(\hat Y_i  - \bar Y)^2}{SST} = \frac{\sum[(Y_i - \epsilon_i) - \bar Y]^2}{SST}.$$

When the residual (&sigma;, estimated by &epsilon<sub>i</sub> in sample estimations) is large enough, R<sup>2</sup> could approach a very low score towards zero.


```{r rsmall, fig.align='center'}
r2 <- function(sig){
  x <- seq(1,10,length.out = 1000)        # our predictor
  y <- 2 + 1.2*x + rnorm(1000,0,sd = sig) # our response; a function of x plus some random noise
  summary(lm(y ~ x))$r.squared           # print the R-squared value
}

df_r <- tibble(sigma = seq(0.5,20,length.out = 20), 
               rout = map_dbl(sigma, r2))

ggplot(data = df_r, aes(x = sigma, y = rout)) +
  geom_point() + 
  ylab(expression(R^2)) + 
  xlab(expression(sigma)) +
  theme(
  plot.title = element_text(size = 28),
  axis.text=element_text(size=20),
  axis.title=element_text(size=20,face="bold")
)
```

## Why Is R<sup>2</sup> Bad? Very [high]{.red} for a misspecified model

```{r rlarge, fig.align='center'}
df_rLarge <-
    tibble(x = rexp(50, rate = 0.005),
           # from an exponential distribution
           y = (x - 1) ^ 2 * runif(50, min = 0.8, max = 1.2))# non-linear data generation

rout <-
    summary(lm(y ~ x, data = df_rLarge))$r.squared %>% round(digits = 2)

ggplot(data = df_rLarge, aes(x, y)) +
    geom_point() +
    xlab("X") +
    ylab("Y") +
    labs(
        title = expression("True model:" ~ Y == (X - 1) ^ 2),
        subtitle = expression("Estimated model:" ~ Y == beta[0] + beta[1] *
                                  X + sigma),
        caption = bquote(R ^ 2 == ~ .(rout))
    ) +
    theme(
        plot.title = element_text(size = 28),
        plot.subtitle = element_text(size = 20),
        plot.caption = element_text(size = 20),
        axis.text = element_text(size = 20),
        axis.title = element_text(size = 20, face = "bold")
    )
```

## Why Is R<sup>2</sup> Bad? Very [high]{.red} for a redundant model

\begin{align}
R^2 =& \frac{SSR}{SST} = \frac{\sum(\hat Y_i - \bar Y)^2}{SST}\\
    =& \frac{\sum[\hat\beta_0 + (\hat\beta_1X_1 +\cdots +\hat\beta_nX_n) - \bar Y]^2}{SST}
\end{align}

Therefore, the more Xs are added, the larger SSR (and thus R<sup>2</sup>) is, a.k.a., the "trash-can" model.

:::{.notes}
$\hat Y_i  - \bar Y = 0$
:::

:::{.fragment}
**Revision**

$$\text{Adj.} R^2 = 1 - (1 - R^2)\frac{n - 1}{n - k - 1}.$$
:::


:::: {.columns}

::: {.column width="50%"}
### Issue adjusted

X booming
:::

::: {.column width="50%"}
### Issues not adjusted

* Goodness of fit;
* Predictive error;
* Model comparison;
* X's explanatory power.
:::

::::

[*When can R<sup>2</sup> be useful then?*]{.fragment .r-fit-text}

:::{.notes}
Model comparison
:::


# Post-Estimation Inferences

## Predicted Value 


### Goal

1. Forecast
1. Interpretation: 
    + How the model is close to the reality?
    + What substantive changes can Xs make?

:::{.fragment}
### Approach

1. Expected value (average) of $\hat Y$
1. A one-time draw of $\hat Y$
1. Hypothesis testing
:::


## Forecasting

### Expected Value

:::: {.columns}
::: {.column width="50%"}

Let X<sub>0</sub> be the values at which we want to predict, the the expected value of Y is:

\begin{align}
E(\hat Y_0|X_0) =& E(\hat Y_0|X = X_0) = \boldsymbol{X_0\beta}\\
var(\hat Y_0|X_0) =& var(\hat\beta_0) + var(\hat\beta_1)X_0^2 \\
                   &+ 2cov(\hat\beta_0, \hat\beta_1)X_0\\
                  =& \sigma^2[\frac{1}{n} + \frac{(X_0 - \bar X)^2}{\sum(X_i - \bar X)^2}].
\end{align}
:::

::: {.column width="50%"}
```{r predicted}
ggplot(mtcars, aes(x = mpg, y = wt)) +
    geom_smooth(method = lm, se = TRUE) +
    ylab(bquote(hat(Y[0]))) +
    xlab(bquote(hat(X[0]))) +
    labs(caption = "Why is the ribbon wider at the two ends?") +
    theme(
        plot.title = element_text(size = 28),
        axis.text = element_text(size = 20),
        axis.title = element_text(size = 20, face = "bold")
    )
```
:::

::::

:::{.notes}
Fewer observations at two terminals, thus wider rainbow.
:::

:::{.fragment}
### Single-Point Forecast

\begin{align}
\hat Y_0 =& \hat\beta_0 + \hat\beta_1X_0 + \hat u\\
var(\hat Y_0|X_0) =& \sigma^2[1 + \frac{1}{n} + \frac{(X_0 - \bar X)^2}{\sum(X_i - \bar X)^2}].
\end{align}
:::

:::{.fragment}
There is an error term to account for.

In other words, single prediction is [more uncertain]{.red} than the average prediction.
:::



## Hypothesis Testing

Let's set &alpha; = 0.05.

:::: {.columns}

::: {.column width="50%"}

**On the coefficient**

Hypothesis:

\begin{align}
H_0: \beta =& \beta^*;\\
H_1: \beta \neq& \beta^*.
\end{align}


Statistics:

$$\frac{\hat\beta - \beta^*}{\sqrt{\frac{\hat\sigma^2}{\sum(X_i - \bar X)^2}}}\sim t_{n-k}.$$


Interpretation: 

For every unit change of X<sub>1</sub>, Y changes by &beta;<sub>1</sub>, [holding everything else constant]{.red}.
:::

::: {.column .fragment width="50%"}
**On the variance**

Hypothesis:

\begin{align}
H_0: \sigma =& \sigma^*;\\
H_1: \sigma \neq& \sigma^*.
\end{align}


Statistics:

$$(n - k)\frac{\hat\sigma^2}{\sigma^2}\sim\chi^2.$$
:::

::::


## Hypothesis Test for the Restricted Model

Let's set &alpha; = 0.05.

$$H_0: \beta_1 + 2\beta_2 = 3\Rightarrow \beta_1 = 3 - 2\beta_2; H_1: \beta_1 + 2\beta_2 \neq 3.$$

:::{.fragment}
Then, 

\begin{align}
Y =& \beta_0 + \beta_1X_1 + \beta_2X_2 + u, \text{(unrestricted)}\\
  =& \beta_0 + 3X_1 + \beta_2(X_2 - 2X_1) + u\\
\Leftrightarrow Y - 3X_1 =& \beta_0 + \beta_2(X_2 - 2X_1) + u\\
Y^* =& \beta_0' + \beta_2'Z + u, \text{(restricted)}
\end{align}
where $Y^*=Y - 3X_1; Z = X_2 - 2X_1.$

The test is thus transformed to $H_0: \beta_2' = \beta_2; \beta_0' = \beta_0$
:::


:::{.fragment}
Statistics:

$$\frac{\frac{SSR_R - SSR_U}{\Delta k}}{\frac{SSR_U}{n - k_U - 1}} = \frac{\frac{R_U^2 - R_R^2}{\Delta k}}{\frac{1 - R_U^2}{n - k_U - 1}}\sim F_{\Delta k, n - k - 1}$$
:::



:::{.fragment}
If the hypothesis is rejected, the unrestricted model is better.
:::

:::{.notes}
Hint: When testing general linear restrictions, that is, comparing the pair of nested models with different Ys, one should not use R<sup>2</sup> to deduct the F test, because $SST_U \neq SST_R$ in this case.
:::



## "When" to use OLS

:::{.r-stretch}

{{< video src="https://link.jscdn.cn/1drv/aHR0cHM6Ly8xZHJ2Lm1zL3UvcyFBcnR0dk83MHdLSU8xbUxzbHZTc01NeGF2Sm54P2U9MWViYjIx.mp4" >}}

:::

:::{.fragmnt style="text-align:center;}
Just kidding~~😂
:::



## <img src = "images/ci_fsmrof.png" height = 40> Latin Terms

- *a fortiori*: Not even saying.    
- *ad hoc*: To this, immediate purpose.   
- *ibid.*: "ibidem," in the same place.   
- [*ceteris paribus*]{.red}: with other conditions - remaining the same.    
- *c.f.*: "confer," compare   
- [*e.g.*]{.red}: "exempli gratia," for the sake of - example.    
- *etc.*: "et cetera," and the rest.    
- *i.e.*: "id est," that is.    
- [*n.b./NB*]{.red}: "nota bene," note well.    
- *per se*: By/of/for/in itself.    
- [*QED*]{.red}: $\blacksquare$ "quod erat demonstrandum," that which was to have been shown.    


- Bonus: Good.
- Vise versa: In a turned position.

## Take-home point

![](images/mr_mindmap.png){.r-stretch}


## Have a break

{{< video src="https://link.jscdn.cn/1drv/aHR0cHM6Ly8xZHJ2Lm1zL3YvcyFBcnR0dk83MHdLSU8xSDM3UzNHdnNiNExETzJUP2U9SmIxVGFw.mp4" >}}

