---
title: "Multicollinearity & Heteroskedasticity"
subtitle: "Large N & Leeuwenhoek (70700173)"

author: "Yue Hu"
institute: "Tsinghua University" 

knitr: 
    opts_chunk: 
      echo: false

format: 
  revealjs:
    css: style_basic.css
    theme: goldenBlack.scss
    # logo: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_zzxx.png?inline=true
    slide-number: true
    incremental: true
    preview-links: false # open an iframe for a link
    link-external-newwindow: true
    self-contained: false
    chalkboard: true # allwoing chalk board B, notes canvas C
    # callout-icon: false
    
    show-slide-number: all # `speaker` only print in pdf, `all` shows all the time
    title-slide-attributes:
      data-background-image: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_THPS.png?inline=true
      data-background-size: 250px   
      data-background-position: top 10% right 5%
---

```{r setup, include = FALSE}
if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  tidyverse,
  patchwork,
  drhutools,
  ggeffects,
  dotwhisker
) 


# Functions preload
set.seed(114)

theme_set(
  theme_minimal(base_size = 18)
)

theme_update(
  plot.title = element_text(size = 18), 
  axis.title = element_text(size = 22), 
  axis.text = element_text(size = 18)
)

```

## A Question Left from the Last Lecture

:::{.fragment style="text-align:center"}
Why should we use multiple regression? 
:::

:::{.fragment style="text-align:center"}
Why *controls*? 
:::


:::: {.columns}

::: {.column width="50%"}
![](images/mh_drStrange1.gif){.r-stretch .fragment}
![](images/mh_drStrange2.gif){.r-stretch .fragment}

:::{.notes}
We wanna understand things realistically and predict them.

Things happens all together, while we care what's the most important
:::

:::

::: {.column .fragment width="50%" style="margin-top: 5em"}
> One observes a set of covariates that is, after [statistical adjustment]{.blue}, sufficient to make treatment status [as-if random]{.red}.---Keele et al. 2020
:::

::::


## Statistical Adjustment

1. Linearity in the parameter; &larr; [&#9083;]{.green} [Specification](/slides_gh/slides/courses/analysisOfPoliticalData/11_specification.html), [Moderation](/slides_gh/slides/courses/analysisOfPoliticalData/14_moderation.html) & [GLM](/slides_gh/slides/courses/analysisOfPoliticalData/15_logitMissing.html)
1. Nonstochastic X ("given X," a.k.a., "X is fixed");  &larr; [&check;]{.gray} Data collection
1. X has positive noninfinite variance (var(X)); &larr; [&check;]{.gray} Data collection
1. Correct specification;  &larr; [&check;]{.green} [Specification](/slides_gh/slides/courses/analysisOfPoliticalData/11_specification.html)
1. Identification (N > K; K = 2 for a simple OLS);  &larr; [&check;]{.gray} Data collection & perfect collinearity [(Today)]{.golden}
1. Mean zero errors (E(u<sub>i</sub>|X<sub>i</sub>) = 0); &larr; [&#9083;]{.green} [Specification](/slides_gh/slides/courses/analysisOfPoliticalData/11_specification.html) (Omited variables, etc.) & [Missing](/slides_gh/slides/courses/analysisOfPoliticalData/15_logitMissing.html)
1. No covariance between X<sub>i</sub> and u<sub>i</sub>; &larr; [&cross;]{.red} [Endogeneity issue](/slides_gh/slides/courses/analysisOfPoliticalData/13_autoEndo.html)
1. No autocorrelation; &larr; [&cross;]{.red} [Autocorrelation issue](/slides_gh/slides/courses/analysisOfPoliticalData/13_autoEndo.html)
1. No perfect collinearity &larr; [&cross;]{.red} [(Today)]{.golden}
1. Homoskedasticity; &larr; [&cross;]{.red} [(Today)]{.golden}

## Quiz Time

Based on what you've known, what does the youtuber did right & wrong?

{{< video src="https://link.jscdn.cn/1drv/aHR0cHM6Ly8xZHJ2Lm1zL3YvcyFBcnR0dk83MHdLSU8xMWhrbmMyN1VtWnZicDlRP2U9ZzIxQjNt.mp4" >}}



## Overview

1. Collinearity
    + Violation
    + Diagnosis
    + Adjustment
1. Heteroscedasticity
    + Violation
    + Diagnosis
    + Adjustment


# Collinearity


## Perfect Collinearity

$$X_{1i} = \hat\delta_0 + \hat\delta_1X_{2i} + \hat r_{1i}$$

When cov(X<sub>1</sub>, X<sub>2</sub>) = 1, r<sub>1i</sub> = 0, then $\hat\beta_1 = \frac{\sum\hat r_{1i}y_i}{\hat r_{1i}^2}$ cannot be estimated.

:::: {.columns}

::: {.column .fragment width="50%"}

![](images/mh_zhenJiaMeiHouWang.jpeg){width=450}

:::

::: {.column .fragment width="50%"}
e.g., Let's say in the PRF Y<sub>i</sub> = &beta;<sub>0</sub> + &beta;<sub>1</sub>X<sub>1</sub> + &beta;<sub>2</sub>X<sub>2</sub> + u, X<sub>2</sub> = 1 + 2X<sub>1</sub>, then

\begin{align}
Y_i =& \hat\beta_0 + \hat\beta_1X_{1} + \hat\beta_2X_{2} + \hat u \\
    =& \hat\beta_0 + \hat\beta_1X_{1} + \hat\beta_2(1 + 2X_1) + \hat u \\
    =& (\hat\beta_0 + \hat\beta_2) + (\hat\beta_1 + 2\hat\beta_2)X_1 + \hat u\\
\Rightarrow Y_i =& \hat\alpha_0 + \hat\alpha_1X_{1} + \hat u
\end{align}

PRF (&beta;<sub>1</sub> or &beta;<sub>2</sub>) is non-identifiable. 

:::{.notes}
PRF: population regression function
:::

:::

::::

:::{.fragment .r-fit-text}
*Problematic, but [only]{.red} when perfect*
:::



## Multicollinearity

If **X**<sub>2</sub> is not a transformation of X<sub>1</sub>, the estimators (**&beta;**) remain [unbiased]{.red}, yet

:::{.fragment}
\begin{align}
var(\hat\beta_1|X) =&\frac{\sigma^2\sum(X_{2i} - \bar X_2)^2}{\sum(X_{1i} - \bar X_1)^2\sum(X_{2i} - \bar X_2)^2 - [\sum(X_{1i} - \bar X_1)(X_{2i} - \bar X_2)]^2}\\
=&\frac{\sigma^2}{\sum(X_{1i} - \bar X_1)^2(1 - \beta_{12}^2)}, \text{where }\beta_{12} = cov(X_1, X_2).
\end{align}
:::

:::{.fragment}
As &beta;<sub>12</sub> increases, the variance also increases (What does that mean?).
:::


---

## Diagnosis

**Variance Inflation Factors** (VIF, [1, +&infin;]): A measure of how much the variance of the estimated coefficient &beta;<sub>k</sub> is "inflated" by the correlation among the predictor variables.

$$VIF = \frac{1}{1 - {R}^{2}_{k}} = \frac{1}{Tolerance}$$


:::{.fragment}
1 means that no correlation among the k<sup>th</sup> predictor and the remaining predictor variables, and hence the variance of the coefficient is not inflated at all.
:::


:::{.fragment}
Rule of thumb: 4, 10
:::

:::{.fragment}
:::{.callout-tip}
In most cases, you don't need this, why?
:::
:::



:::{.notes}
Perfect then unidentified; but if your core X is insignificant.
:::


:::{.fragment}
### Adjustment

1. Removed the highly collinear variables.
1. Remeasure the collinear variables (e.g., EFA, CFA, IRT)
:::



# Heteroscedasticity

##  What's Heteroscedasticity

Homo/heteroscedasticity &asymp; Homo/heterogeneity (of variance)

:::{.notes}
The word “heteroscedasticity” comes from the Greek, and quite literally means data with a different (hetero) dispersion (skedasis).

Regression: Heteroscedasticity

ANOVA: Heterogeneity (of variance)
:::


:::{.fragment}
In CLR words, var(u<sub>i</sub>|X) = &sigma;<sup>2</sup>
:::

:::{.fragment}
Violation: $var(u_i|X) = \sigma_i^2, \sigma_i^2 \neq \sigma_j^2, \forall i, j.$
:::


![](images/mh_heteroscedasticity1.png){.r-stretch .fragment}

## Heteroscedasticikty in Distribution

![](images/mh_heteroscedasticity2.webp){.r-stretch}


## Consequence

:::: {.columns}

::: {.column width="50%"}
[Unbiased]{.red}, expected consistent, but inefficient

\begin{align}
var(\hat\beta|X) =& var(\frac{\sum(X_i - \bar X)(Y_i - \bar Y)}{\sum(X_i - \bar X)^2}|X),\\
                 =& var(\frac{\sum(X_i - \bar X)Y_i}{\sum(X_i - \bar X)^2}|X),\\
                 =& \frac{\sum(X_i - \bar X)^2}{[\sum(X_i - \bar X)^2]^2}var(Y_i|X),\\
                 =& \frac{\sum(X_i - \bar X)^2}{[\sum(X_i - \bar X)^2]^2}\sigma_i^2.
\end{align}
:::

::: {.column .fragment width="50%"}
\begin{align}
H_0: var(\hat\beta|X) =& \sum(X_i - \bar X)^2;\\
\Leftrightarrow\frac{\sum(X_i - \bar X)^2}{[\sum(X_i - \bar X)^2]^2}\sigma_i^2 =& \frac{\sigma^2}{\sum(X_i - \bar X)^2},\\
\sum(X_i - \bar X)^2\frac{\sigma_i^2}{\sigma^2} =& \sum(X_i - \bar X)^2
\end{align}

Only when $\frac{\sigma_i^2}{\sigma^2} = 1$, the estimate is efficient.
:::

::::



## Diagnosis

Goldfeld-Quandt test, Park test, Breusck-Pagan(-Godfreg) test, White test

:::{.fragment}
**Goldfeld-Quandt test**:

1. Divide the domain of X to three parts, $\frac{n -c}{2}, c, \frac{n -c}{2}$. 
1. Regress Y on X as usual for each part.
1. Calculate $\sigma_L^2$ from part 1 and $\sigma_H^2 = \frac{\sum^n_{i = \frac{n + c}{2} + 1}}{\frac{n - c}{2} - k}\hat u_i^2$ from part 3.
1. Then test $H_0: \sigma_L^2 = \sigma_H^2$
    + Statistics: $\frac{\sigma_L^2}{\sigma_H^2}\sim F_{\frac{n - c - 2k}{2}, \frac{n - c - 2k}{2}}$
:::


:::{.fragment}
Theoretically, the smaller the c is, the more power the test has. However, too small c may also cause the difference between low and high groups difficult to be identified.
:::

---

**Park test**

Ocular-inspection test: Use the scalar points of $\hat u_i^2$ against $X_i$, or $\hat u_i^2$ against $Y_i$

Regress $ln(\hat u_i^2)$ on some $X_{ki}$:

$$ln(\hat u_i) = \hat\delta_0 + \hat\delta_1ln(X_{ki}) + \hat\gamma_i.$$

Do the t-test of coefficient in $ln(X_i)$: $H_0: \hat\delta_1 = 0.$


:::{.notes}
Ocular: 望远镜

<img src="images/mh_parkTest.png" height = 200 />
:::

:::: {.columns}

::: {.column .fragment width="50%"}
**Breusck-Pagan(-Godfreg) test**

1. Assuming $\boldsymbol{\sigma_i^2} = \boldsymbol{X\delta} + \boldsymbol{v_i}$
1. Regress $\hat u_i^2$ on $X_1, X_2, \cdots, X_{k - 1}$, 
    - $H_0: \sigma_1 = \sigma_2 =\cdots=\sigma_{k - 1} = 0$
    - $H_1$: At least one of the &sigma;&ne;0
    - Statistics $F_{k - 1, n - k}$
:::

::: {.column .fragment width="50%"}
**White Test**

1. Regress Y on X to get $\hat u_i^2$
1. Regress $\hat u_i^2$ on $\sum X_i + \sum X_i^2 + \sum X_iX_j$ or $\hat Y_i$.
1. Test this model against a model with no variables (F test).
:::

::::



## Adjustment: White covariance matrix

(a.k.a., Heteroscedasitic-consistent covariance matrix).

$$E(u_i|X) = E\{\sqrt{[u_i - E(u_i)]^2}|X\}$$

:::{.fragment}
Since E(u<sub>i</sub>|X) = 0 by assumption, we can estimate $u_i$ with $\hat u_i^2$ and estimate $var(\hat\beta_1|X)$ with $\frac{\sum(X_i - \bar X)^2}{[\sum(X_i - \bar X)^2]^2}\hat u_i^2.$
:::


:::{.fragment}
The estimates turn out to be [biased]{.red}, but converge asymptotically in n to the true distribution. (What does this imply?) 
:::


:::{.notes}
Means it's fine when N is large enough.
:::

## Adjustment: Weighted least square

Reduce the substantive effect of the change in X by $\frac{1}{\sigma_i}$, in order to squeeze the value towards the middle: If &sigma;<sub>i</sub> is known, 

\begin{align}
\frac{Y_i}{\sigma_i} =& \frac{\beta_0}{\sigma_i} + \frac{\beta_1}{\sigma_i}X_i + \frac{u_i}{\sigma_i};\\
\text{Then, } var(\frac{u_i}{\sigma_i}) =& \frac{1}{\sigma_i^2}var(u_i) = \frac{\sigma_i^2}{\sigma_i^2} = 1;\\
\Rightarrow Y_i^* =& \beta_0X^*_{0i} + \beta_1X^*_{1i} + u_i^*. 
\end{align}

:::{.fragment}
The last equation is homoscedastistic. 
However, in most cases, we don't know &sigma;<sub>i</sub> &rArr; var(u<sub>i</sub>)&sim; X<sub>1i</sub>, i.e., var(u<sub>i</sub>) = &sigma;<sub>i</sub><sup>2</sup> = &sigma;<sup>2</sup>X<sub>i</sub> = h<sub>i</sub>&sigma;<sup>2</sup>.
:::


:::: {.columns}

::: {.column .fragment width="70%"}
Given the goal var(u<sup>\*</sup><sub>i</sub>) = var(u<sup>\*</sup><sub>j</sub>), &forall; i,j. 

\begin{align}
u^*_i =& \frac{u_i}{\sqrt{h_i}},\\
var(\frac{u_i}{\sqrt{h_i}}) =& \frac{var(u_i)}{h_i} = \frac{h_i\sigma^2}{h_i} = \sigma^2,\\
\Rightarrow Y_i^* =& \frac{Y_i}{\sqrt{h_i}}; X_{0i}^* = \frac{1}{\sqrt{h_i}}; X_{1i}^* = \frac{X_{1i}}{\sqrt{h_i}}, \text{ assuming } X_i\in R^+.
\end{align}
:::

::: {.column .fragment width="30%"}
:::{.callout-tip}
In practice, there are different ways to estimate the weight, one need to carefully choose the proper one.
:::
:::

::::


## Adjustment: Feasible Generalized Linear Squares (FGLS)

\begin{align}
\boldsymbol{Y} =& \boldsymbol{X\beta} + \boldsymbol{u}; \\
var(\boldsymbol{u}) =& \Omega_{n\times n} = \left(
       \begin{array}{cccc}
       \sigma_1^2 & 0 & \cdots & 0\\
       0 & \sigma_2^2 & \cdots & 0\\
       \vdots & \vdots & \vdots & \vdots \\
       0 & 0 & \cdots & \sigma_n^2\\
       \end{array}\right),\\
\text{Then, } \boldsymbol{\hat\beta_{GLS}} =& (\boldsymbol{X'\Omega X})^{-1}(\boldsymbol{X'\Omega Y}).\\
\text{Let }\boldsymbol{H}: \boldsymbol{\Omega} = \boldsymbol{HH^{-1}},\text{then, } \boldsymbol{H^{-1}Y} =& \boldsymbol{H^{-1}X\beta} + \boldsymbol{H^{-1}u},
\boldsymbol{H^{-1}u} = \boldsymbol{H^{-1}(H^{-1})'};\\
var(\boldsymbol{u})=& (\boldsymbol{HH'})^{-1}\boldsymbol{\Omega} = \boldsymbol{\Omega}^{-1}\boldsymbol{\Omega} = \boldsymbol{I},\\
var(\boldsymbol{\hat\beta_{GLS}}) =& (\boldsymbol{X'X})^{-1}(\boldsymbol{X'\Omega X})(\boldsymbol{X'X})^{-1}.
\end{align}

. . .

* If there's no heteroscedasticity, $\boldsymbol{\Omega} = \sigma\boldsymbol{I}.$
* If there is heteroscedasticity and $\boldsymbol{\Omega}$ is known, then WLS (a special type of GLS).
* If $\boldsymbol{\Omega}$ is unknown, run $\boldsymbol{Y} = \boldsymbol{X\beta} + \boldsymbol{u}$ to estimate $\boldsymbol{\Omega}$ with $\boldsymbol{\hat\Omega} = \boldsymbol{\hat u\hat u'}$

:::{.callout-warning}
NB: This method does not get SE, and also [biased].red for small N.
:::


## "Sandwich" Matrix

FGLS is a type of "sandwich" estimator.

In a more general view, let $\boldsymbol{Q} = \boldsymbol{X'X}$ and 

\begin{align}
\boldsymbol{Q} = \left(
\begin{array}{cccc}
\sigma_1^2 & 0 & \cdots & 0\\
0 & \sigma_2^2 & \cdots & 0\\
\vdots & \vdots & \vdots & \vdots \\
0 & 0 & \cdots & \sigma_n^2\\
\end{array}\right)
\end{align}

Then for regular OLS, $var(\beta) = \sigma^2(\boldsymbol{X'X})^{-1} = \sigma^2\boldsymbol{Q}^{-1}$.

But when heteroscedasticity occurs, $var(\boldsymbol{\beta}|X)\neq \sigma^2\boldsymbol{Q}^{-1}$.

Instead, let $\boldsymbol{G} = \boldsymbol{X'GX}$, then $var(\boldsymbol{\beta}|X) = \boldsymbol{Q^{-1}GQ}^{-1}.$


## Heterogeneity in TSCS Data

TSCS: Time-series cross-sectional data.
Sometimes, they are called "pooled data", "panel data"(wrong!)

Dealing with heterogeneity in TSCS data:

1. Robust standard error
1. Fixed effect
1. Multilevel modeling

---

## Robust Standard Error

**Sandwich SE**

In the FGLS version, the "meat" can be identified only when T > N, and before this, the autocorrelation ($cov(u_i, u_j|X_i, X_j) = 0, \forall i, j$) has to be eliminated.

. . .

**Panel-corrected SE**

An alternative version of the sandwich SE. &Omega; is clustered by time periods: $\hat\Omega_{ij} = \frac{\sum^T_{t = 1}\epsilon_{it}\epsilon_{jt}}{T}$

. . .

**Cluster SE**

Adjust SE to account for correlations within clusters.


:::{.fragment}
**Within-between model**

1. Run separate models in each group
1. Run an aggregate regression

:::

 
:::{.fragment}

Let $\bar Y_i = \frac{\sum^n_{i=1}Y_{it}}{n_i}, \bar X_i = \frac{\sum^n_{i=1}X_{it}}{n_i}, \bar u_i = \frac{\sum^n_{i=1}u_{it}}{n_i}$. Then,

\begin{align}
\bar{Y_i} =& \beta_0 + \beta_1\bar{X_i} + \epsilon_i + u_i\\
Y_{it} - \bar{Y_i} =& (\beta_0 - \beta_0) + \beta_1(X_{it} - \bar{X_i}) + (u_{it} - u_i)\\
\hat{Y_i} =& \beta_1\hat{X_{it}} + \hat{u_{it}}, \text{(a.k.a., the within model)}\\
Y_{it} =& \beta_0 + \beta_1(X_{it} - \bar{X_i}) + \beta_2\bar{X_i} + u_{it}, \text{(a.k.a., the between model)}
\end{align}
:::


---

## Fixed Effect

**Least Square with Dummy Variables** (LSDV)

Type I: 

$$Y_{it} = \beta_1X_{it} + \alpha_i + u_{it},$$ in which &alpha; is unit-specific mean differences (unit fixed effect). 

. . .

Type II: 

$$Y_it = \sum^T_{t = 1}\delta_tD_{t_i} + \beta_1X_{it} + u_{it},$$ in which $\delta_tD_{t_i}$ is the fixed effect for time.

. . .

### Issues of LSDV

1. Adding a lot of variables (risk of using out of the d.f.);
1. Additional high multicollinearity;
1. Losing time invariant variables;
1. Inefficient estimates of FE on binary and dependent variables.

:::{.notes}
For the last two concerns, considering using duration models.
:::

## Alternatives adjustments

1. Demeaning
1. Fixed effect vector decomposition (FEVD)

:::{.fragment}
Con : 

1. Can't offer sufficient information (heterogeneous to what extent?).
1. Difficult to calculate substantive effects, e.g., first differences.
:::

:::{.fragment style="text-align:center; margin-top: 2em"}
&dArr;

Multilevel modeling (MLM)
:::


## MLM

Heteroscedasticity can be theoretically meaningful, esp., when researchers focus on how the variance changes. 

:::{.notes}
e.g., Assuming var(u<sub>i</sub>) = &sigma;<sup>2</sup>e<sup>&gamma;<sub>1</sub>X<sub>2i</sub></sup>. 

If &gamma;<sub>1</sub> = 0, then the model is homoscedastistic; otherwise, one has to use MLE to test H<sub>0</sub>: &gamma;<sub>1</sub> = 0.
:::

:::{.fragment}
*Modeling variance*: 

Random Intercept (Two-Level)

\begin{align}
Y_{ij} = \beta_{0j}& + \beta_{1j}X_{ij} + \epsilon_{ij}, \epsilon_{ij}\sim N(0, \sigma^2)\\
       \beta_{0j}& = \gamma_{00} + \gamma_{01}Z_j + u_{0j}, u_{0j}\sim N(0, \tau^2)
\end{align}

Z is the group indicator.
:::

:::{.fragment}
**Intraclass correlation**: $\rho = \frac{\tau^2}{\sigma^2 + \tau^2}.$
:::

:::{.notes}
Represent the similarity between individual and group level. 0 means no group effect.
:::

:::: {.columns}

::: {.column .fragment width="60%"}
Random Slop:

\begin{align}
Y_{ij} = &\beta_{0j} + \beta_{1j}X_{ij} + \epsilon_{ij}\\
       &\beta_{0j} = \gamma_{00} + \gamma_{01}Z_j + u_{0j}\\
       &\beta_{1j} = \gamma_{10} + u_{1j}.\\
\text{Assume}\left(
\begin{array}{c}
u_{0j}\\ u_{1j}\end{array}\right)&\sim BVN\left[\left(\begin{array}{c}
0\\0\end{array}\right), \left(\begin{array}{cc}
\tau_0^2 & \tau_0\tau_1\\ 
\tau_0\tau_1 & \tau_1^2
\end{array}\right)\right]
\end{align}

:::{.notes}
BVN: Bivariate Normal

The two variables in a bivariate normal are both are normally distributed, and they have a normal distribution when both are added together. Visually, the bivariate normal distribution is a three-dimensional bell curve.
:::

:::

::: {.column .fragment width="40%"}

\begin{align}
\left(\begin{array}{cc}
\tau_0^2 & \tau_0\tau_1\\ 
\tau_0\tau_1 & \tau_1^2
\end{array}\right)
\end{align}

1. Identity: &tau;<sub>0</sub><sup>2</sup> = &tau;<sub>1</sub><sup>2</sup> = 1;
1. [Exchangeable: &tau;<sub>0</sub><sup>2</sup>= &tau;<sub>1</sub><sup>2</sup>;]{.navy}
1. Independent: &tau;<sub>0</sub><sup>2</sup>&tau;<sub></sub><sup>2</sup> = 0;
1. Unstructured: no specific relationships is assumed for the &tau;s.

:::{.notes}
Covariate Matrix 
:::

:::

::::

## Take-home point

![](images/mh_mindmap.png){.r-stretch}


## Have a break

{{< video src="https://link.jscdn.cn/1drv/aHR0cHM6Ly8xZHJ2Lm1zL3YvcyFBcnR0dk83MHdLSU8xSDM3UzNHdnNiNExETzJUP2U9SmIxVGFw.mp4" >}}



