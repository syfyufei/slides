---
title: "Probability Theory"
subtitle: "Large N & Leeuwenhoek (70700173)"

author: "Yue Hu"
institute: "Tsinghua University" 

knitr: 
    opts_chunk: 
      echo: true

format: 
  revealjs:
    css: style_basic.css
    theme: goldenBlack.scss
    # logo: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_zzxx.png?inline=true
    slide-number: true
    incremental: true
    preview-links: false # open an iframe for a link
    link-external-newwindow: true
    self-contained: false
    chalkboard: true # allwoing chalk board B, notes canvas C
    
    show-slide-number: all # `speaker` only print in pdf, `all` shows all the time
    width: 1600
    height: 900
    title-slide-attributes:
      data-background-image: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_THPS.png?inline=true
      data-background-size: 250px   
      data-background-position: top 10% right 5%
---


## Overview

* Event Probability
* Calculating probabilities
* Presenting Probability
* Distribution


# Understanding Probability


## Perspectives of Probability

- **Frequentist ("objective")**
    - "Coin has two sides, balance" &rArr; Pr(head) = 0.5.
- **Relative frequency**
    - You are observing a random sample.
    - $$\lim_{n\to \infty}\frac{f(x)}{n}$$
- **Evolutionary ("subjective")**: Bayesian, based on beliefs (prior), game theory



## How Do You Know the Probability

> Sample Space: Space of all possible outcomes

1. &sum; Pr = 1
1. All of the probabilities constitute the probability distribution
1. For all event in the space, $P(E) + P(\tilde E) = 1$


:::{.fragment}

### Methods to "find" probability in a sample space
1. Sample Point Method
1. Event Composition Method
1. Bayesian Method
:::


## Sample Point Method

"Count out"

*(Objective perspective)*

1. Define experiment event;
1. Define sample space;
1. Assign probabilities, P(E<sub>i</sub>) &GreaterEqual; 0, &sum; P(E<sub>i</sub>) = 1;
1. Define event of interest, A;
1. Find Pr(A) by summing probability of A.


## Replacements

W/o replacement means once being picked out, never put it back.

::::{.columns}
:::{.column width="50%" .fragment}
![](images/prob_drawBalls.jpg)
5 soccer balls, 5 basketballs (in the same size and surface).

The chance to get 5 soccer balls in five spilling out?

:::

:::{.column width="50%" .fragment}
- W. replacement: $0.5^5$; 
- W/o replacement: ${10 \choose 5}^{-1} = \frac{5!(10 - 5)!}{10!}.$ 
:::
::::

:::{.notes}
n! factorial

- w. independent
- w.o no repitation
:::

:::{.r-fit-text .fragment}
Q: Is cross-sectional survey sampling a w. or w.o. replacement process? 
:::



## Event Composition Method (ECM)

"Calculate"

*(Objective perspective)*

Applying [multiplicative and additive]{.red} rules.

::::{.columns}
:::{.column width="50%" style="text-align: right"}
- E<sub>1</sub>: m outcomes
- E<sub>2</sub>: n outcomes
:::

:::{.column .fragment width="50%"}
&rArr; $m\times n$ possible outcomes for E<sub>1</sub>E<sub>2</sub>. 
:::
::::

::::{.columns}
:::{.column width="50%" .fragment}
![](images/prob_toys.jpeg)
:::

:::{.column width="50%" .fragment}
Six heads, 15 bodys, 25 colors, how many distinctive types of toys?

$$6 \times 15 \times 25 = 2,250.$$
:::
::::


## Rules of ECM

::::{.columns}
:::{.column width="50%" .fragment}
Permutation: Ordered

$$P(n, r) = \frac{n!}{(n - r)!}.$$
:::

:::{.column width="50%" .fragment}
Combination: Order doesn't matter

$${n \choose r} = \frac{n!}{r!(n - r)!}.$$
:::
::::


:::{.fragment style="text-align: center"}

Event G and H

* Union: G&cup;H = $\{S_i: S_i\in G\ or\ S_i\in H\}$
    - P(G&cup;H) = P(G) + P(H) - P(G)&cap;P(H).
* Intersection: G&cap;H = $\{S_i: S_i\in G\ and\ S_i\in H\}$
* Exclusive: G&cap;H = &empty;
* Complement: J &sub; -G &cap; H
* Conditional: $P(H|G) = \frac{P(G\cap H)}{P(G)}$
* Independent: P(H|G) = P(H); P(G&cap;H) = P(G)P(H)
:::


## Baysian Method

"Figure out"

::::{.columns}
:::{.column width="50%" .fragment}
Bayes' Theorem

$$
\begin{aligned}
P(A|B)P(B) =& P(B|A)P(A) = P(A\cap B);\\
P(A|B)=& \frac{P(B|A)P(A)}{P(B)},\\
=& \frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|\tilde A)P(\tilde A)}
\end{aligned}
$$

:::

:::{.column width="50%" style="text-align: right"}
![](images/prob_Bayes.gif)

Thomas Bayes (1702--1761) 
:::
::::


:::{.notes}
1. Conjugate prior: posterior distribution = prior distribution
1. Cromwell's rule: if a region of the parameter space has 0 prior probability, then it also has 0 posterior probability. 
:::

---

![](images/prob_bayesComponents.png){height="900"}
 

:::{.notes}

![](images/prob_ar.gif)

A new season of Antiques Roadshow verifies antiques for buyers based on machine-learning techniques.
Based on experience of previous seasons, .red[**30%**] of the products were knockoff in the platform. 
When people bring a product to verify, AR can tell .red[**90%**] knockoff knockoff, and .red[**80%**] authentic products authentic. 
Then, what's the probability that a product is knockoff, if AR said so?

$$\begin{align}
& P(K|VK) \\
=& \frac{P(VK|K)P(K)}{P(VK|K)P(K) + P(VK|\tilde K)P(\tilde K)}, \\
=& \frac{.9 * .3}{.9 * .3 + .2*.7} = .659.
\end{align}$$


```{r diagramCar, echo = FALSE, fig.height=6}

library("DiagrammeR")

grViz("
digraph usedCar {

  # a 'graph' statement
  graph [overlap = true, fontsize = 10]

  # several 'node' statements
  node [shape = circle,
        fixedsize = true,
        width = 0.9] // sets as circles
  N[label = 'Nature'];
  MF[label = 'Verify'];
  MS[label = 'Verify'];
  RFF[label = 'Result'];
  RFS[label = 'Result'];
  RSF[label = 'Result'];
  RSS[label = 'Result']

  # several 'edge' statements
  N -> MF[label='K: .3']
  N -> MS[label='A: .7']
  MF -> RFF[label='K: .9']
  MF -> RFS[label='A: .1']
  MS -> RSF[label='K: .2']
  MS -> RSS[label='A: .8']
  
}
")

```
:::

## Bayesian probability of your roommate is a pervert

{{< video src="https://link.jscdn.cn/1drv/aHR0cHM6Ly8xZHJ2Lm1zL3YvcyFBcnR0dk83MHdLSU8xSHBMQjlxSVhKaEdFWnlmP2U9WFFvb2xn.mp4" height="850" >}}


## Mutural Diss

::::{.columns}
:::{.column width="50%" .fragment}
Bayesian {{< fa solid gun size=large >}} Frequentist

1. *Repeated sample*: When the population data are collected, the repeat-sample makes no sense.
1. *Future event*: It is impossible to have repeat samples for future.
:::

:::{.column width="50%" .fragment}
Frequentist {{< fa solid gun size=large >}} Bayesian
![](images/prob_freqBayesian.png){width=800}
:::
::::


# Presenting Probability

## Four Common Ways to Presenting Probability

1. Percentage
1. Relative risk
1. Odds Ratio
1. Bayesian method



## Relative Risk

* $\hat{\pi}_{1|1}/\hat{\pi}_{1|2} = \frac{n_{11}/n_{\bullet 1}}{n_{12}/n_{\bullet 2}}$

:::{.fragment}
E.g., A hospital investigated whether drinking has a relation with fatal illness. 

| Drinking 	| Yes 	| No 	|
|----------	|-------	|----------	|
| Fatal      	| 1,601  	| 510   	|
| Nonfatal       	| 162,527   	| 412,368   	|

$$Risk_{fatal} = \frac{1601/(1601 + 162527)}{510/(510 + 412368)} = 7.897$$

- That is, when people drink, they are **7.9 more times** to die in a fatal accident than those w/o.
:::

## Odds Ratio

::::{.columns}
:::{.column width="50%" .fragment}
\begin{align}
O_{12} =& \frac{\pi_{1|1}/\pi_{1|2}}{\pi_{2|1}/\pi_{2|2}}, \\
 =& \frac{n_{11}/n_{12}}{n_{21}/n_{22}}, \\
 =& n_{11}n_{22}/n_{21}n_{12}.
\end{align}
:::

:::{.column width="50%" .fragment}
The previous e.g., 

| Drinking 	| Yes 	| No 	|
|----------	|-------	|----------	|
| Fatal      	| 1,601  	| 510   	|
| Nonfatal       	| 162,527   	| 412,368   	|

$O_{fatal} = \frac{1601/510}{162527/412368}$ = 7.965
:::
::::


- The fatal risk of wearing a belt is **8 times larger** than not
- *Log odds*: mapping the range (0,1) to (-&infin;, +&infin;) using log of odds, a.k.a., "logistic unit" (logit)
    - If independent, then log odds = 0.


## RR vs. OR

:::{style="text-align: center"}
![](images/prob_rrOr.png){height="500"}
:::

* For common cases, RR is better.
* For rare cases, both ok.


# Probability &rarr; Distribution


## Puting Probability Together

(Probability) distribution: 

The mathematical [function]{.red} that gives the probabilities of occurrence of different possible [outcomes]{.red} for an [experiment]{.red}.


::::{.columns}
:::{.column width="50%" .fragment}
**Marginal and joint distributions**

- Marginal \begin{align}
\pi_{i\bullet} =& \sum_j \pi_{ij}, \hat{\pi_{i\bullet}} =  n_{i\bullet}/n_{\bullet\bullet}; \\
\pi_{\bullet j} =& \sum_i \pi_{ij}, \hat{\pi_{\bullet j}} =  n_{\bullet j}/n_{\bullet\bullet}.
\end{align}

- Joint: $\pi_{\bullet\bullet}$ = &sum;&sum; &pi;<sub>ij</sub> = 1.
:::

:::{.column width="50%" .fragment}
**Marginal and joint totals**

\begin{align} 
n_{i\bullet} =& \sum_j n_{ij}; n_{\bullet j} = \sum_i n_{ij}; \\
n_{\bullet\bullet} =& \sum\sum n_{ij}.
\end{align}

**Conditional distribution**

\begin{align} 
\pi_{i|j} = \pi_{ij} / \pi_{\bullet j}; \\
\hat{\pi_{i|j}} = n_{ij}/n_{\bullet j}.
\end{align}
:::
::::


---

E.g., Do you like ”Love Between Fairy and Devil"? 

| Gender 	| Yes 	| No 	| Total 	|
|--------	|-----	|----	|-------	|
| Male   	| 5   	| 12 	| 17    	|
| Female 	| 6   	| 5  	| 11    	|
| Total  	| 11  	| 17 	| 28    	|

- What's the probability for a female that don't like?
    - $Pr(FN) = 5/28.$ (event probability)
- What's the probability for finding a disliker? 
    - $Pr(N) = 17/28.$ (marginal distribution)
- What's the probability of disliking given the respondent is a female?
    - $P(N|F) = 5/11.$ (conditional distribution)
    
    
:::{.notes}
苍兰诀
:::


## Independence

$$P(X\cap Y) = P(X)P(Y); P(Y|X) = P(X).$$
::::{.columns}
:::{.column width="40%" .fragment}
Such event's probability is: 

- Population: $\pi_i = n_i/N$;
- Sample: $\hat{\pi_i} = n_i/n_{\bullet\bullet}.$
:::

:::{.column width="60%" .fragment}
E.g., Do you like ”Love Between Fairy and Devil"? 

| Gender 	| Yes 	| No 	| Total 	|
|--------	|-----	|----	|-------	|
| Male   	| 5   	| 12 	| 17    	|
| Female 	| 6   	| 5  	| 11    	|
| Total  	| 11  	| 17 	| 28    	|

- Are gender and approval independent? 
    - Pr(MY) = [11/28 &times; 17/28]{.blue} &ne; 5/28
:::
::::

:::{.notes}
11/28 &times; 17/28: [Expected value for independence]{.red}

$\chi^2 = \sum(\frac{Observation - Expected}{Expected})^2.$
:::


## Take-home point

::: {style="text-align: center"}
![](images/prob_mindmap.png){ height="850" }
:::