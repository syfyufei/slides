---
title: "Simple OLS and Properties of Estimators"
subtitle: "Large N & Leeuwenhoek (70700173)"

author: "Yue Hu"
institute: "Tsinghua University" 

knitr: 
    opts_chunk: 
      echo: false

format: 
  revealjs:
    css: style_basic.css
    theme: goldenBlack.scss
    # logo: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_zzxx.png?inline=true
    slide-number: true
    incremental: true
    preview-links: false # open an iframe for a link
    link-external-newwindow: true
    self-contained: false
    chalkboard: true # allwoing chalk board B, notes canvas C
    
    show-slide-number: all # `speaker` only print in pdf, `all` shows all the time
    width: 1600
    height: 900
    title-slide-attributes:
      data-background-image: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_THPS.png?inline=true
      data-background-size: 250px   
      data-background-position: top 10% right 5%
---


```{r setup, include = FALSE}


if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  tidyverse,
  patchwork,
  drhutools,
  greekLetters,
  XICOR
) 


# Functions preload
set.seed(114)

theme_set(
  theme_minimal(base_size = 18)
)

theme_update(
  plot.title = element_text(size = 18), 
  axis.title = element_text(size = 22), 
  axis.text = element_text(size = 18)
)

```

## Overview


1. Road to OLS
1. OLS Principles
    + Goal
    + Model
    + Uncertainty
1. OLS Components
    + Coefficients
    + Variances/SEs


## Where are We

Goal: Understanding data patterns in terms of [random variable]{.red}

:::: {.columns}

::: {.column width="50%"}
*Path through*

Description

- Single: Moments
- Relation: Correlation

Distribution

- Probability theory
- PDF/CDF
    - Confidence intervals/p-values

Hypothesis testing

- Significance

:::

::: {.column width="50%" style="text-align:center"}

:::{.fragment}
*Path forward* [&star;]{.golden}

Linear (OLS)

- Simple linear regression
- Multiple regression
:::

:::{.fragment}
&darr;

Generalized linear

- MLE(Binary/Ordered/Nominal)
:::


:::{.fragment}
&darr;

Nonlinear

- Marginal effects & Moderation
:::

:::

::::



## Review: Expectation

:::: {.columns}

::: {.column width="50%"}
\begin{align}
E(X)_{discrete} =& \sum^n_{i = 1} X_if(X_i).\\
E(X)_{continuous} =& \int^{+\infty}_{-\infty}X_if(X_i)dX,\\
\sigma^2 =& E[X - E(X)]^2, \\
=& E[(X - \mu)^2],\\
=& E(X^2 - 2X\mu + \mu^2),\\
=& E(X^2) - 2\mu E(X) + \mu^2,\\
=& E(X^2) - \mu^2.
\end{align}
:::

::: {.column .fragment width="50%"}

\begin{align}
cov(X, Y) =& E[X - E(X)][Y - E(Y)],\\
          =& E[XY - X\cdot E(Y) -\\
          &Y\cdot E(X) + E(X)E(Y)],\\
          =& E(XY) - E(Y)E(X) -\\
          &E(X)E(Y) + E(X)E(Y),\\
          =& E(XY) - E(X)E(Y).
\end{align}

:::

::::



## Review (Continued): Variance

:::{.fragment}
\begin{align}
var(aX + b) =& E[(aX + b) - E(aX + b)]^2 = E[aX - aE(X)]^2, \\
=& a^2E[X - E(X)]^2 = a^2var(X).
\end{align}
:::

:::{.fragment}
$$var(aX_1 + bX_2+ c) = a^2var(X_1) + b^2var(X_2) + 2ab\cdot cov(X_1, X_2).$$
:::

:::{.fragment}
When X<sub>1</sub> is i.i.d., 
$$var(a_1X_1 + a_2X_2 + ... + a_nXn) = var(\sum a_iX_i) = \sum a_i^2var(X_i).$$
:::


:::{.fragment}
$$var(\bar X) = var(\frac{\sum X_i}{n}) = \sum^n_{i = 1}\frac{var(X_i)}{n^2} = \frac{n\sigma^2}{n^2} = \frac{\sigma^2}{n}$$
:::

## Review (Continued): Property of Estimator $\hat \theta$

* Unbiased: On average, the estimator gives the right answer, formally, $E(\hat\sigma) = \sigma.$
* Consistent: As the sample size increases, the variance decreases.
* Efficiency: Smallest variance among unbiased estimators^[[(New)]{.blue} RMES: Root mean square error, $\sqrt{bias^2 + var}$.]

:::{.fragment}
```{r efficiency}
#| fig.align: 'center'
#| fig-height: 5

ggplot(data.frame(x = c(-7, 7)), aes(x = x)) +
  stat_function(fun = function(x) dnorm(x, mean = 0, sd = 0.5), aes(color = "0.5")) +
  stat_function(fun = function(x) dnorm(x, mean = 0, sd = 1), aes(color = "1")) +
  stat_function(fun = function(x) dnorm(x, mean = 0, sd = 2), aes(color = "2")) +
  ylab("Probability Density") + 
  xlab("X") +
  labs(color = "SD")
```
:::


:::{.notes}
Sometimes, one may want to trade off a little bias against variance.
:::

## Ordinary Least Squares (OLS)

Substantively, how does some outcome variable Y change [when]{.blue} some explanatory variable X changes? 

:::{.fragment}
One type of [simulation]{.red} of the reality among many others (unnecessarily the best one).
:::


![How to know](images/ols_leastSquare.gif){.fragment fig-align="center" height=600}

:::{.notes}

![](images/ols_illustrateSimpleOLS.png)

:::


## OLS Component: Linear Estimator

:::{style="text-align:center"}
What's linear?
:::

:::{.notes}
- First three are linear.
- the second one isn't.
- The last one isn't linear until redefining.
:::

:::{.fragment}
\begin{align}
Y_i =& \beta_0 + \beta_1X_i + ui;\\
Y_i =& \beta_0 + \frac{\beta_1}{X_i} + ui;\\
Y_i =& \beta_0 + \beta_1ln(X_i) + ui;\\
Y_i =& \beta_0 + X_i^{\beta_1} + ui;\\
Y_i =& \frac{1}{\beta_0} + \frac{X_i}{\beta_1} + ui.
\end{align}
:::

:::{.fragment}
The outcome variable [changes]{.red} linearly with the explanatory variable.
:::

:::{.fragment}
![](images/ci_fsmrof.png){height=50} Estimand/parameter, estimator, estimate
:::

:::{.notes}
- Estimand: Parameter in the population which is to be estimated in a statistical analysis
- Estimator: A rule for calculating an estimate of a given quantity based on observed data. Function of the observations, i.e., how observations are put together
- Estimation: The process of finding an **estimate**, or approximation, which is a value that is usable for some purpose even if input data may be incomplete, uncertain, or unstable (value derived from the best information available)
:::


## OLS Formally

\begin{align}
Y_i =& \beta_0 + \beta_1X_1 + u_i.\\
E(Y_i|X_i) =& E(\beta_0 + \beta_1X_1 + u_i|X_i),\\
           =& E(\beta_0|X_i) + E(\beta_1X_1|X_i) + E(u_i|X_i),\\
           =& \beta_0 + \beta_1X_i + E(u_i|X_i).
\end{align}

:::{.fragment}
u<sub>i</sub>: Things we don't know (or human errors) but [hope]{.red} to be zero.
:::

:::{.notes}
$\beta_1X_i$ consistent

E(u_i|X_i) assumed 0
:::

:::{.fragment}
**Sample Regression Function (SRF)**

\begin{align}
Y_i =& (\hat\beta_0 + \hat\beta_1X_i) + \hat u_i,\\
    =& \hat Y_i + \hat u_i.\\
    =& E(Y_i|X_i) + u_i,\\
\Leftrightarrow u_i =& Y_i - E(Y_i|X_i).
\end{align}
:::



## A Good Estimator &rarr; Minimizing Expected u<sub>i</sub>

 &hArr; &beta;<sub>0</sub> and &beta;<sub>1</sub> that make $\sum[Y_i - (\hat\beta_0 + \hat\beta_1X_i)]^2 = 0$


\begin{align}
\frac{\partial\sum[Y_i - (\hat\beta_0 + \hat\beta_1X_i)]^2}{\partial\hat\beta_1} = -\sum 2X_i(Y_i - \hat\beta_0 - \hat\beta_1X_i) =& 0\\
\Leftrightarrow \sum X_iY_i - \sum X_i\hat\beta_0 - \sum\hat\beta_1 X_1^2 =& 0\\
\sum X_iY_i =& \sum X_i\hat\beta_0 + \sum\hat\beta_1 X_1^2\\
\sum X_iY_i - \sum X_i(\bar Y - \hat\beta_1\bar X) - \sum\hat\beta_1 X_1^2 =& 0\\
\sum X_iY_i - \sum X_i\bar Y + \sum X_i\hat\beta_1\bar X - \sum\hat\beta_1 X_1^2 =& 0\\
\sum X_i(Y_i - \bar Y) + \hat\beta_1\sum X_i(\bar X - X_i) =& 0\\
\sum X_i(Y_i - \bar Y) =& \hat\beta_1\sum X_i(X_i - \bar X)
\end{align}

---

A transformation gadget:

\begin{align}
\sum X_i(Y_i - \bar Y) =& \sum X_i(Y_i - \bar Y) - \bar X(n\bar Y - n\bar Y)\\
                       =& \sum X_i(Y_i - \bar Y) - \bar X(\sum Y_i - \sum\bar Y)\\
                       =& \sum X_i(Y_i - \bar Y) - \bar X\sum (Y_i - \bar Y)\\
                       =& \sum (X_i - \bar X)(Y_i - \bar Y) \blacksquare
\end{align}

:::{.notes}
 "Quod Erat Demonstrandum" (QED) which loosely translated means "that which was to be demonstrated".
:::

:::{.fragment}
\begin{align}
Given \sum(X_i - \bar X)(Y_i - \bar Y) =& \sum X_i(Y_i - \bar Y)\\
\sum X_i(Y_i - \bar Y) =& \hat\beta_1\sum X_i(X_i - \bar X) (Last\ page)\\
\sum (X_i - \bar X)(Y_i - \bar Y) =& \hat\beta_1\sum (X_i - \bar X)(X_i - \bar X)\\
\hat\beta_1 =& \frac{\sum (X_i - \bar X)(Y_i - \bar Y)}{\sum (X_i - \bar X)^2}.
\end{align}
:::


## Coefficient: &beta;<sub>0</sub>

\begin{align}
\frac{\partial\sum[Y_i - (\hat\beta_0 + \hat\beta_1X_i)]^2}{\partial\hat\beta_0} =& -\sum 2(Y_i - \hat\beta_0 - \hat\beta_1X_i) = 0\\
\Leftrightarrow\sum Y_i - \sum\hat\beta_0 - \sum\hat\beta_1X_i =& 0\\
\sum Y_i =& \sum\hat\beta_0 + \sum\hat\beta_1X_i\\
                         =& n\hat\beta_0 + \hat\beta_1\sum X_i\\
\hat\beta_0 =& \frac{\sum Y_i}{n} - \hat\beta_1\frac{\sum X_i}{n}\\
                        =& \bar Y - \hat\beta_1\bar X
\end{align}


:::{.fragment}
*Normal equations*

\begin{align}
\sum Y_i =& n\hat\beta_0 + \hat\beta_1\sum X_i\\
\sum X_iY_i =& \sum X_i\hat\beta_0 + \sum\hat\beta_1 X_1^2
\end{align}
:::


## Coefficient: &beta;<sub>1</sub>^[$\hat\beta_0, \hat\beta_1$ are called [co]{.red}efficients.]

:::{.fragment}
\begin{align}
\hat\beta_1 =& \frac{\sum (X_i - \bar X)(Y_i - \bar Y)}{\sum (X_i - \bar X)^2}\\
        =& \frac{\sum (X_i - \bar X)(Y_i - \bar Y)}{\sqrt{\sum (X_i - \bar X)^2}\sqrt{\sum (Y_i - \bar Y)^2}}\cdot\frac{\sqrt{\sum (Y_i - \bar Y)^2}}{\sqrt{\sum (X_i - \bar X)^2}}\\
        =& r_{X, Y}\frac{s_Y}{s_X}
\end{align}
:::

:::{.fragment}
So when the variance of Y(s<sub>Y</sub>) increases, &beta;<sub>1</sub> increases.

Special case: Standardized X and Y, i.e., $s_Y, s_X$ are 1s, then,

$$\beta_1 = r_{X, Y}\frac{s_Y}{s_X} = r_{X,Y}.$$

:::


:::{.notes}
&beta;<sub>0</sub> is efficient (minimizng the u) depending on &beta;<sub>1</sub>
:::

## Linearity of Coefficient

Remember $\sum X_i(Y_i - \bar Y) = \sum(X_i - \bar X)(Y_i - \bar Y).$

:::: {.columns}

::: {.column width="50%"}
\begin{align}
\hat\beta_1 =& \frac{\sum (X_i - \bar X)(Y_i - \bar Y)}{\sum (X_i - \bar X)^2},\\
            =& \frac{1}{\sum (X_i - \bar X)^2}\sum (X_i - \bar X)Y_i,\\
            =& \frac{1}{\sum (X_i - \bar X)^2}\sum (X_i - \bar X)(\beta_0 + \beta_1X_i + u_i),\\
            =& \frac{1}{\sum (X_i - \bar X)^2}[\sum (X_i - \bar X)(\beta_0 + \beta_1X_i) + \sum (X_i - \bar X)u_i],\\
            =& \frac{\sum (X_i - \bar X)(\beta_0 + \beta_1X_i)}{\sum (X_i - \bar X)^2} + \frac{\sum (X_i - \bar X)u_i}{\sum (X_i - \bar X)^2}.
\end{align}
:::

::: {.column .fragment width="50%"}
Let $k_i=\frac{X_i - \bar X}{\sum (X_i - \bar X)^2},$ then $\hat\beta_1 = \beta_1 + \sum k_iu_i$. 

+ A [linear]{.red} combination of errors
+ Min/max(X) influences a lot.
:::

::::

:::{.notes}
$\sum(X_i - \bar X) = 0.$
:::


## Characteristics of OLS Coefficients

:::: {.columns}

::: {.column width="55%"}
1. Calculated using [observed]{.red} data
1. Unique point estimates
1. SRF passes through $(\bar X, \bar Y)$
1. $\bar{\hat Y} (\text{predicted}) = \hat Y (\text{observed})$, $\frac{\sum\hat Y_i}{n} = \frac{\sum Y_i}{n}$
1. $\bar{\hat u_i} = \frac{\sum{\hat u_i}}{n} = 0$
1. $\sum X_i\hat u_i = 0$

:::

::: {.column width="45%"}

:::{.fragment}
$\bar{\hat u_i} = \frac{\sum{\hat u_i}}{n} = 0$

\begin{align}
\frac{\partial\sum[Y_i - (\hat\beta_0 + \hat\beta_1X_i)]^2}{\partial\hat\beta_0} =& 0\\ -2\sum[Y_i - (\hat{\beta_0} + \hat{\beta_1}X_i)] =& 0\\
\sum (Y_i-\hat Y_i) = \sum{\hat u_i} =& 0\\
\Rightarrow \frac{\sum{\hat u_i}}{n} = \bar{\hat u_i} =& 0 \blacksquare
\end{align}
:::

:::{.fragment}
$\sum X_i\hat u_i = 0$

\begin{align}
cov(X_i, u_i) =& 0,\\
\frac{\sum(X_i - \bar X)(\hat u_i - \bar{\hat u_i})}{n-1} =& 0,\\
\frac{\sum X_i\hat u_i}{n-1} =& 0. \blacksquare
\end{align}
:::

:::

::::

---

![](images/ols_minimizeSS.gif){fig-align="center" height=800}


## Estimating Uncertainty for OLS: &beta;<sub>1</sub>

:::: {.columns}

::: {.column width="30%"}
\begin{align}
\sigma^2 =& var(u_i|X),\\
         =& var(Y_i - \hat\beta_0 - \hat\beta_1X_i|X),\\
         =& \frac{\sum(\hat u_i^2)}{n - 2},\\
         =& \hat\sigma^2.
\end{align}
:::

:::{.notes}
n - 2: $\hat\beta_0, \hat\beta_1$
:::


::: {.column width="70%"}
\begin{align}
var(\hat \beta_1|X) =& var(\frac{\sum(X_i - \bar X)(Y_i - \bar Y)}{\sum(X_i - \bar X)^2}|X)\\
                  =& var(\beta_1 + \sum k_iu_i|X)\\
                  =& var(\sum k_iu_i|X), \text{given}\ \beta_1\sum k_iX_i \text{constant}\\
                  =& \sum var(k_iu_i|X), \text{assuming}\ u_i\ \text{independent}\\
                  =& \sum k_i^2 var(u_i|X)\\
                  =& \sum[\frac{X_i - \bar X}{\sum (X_i - \bar X)^2}]^2\sigma^2\\
                  =& \frac{\sum(X_i - \bar X)^2}{[\sum (X_i - \bar X)^2]^2}\sigma^2 = \frac{\sigma^2}{\sum (X_i - \bar X)^2}
\end{align}
:::

:::{.notes}
If u<sub>i</sub> is not independent, then cov(k, u) > 0, and this estimator is underestimated.

(That is, the assumption is .large[.red[IMPORTANT]])

:::

::::


## Uncertainty of OLS Estimators: &beta;<sub>0</sub>

\begin{align}
var(\hat \beta_0|X) =& var(\bar Y - \hat\beta_1\bar X|X),\\
                   =& var[\frac{\sum(\beta_0 + \beta_1X_i + u_i)}{n} - \hat\beta_1\bar X|X],\\
                  \because \beta_0 +& \beta_1X_i\text{ is constant & independent},\\
                  =& var(\frac{\sum u_i}{n}|X) + var(\hat\beta_1\bar X|X), \\
                  =& var(\frac{\sum u_i}{n}|X) + var(\hat\beta_1\bar X|X) = \frac{var(\sum u_i|X)}{n^2} + \bar X^2var(\hat\beta_1|X),\\
                  =& \frac{n\sigma^2}{n^2} + \frac{\bar X^2\sigma^2}{\sum (X_i - \bar X)^2} = \sigma^2[\frac{1}{n} + \frac{\bar X^2}{\sum (X_i - \bar X)^2}],\\
                  =& \sigma^2[\frac{\sum (X_i - \bar X)^2 + n\bar X^2}{n\sum (X_i - \bar X)^2}] = \sigma^2[\frac{\sum X_i^2 - \sum\bar X^2 + n\bar X^2}{n\sum (X_i - \bar X)^2}],\\
                  =& \sigma^2[\frac{\sum X_i^2 - n\bar X^2 + n\bar X^2}{n\sum (X_i - \bar X)^2}]= \sigma^2\frac{\sum X_i^2}{n\sum (X_i - \bar X)^2}.
\end{align}



## Take-home point

::: {style="text-align: center"}
![](images/ols_mindmap.png){height="750"}
:::


## Have a break

{{< video src="https://link.jscdn.cn/1drv/aHR0cHM6Ly8xZHJ2Lm1zL3YvcyFBcnR0dk83MHdLSU8xSDM3UzNHdnNiNExETzJUP2U9SmIxVGFw.mp4" height="850" >}}