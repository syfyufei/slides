---
title: "话语分析方法及应用"
subtitle: "见字为数"
author: "孙宇飞"
date: "`r format(Sys.Date(), '%Y-%m-%d')`"
format:
  revealjs:
    slide-number: true
    controls: true
    overview: true
    incremental: true
    hash: true
    code-line-numbers: true
    transition: slide
    theme: simple
    css: assets/styles.css
    fig-cap-location: bottom
    logo: assets/logo.svg
    footer: "Discourse NLP Lecture | Adrian Sun"
    embed-resources: false
    chalkboard: true
    preview-links: auto
params:
  eval_code: true
  data_source: "quanteda_rds"
  smooth_window: 5
  standardize_by_decade: true
  lang: "zh"
  show_notes: false
  show_code: false
execute:
  echo: true
  eval: true
  warning: false
  message: false
  cache: false
toc: false
---

```{r setup}
#| include: false
#| eval: true

# Set global eval based on params (default to TRUE)
EVAL_CODE <- ifelse(
  exists("params") && !is.null(params$eval_code),
  isTRUE(params$eval_code),
  TRUE
)

# Control whether to show code in slides (default to FALSE)
SHOW_CODE <- ifelse(
  exists("params") && !is.null(params$show_code),
  isTRUE(params$show_code),
  FALSE
)

# Utility: resolve an available font that supports Chinese glyphs
resolve_font <- function(candidates, default = "sans") {
  if (!requireNamespace("systemfonts", quietly = TRUE)) {
    return(default)
  }
  for (candidate in candidates) {
    info <- tryCatch(systemfonts::font_info(family = candidate), error = function(e) NULL)
    if (!is.null(info) && nrow(info) > 0) {
      return(info$family[1])
    }
  }
  default
}

BASE_FONT <- resolve_font(
  c(
    "PingFang SC",
    "Microsoft YaHei",
    "Noto Sans CJK SC",
    "Source Han Sans SC",
    "WenQuanYi Micro Hei",
    "SimHei",
    "Arial Unicode MS"
  )
)

options(slides_base_font = BASE_FONT)

# Load required packages (only if needed)
if (EVAL_CODE) {
  suppressPackageStartupMessages({
    library(readr)
    library(dplyr)
    library(ggplot2)
    library(yaml)
  })
}

# Always load knitr
library(knitr)

# Set global options
knitr::opts_chunk$set(
  eval = EVAL_CODE,            # Controlled via params$eval_code
  echo = SHOW_CODE,            # Controlled via params$show_code
  warning = FALSE,
  message = FALSE,
  fig.width = 10,
  fig.height = 6,
  fig.align = "center",
  cache = FALSE,
  engine.path = list(python = Sys.which("python3"))
)

# Set ggplot theme
if (EVAL_CODE) {
  theme_set(theme_minimal(base_size = 14, base_family = BASE_FONT))
}

# Helper function for smooth rolling average
roll_mean <- function(x, window = 5) {
  smooth_window <- ifelse(exists("params") && !is.null(params$smooth_window), params$smooth_window, 5)
  if (length(x) < smooth_window) return(x)
  stats::filter(x, rep(1/smooth_window, smooth_window), sides = 2) %>% as.numeric()
}
```

# 开场故事 {.center}

---

## 一个有趣的研究

:::: {.columns}

::: {.column width="60%"}
**一组研究者做了一件事**^[Baturo, Alexander, Niheer Dasandi, and Slava J. Mikhaylov. 2017. "Understanding State Preferences With Text As Data: Introducing the UN General Debate Corpus." *Research & Politics* 4(2).]：

他们收集了 1970-2016 年间所有联合国大会一般性辩论的演讲稿，共 **7,700+ 篇演讲**，涵盖 196 个国家，问了一个问题：

> 各国领导人在联大的发言文本，能揭示他们的外交立场和意识形态吗？

**结果**：仅仅通过分析演讲文本中的词语分布，就能：
- 识别各国的外交政策立场（左派/右派/中立）
- 追踪一个国家几十年间的立场变化
- 预测国家之间的联盟关系

**他们怎么做到的？** → 把**演讲文本**变成了**可量化的变量**
:::

::: {.column width="40%"}
![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/10/Methodist_Central_Hall.JPG/250px-Methodist_Central_Hall.JPG){height=400}


:::

::::

::: {.callout-note appearance="minimal"}
## 📚 延伸阅读

- Baturo et al. (2017). "Understanding State Preferences With Text As Data: Introducing the UN General Debate Corpus." *Research & Politics* 4(2).
- Rheault & Cochrane (2020). "Word Embeddings for the Analysis of Ideological Placement in Parliamentary Corpora." *Political Analysis* 28(1).
:::

::: notes
讲者备注：
- 用真实研究案例吸引注意力（Bailey, Voeten 等的研究）
- 强调"文本 → 数字 → 预测"的转化魔力
- 为"见字为数"概念做铺垫
- 控制 2-3 分钟
:::

---

## 文本无处不在，问题也无处不在

:::: {.columns}

::: {.column width="50%"}
**政治科学**

- 总统演讲的"强硬度"变了吗？

- 立法辩论中的"框架"如何影响投票？

- 媒体报道的"偏向性"如何测量？

**社会学**

- 社交媒体上的"极化"有多严重？

- 性别刻板印象在语言中如何体现？

- 社会运动的"话语策略"如何演化？
:::

::: {.column width="50%"}
**经济学 & 管理学**

- 企业财报的"情感色彩"影响股价吗？

- 客户评论中的"痛点"是什么？

- 招聘广告的"性别暗示"存在吗？

**历史学 & 文化研究**

- 不同时代的"价值观"如何迁移？

- 文学作品中的"叙事模式"变了吗？
:::

::::

::: {.fragment}
::: {.callout-important}
## 共同点

这些问题的答案，都**藏在文本里**。但文本不是数字，**怎么用数据科学的方法来研究它？**
:::
:::

::: notes
讲者备注：
- 展示文本分析的跨学科应用广度
- 引出核心矛盾：文本 vs 数字
- 为 NLP 方法论引入做铺垫
- 2-3 分钟
:::

---

## 自然语言处理（NLP）：把文字变成数据

::: {.r-fit-text}
**Natural Language Processing** = 让计算机"读懂"人类语言的技术
:::

:::: {.columns}

::: {.column width="50%"}
**NLP 的三个层次**

1. **结构层**（Structure）
   分词、词性、句法树...
   *"把句子拆开"*

2. **语义层**（Semantics）
   词义、实体、关系...
   *"理解说了什么"*

3. **语用层**（Pragmatics）
   情感、立场、意图...
   *"理解为什么这么说"*
:::

::: {.column width="50%"}
**社会科学关心什么？**

::: {.fragment}
我们**不只是想读懂**一句话，
更想知道：

- **谁**在说？（发言人特征）
- **对谁**说？（受众定位）
- **在什么情境**说？（历史背景）
- **说了之后产生什么效果？**（因果推断）

→ 这就是 **Discourse Analysis**（话语分析）
:::
:::

::::

::: notes
讲者备注：
- 简要介绍 NLP 三层次（不深入技术细节）
- 强调社会科学的独特视角：不只是"读懂"，更要"解释"
- 为"见字为数"的方法论定位
- 3 分钟
:::

---

## 在计算机眼里，文本是什么？

::: {.r-fit-text}
**提问**：计算机如何"看"一段文字？
:::

:::: {.columns}

::: {.column width="55%"}
**传统方法：词袋模型（Bag of Words）**

```
"I love natural language processing"

      ↓ 分词

["I", "love", "natural", "language", "processing"]

      ↓ 统计

I: 1次, love: 1次, natural: 1次, ...
```

::: {.fragment}
**核心思想**：文本 = 词的**概率分布**

$$P(\text{词} | \text{文档}) = \frac{\text{词频}}{\text{总词数}}$$
:::
:::

::: {.column width="45%"}
::: {.fragment}
**现代方法：语言模型（Language Model）**

```
"The president said ___"

计算机预测下一个词：
- "that"    (概率: 0.35)
- "he"      (概率: 0.25)
- "we"      (概率: 0.15)
- ...
```

::: {.callout-tip appearance="minimal"}
**大语言模型（LLM）= 超级复杂的概率预测机器**

给定前文，预测下一个词的概率分布
:::
:::
:::

::::

::: notes
讲者备注：
- 引入语言模型的概念：文本是概率分布
- 从词袋法到语言模型的演进
- 为后面"黑箱"概念做铺垫
- 2 分钟
:::

---

## NLP vs 传统话语分析：有何不同？

| 维度 | **传统话语分析** | **NLP 方法** |
|------|-----------------|-------------|
| **数据规模** | 小样本（几十篇） | 大规模（成千上万篇） |
| **分析单位** | 深度解读单个文本 | 跨文本的模式识别 |
| **方法论** | 质性编码、理论建构 | 统计模型、机器学习 |
| **可重复性** | 依赖研究者判断 | 代码可复现 |
| **理论深度** | 强（情境、权力、建构） | 弱（模式、关联） |
| **因果推断** | 机制解释 | 预测关联 |

::: {.fragment}
::: {.callout-important}
## 🎯 本课程的立场

**不是"NLP取代传统方法"**，而是：

- 用 NLP 处理大规模文本，找到**系统性模式**
- 用传统方法深度解读**关键案例**，理解**机制**
- **混合方法**（Mixed Methods）才是王道
:::
:::

::: notes
讲者备注：
- 强调两种方法的互补性，而非对立
- NLP 擅长"发现模式"，传统方法擅长"解释机制"
- 为学生的研究设计提供清晰定位
- 2 分钟
:::

---

# 见字为数：核心理念 {.center}

---

## 什么是"见字为数"？

::: {.r-fit-text}
把文本**稳稳地**转化为**可测量、可比较、可检验**的数值变量
:::

:::: {.columns}

::: {.column width="50%"}
### 传统社会科学工具箱

```
问卷调查 → 数值变量
实验干预 → 对照组比较
行政数据 → 面板数据
         ↓
    统计模型
    因果推断
    可视化
```

::: {.fragment}
**问题**：文本数据怎么办？
:::
:::

::: {.column width="50%"}
### "见字为数"的思路

```
文本语料 → 数值变量
         ↓
    【黑箱】
         ↓
    统计模型
    因果推断
    可视化
```

::: {.fragment}
**关键**：打开【黑箱】，
**不换工具箱**，只是让文本变成"能进工具箱的变量"
:::
:::

::::

::: notes
讲者备注：
- 类比传统方法，降低陌生感
- 强调"不换工具箱"——这是给社会科学学生的定心丸
- 为后续"五条路线"做总纲
- 2 分钟
:::

---

## 打开【黑箱】：语言模型是什么？

::: {.r-fit-text}
**语言模型 = 从文本学习概率分布的数学函数**
:::

:::: {.columns}

::: {.column width="50%"}
### 一个具体例子

假设我们有3篇SOTU演讲：

```
文档1: "war threat security"
文档2: "peace cooperation dialogue"
文档3: "war peace security"
```

**词袋模型**统计每个词的频率：

$$P(\text{"war"}) = \frac{2}{9} = 0.22$$
$$P(\text{"peace"}) = \frac{2}{9} = 0.22$$
$$P(\text{"security"}) = \frac{2}{9} = 0.22$$

:::

::: {.column width="50%"}
::: {.fragment}
### 语言模型的进化

**传统语言模型（N-gram）**：
$$P(w_i | w_{i-1}, w_{i-2}) = \frac{\text{count}(w_{i-2}, w_{i-1}, w_i)}{\text{count}(w_{i-2}, w_{i-1})}$$

**神经网络语言模型（Word2Vec）**：
$$\text{vec}(\text{"war"}) = [0.8, -0.3, 0.5, ...]$$
词被表示为高维向量（embedding）

**大语言模型（LLM/Transformer）**：
$$P(w_i | w_1, w_2, ..., w_{i-1}) = \text{Softmax}(\text{Transformer}(...))$$
:::
:::

::::

::: {.fragment}
::: {.callout-warning}
## 🔒 为什么说是"黑箱"？

**词袋模型**：完全透明，可以看到每个词的频率
**Word2Vec**：半透明，可以检查词向量的相似度
**GPT/BERT**：黑箱，数十亿参数，无法直接解释每个决策

→ 本课程重点：**可解释的方法**（字典法、主题模型、网络）+ **谨慎使用黑箱**（LLM标注）
:::
:::

::: notes
讲者备注：
- 用简单例子展示语言模型如何工作
- 从词袋→Word2Vec→LLM的演进路径
- 强调黑箱的程度递增，但不是完全不可用
- 为课程方法选择提供理论依据
- 3 分钟
:::

---

## "见字为数"的三重境界

:::: {.columns}

::: {.column width="33%"}
### 第一重：数什么

**字典法 / 关键词**

"强硬"的词出现了几次？
"礼貌"的词占比多少？

::: {.fragment}
✅ **优点**：快速、可解释
⚠️ **局限**：依赖预定义
:::
:::

::: {.column width="33%"}
### 第二重：看什么

**主题模型 / 网络分析**

文本在"讲什么主题"？
哪些词"总是一起出现"？

::: {.fragment}
✅ **优点**：发现未知模式
⚠️ **局限**：需要解释
:::
:::

::: {.column width="33%"}
### 第三重：懂什么

**语义嵌入 / LLM 标注**

这句话和"战争话语"有多近？
这段文本的"框架"是什么？

::: {.fragment}
✅ **优点**：捕捉深层语义
⚠️ **局限**：黑箱、成本
:::
:::

::::

::: {.fragment}
::: {.callout-tip}
## 实战建议

**从简单到复杂，从已知到未知，从探索到验证**

字典先验 + 主题探索 + 语义精修 = 稳健的证据链
:::
:::

::: notes
讲者备注：
- 三重境界对应后续"五条路线"的分类逻辑
- 每种方法有优劣，没有银弹
- 组合使用才是王道
- 3 分钟
:::

---

## 今天的路线图

::: {.r-fit-text}
从一个真实案例，走完"见字为数"的全流程
:::

**贯穿案例**：美国总统国情咨文（SOTU, 1790-2023）

**核心问题**：战争时期的总统演讲，真的更"强硬"吗？

**三步走**：

1. **数据描述**：用字典法/主题模型，画出"强硬指数"的时间曲线
2. **网络探索**：用共现分析，看不同年代的"话语结构"
3. **因果推断**：用回归模型，检验"战争 → 强硬"的关系是否稳健

::: {.fragment}
**你会带走**：
- 可复用的代码模板
- 可迁移的分析思路
- 可讲述的故事结构
:::

::: notes
讲者备注：
- 明确全程主线：一个问题，三个步骤
- 预告后续内容，建立期待
- 强调"可迁移"——学生可以用到自己的数据上
- 2 分钟
:::

---

# 课程地图（90-120 分钟）

---

## 内容与时间安排

| 时间 | 模块 | 内容要点 |
|------|------|---------|
| **0-15'** | **破冰与引入** | 故事开场 → NLP 概念 → "见字为数"框架 |
| **15-30'** | **数据准备** | 获取 SOTU 数据 → 文本预处理实操 → 探索性分析 |
| **30-60'** | **五条路线** | 字典法(10') → 主题模型(10') → 共现网络(10') |
| **60-80'** | **进阶方法** | 语义距离(10') → LLM 标注(10') |
| **80-95'** | **实证演示** | 回归分析 → 敏感性检验 → 结论讨论 |
| **95-110'** | **Lab 实践** | 学生动手：跑完一遍完整流程 |
| **110-120'** | **总结回顾** | 梳理要点 → 延伸资源/下一步计划 |

::: notes
讲者备注：
- 把课程地图放在最前面，避免和后面内容重复
- 时间分配可根据实际情况调整
- 强调 Lab 环节的重要性
- 1 分钟
:::

---

## 贯穿式案例：SOTU

:::: {.columns}

::: {.column width="50%"}
**数据来源**

- 美国总统国情咨文（State of the Union）
- 时间跨度：1790—2023 年
- 真实语料，公开可获取
- 236 篇演讲，43 位总统
:::

::: {.column width="50%"}
**研究问题**

1. **讲什么**变了？（主题与搭配）
2. **怎么讲**变了？（强硬/礼貌/模糊等风格）
3. 变化与**总统/年代/事件**的关系？
:::

::::

::: notes
讲者备注：
- SOTU 是经典政治文本语料，适合教学演示
- 强调研究问题的层次：内容（what）、风格（how）、关联（why）
- 为后续五条路线做铺垫
:::

---

# 课前准备：下载资源 {.center}

---

## 📦 下载课程材料

::: {.callout-note icon=false}
## 🔗 下载链接

:::: {.columns}

::: {.column width="50%"}
### 📊 数据文件

**SOTU 语料数据**
- 📥 [下载 sotu.csv](data/sotu.csv) (约 5MB)
- 包含 1790-2023 年所有国情咨文
- 字段：date, president, party, text, year

**或使用脚本自动获取**：
```r
source("scripts/fetch_sotu.R")
```
:::

::: {.column width="50%"}
### 📝 脚本文件

**分析脚本打包下载**
- 📥 [fetch_sotu.R](scripts/fetch_sotu.R) - 数据获取
- 📥 [make_collocation_graph.R](scripts/make_collocation_graph.R) - 网络分析
- 📥 [dict_en.yml](dict/dict_en.yml) - 风格字典

**完整项目**
- 🔗 [GitHub 仓库](https://github.com/adriansun/discourse-nlp-lecture)
- 包含所有代码、数据、文档
:::

::::
:::

::: {.callout-tip}
## 💡 建议

- **跟随演示**：现在下载不是必需的，可以先听讲
- **动手实践**：Lab 环节前下载数据和脚本
- **课后复现**：克隆完整 GitHub 仓库
:::

::: notes
讲者备注：
- 展示下载链接，但不要求立即下载
- 强调 Lab 前下载即可
- 提醒学生记下 GitHub 地址
:::

---

## 📚 安装 R 包（课前或 Lab 前）

**怎么做**：把课程分析用到的 R 包整理成列表，方便学员课前复制到控制台一次安装。

```{r install-packages}
#| eval: false

# 复制粘贴到 R 控制台运行（仅需一次）
install.packages(c(
  # 数据处理
  "readr", "dplyr", "tidyr", "ggplot2",

  # 文本分析
  "quanteda", "quanteda.textstats", "stm",

  # 网络与可视化
  "igraph", "yaml",

  # 统计建模
  "lmtest", "sandwich", "broom", "irr"
))
```

::: {.callout-warning}
**时间提示**：安装大约需要 5-10 分钟，建议课前完成。

如果网络受限，可以使用清华 CRAN 镜像：
```r
options(repos = c(CRAN = "https://mirrors.tuna.tsinghua.edu.cn/CRAN/"))
```
:::

::: notes
讲者备注：
- 快速过一遍，不要在这里花太多时间
- 提醒学生课后可以慢慢安装
- 强调 Lab 环节会用到
:::

---

# 数据获取与准备

---

## 一键获取 SOTU 数据

**怎么做**：运行预先写好的 `fetch_sotu.R`，在线抓取最新版 SOTU 语料并写入 `data/sotu.csv`。

```{r fetch-data}
#| eval: !expr EVAL_CODE

# Run data fetching script
source("scripts/fetch_sotu.R")

# Read data
library(readr)
d <- read_csv("data/sotu.csv", show_col_types = FALSE)
message("✓ Loaded ", nrow(d), " SOTU documents")
```

::: {.callout-tip}
**数据字段**：`date`, `president`, `party`, `text`, `year`

数据源可选：`quanteda_rds`（在线）或 `csv`（离线）
:::

::: notes
讲者备注：
- 演示如何一行命令获取数据
- 如果网络不佳，提前准备好 CSV 文件
- 强调数据结构简单明了：每行一篇演讲
:::

---

## 数据概览

**怎么做**：读取刚生成的 CSV，查看核心元数据并用 `table()` 统计党派分布。

```{r data-overview}
#| eval: !expr EVAL_CODE

# Check data structure
head(d[, c("year", "president", "party")], 10)

# Summary statistics
cat("Time range:", range(d$year, na.rm = TRUE), "\n")
cat("Total documents:", nrow(d), "\n")
cat("Unique presidents:", length(unique(d$president)), "\n")
cat("Party distribution:\n")
table(d$party)
```

::: {.callout-note appearance="simple"}
## 📊 预期输出示例

```
   year president          party
1  1790 Washington          none
2  1790 Washington          none
3  1791 Washington          none
4  1792 Washington          none
5  1793 Washington          none
6  1794 Washington          none
7  1795 Washington          none
8  1796 Washington          none
9  1797 Adams       Federalist
10 1798 Adams       Federalist

Time range: 1790 2019
Total documents: 240
Unique presidents: 38

Party distribution:
Democratic Democratic-Republican Federalist Independent Republican      Whig
        94                   28          4           8         98         8
```
:::

::: notes
讲者备注：
- 快速过一遍数据结构
- 让学生看到真实数据的样子
- 为后续分析建立直觉
- 强调示例输出让学生了解数据长什么样
:::

---

# 课程地图（90-120 分钟）

---

## 内容安排

1. **把"问题"说人话**（15分钟）
2. **从文本到变量**（15分钟）
3. **五条路线，像五种做法**（30分钟）
   - 字典法、主题模型、共现网络、语义距离、LLM 标注
4. **LLM = 勤劳的助教**（20分钟）
5. **一起复刻迷你结论**（10分钟）
6. **Lab：15分钟跑通一遍**（25分钟）
7. **结尾：见字为数**（1分钟）

---

# 从文本到变量（备料）

---

## 文本预处理：为什么重要？

::: {.r-fit-text}
> **做菜比喻**：备料干净，出锅才稳定
:::

:::: {.columns}

::: {.column width="50%"}
**原始文本的"脏"**

```
We must defend our nation's
security!!!   The threats are
GRAVE. We'll act decisively...
```

**问题**：
- 标点符号干扰
- 大小写不统一
- 停用词（the, are）无意义
- 数字、URL、表情符号
:::

::: {.column width="50%"}
**处理后的"净"**

```
defend nation security
threat grave act decisive
```

**好处**：
- ✅ 聚焦实词（content words）
- ✅ 减少噪音，提高信噪比
- ✅ 统一格式，便于比较
- ✅ 降维，提高计算效率
:::

::::

::: notes
讲者备注：
- 用对比展示预处理的价值
- 强调"垃圾进，垃圾出"（Garbage in, garbage out）
- 预处理不是可选项，是必选项
- 2 分钟
:::

---

## 第一步：分词（Tokenization）

**目标**：把句子拆成词的序列

**怎么做**：抽取首篇演讲，放入 corpus，再用 `tokens()` 演示最原始的分词效果。

```{r tokenization-demo}
#| eval: !expr EVAL_CODE

library(quanteda)

# 示例：取第一篇 SOTU
text_sample <- d$text[1]

# 创建 corpus
corp <- corpus(text_sample)

# 分词
tok <- tokens(corp)

# 查看前 50 个词
head(tokens_select(tok, pattern = ".*", valuetype = "regex"), 50)
```

::: {.callout-note appearance="simple"}
## 📊 实际输出

```
Tokens consisting of 1 document.
text1 :
 [1] "Fellow-Citizens" "of"              "the"             "Senate"
 [5] "and"             "House"           "of"              "Representatives"
 [9] ":"               "I"               "embrace"         "with"
[ ... and 21 more ]
```

**关键信息**：
- 每个词（token）独立
- 保留了原始顺序
- 标点符号也是 token
:::

::: notes
讲者备注：
- 展示 quanteda 的 tokens() 函数
- 强调分词是后续所有操作的基础
- 英文简单，中文复杂（需要 jieba 等工具）
- 2 分钟
:::

---

## 第二步：去噪（Cleaning）

**目标**：去掉无意义的词，保留实词

**怎么做**：在刚才的 tokens 基础上，依次去掉标点、数字与停用词，并统一转小写，得到更干净的词序列。

```{r cleaning-demo}
#| eval: !expr EVAL_CODE

# 去除标点和数字
tok_clean <- tokens(
  corp,
  remove_punct = TRUE,      # 去标点
  remove_numbers = TRUE,    # 去数字
  remove_symbols = TRUE     # 去符号
)

# 转小写
tok_clean <- tokens_tolower(tok_clean)

# 去停用词
tok_clean <- tokens_remove(
  tok_clean,
  stopwords("en")  # 英文停用词表（175个常见词）
)

# 查看效果
head(as.character(tok_clean), 50)
```

::: {.callout-note appearance="simple"}
## 📊 实际输出

```
 [1] "fellow-citizens" "senate"          "house"           "representatives"
 [5] "embrace"         "great"           "satisfaction"    "opportunity"
 [9] "now"             "presents"        "congratulating"  "present"
[13] "favorable"       "prospects"       "public"          "affairs"
```

**对比原始版本**：
- ❌ 去掉了 "the", "of", "and", "I" 等停用词
- ❌ 去掉了标点符号 ":", ",", "."
- ✅ 保留了 "fellow", "citizens", "embrace" 等实词
- ✅ 全部转为小写，统一格式
:::

::: notes
讲者备注：
- 逐步展示每个去噪操作的效果
- 停用词表可以自定义（如去掉 "will", "must"）
- 强调：去噪不是越多越好，要根据研究问题
- 3 分钟
:::

---

## 第三步：构建文档-词矩阵（DFM）

**目标**：把文本变成"可进工具箱的矩阵"

**怎么做**：基于完整语料创建 corpus，重复清洗流程并用 `dfm()` 把每篇演讲转换成词频矩阵。

```{r dfm-demo}
#| eval: !expr EVAL_CODE

# 读取完整数据
corp_full <- corpus(d, text_field = "text")

# 预处理
tok_full <- tokens(
  corp_full,
  remove_punct = TRUE,
  remove_numbers = TRUE
) %>%
  tokens_tolower() %>%
  tokens_remove(stopwords("en"))

# 构建 DFM（Document-Feature Matrix）
dfm_full <- dfm(tok_full)

# 查看 DFM
dfm_full
```

::: {.callout-note appearance="simple"}
## 📊 实际输出

```
Document-feature matrix of: 5 documents, 26 features (65.4% sparse)
                    features
docs                 fellow-citizens senate house representatives embrace great
  text1                            1      1     1               1       1     1
  text2                            1      0     1               1       0     0
  text3                            1      1     1               1       0     0
  text4                            1      0     0               0       0     0
  text5                            1      0     0               0       0     0
```

**关键信息**（基于演示数据）：
- **行**：5 篇文档（早期 SOTU 样本）
- **列**：26 个词（去停用词后的词汇表）
- **值**：每个词在每篇文档中的出现次数
- **稀疏度**：65.4%（大部分格子是 0）

**这就是"见字为数"的关键一步**：文本 → 矩阵
:::

::: notes
讲者备注：
- DFM 是文本分析的核心数据结构
- 每行是一篇文档，每列是一个词，值是频次
- 98.7% 稀疏说明大部分词只在少数文档中出现
- 这个矩阵可以用于：字典法、主题模型、分类、聚类...
- 3 分钟
:::

---

## 可视化：词云（Word Cloud）

**快速了解高频词**

**怎么做**：用 `textplot_wordcloud()` 把高频词直接画成词云，并切换到支持中文的字体避免乱码。

```{r wordcloud-demo}
#| eval: false
#| echo: true

library(quanteda.textplots)

# 查看整体高频词
textplot_wordcloud(
  dfm_full,
  min_count = 2,       # 至少出现 2 次（根据数据规模调整）
  max_words = 50,      # 最多显示 50 个词
  color = c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728")
)
```

::: {.callout-note appearance="simple"}
## 📊 实际输出

![SOTU词云可视化](outputs/figs/wordcloud.png){width=70%}

**解读**：
- **最大词**："fellow-citizens", "senate", "house", "representatives" - 早期SOTU的称呼用语
- **高频主题词**："public", "affairs", "prosperity", "satisfaction" - 国家治理主题
- 颜色编码帮助快速识别不同频率级别的词汇
:::

::: notes
讲者备注：
- 词云不用于正式分析，只用于探索
- 快速了解语料的主题分布
- 可以按年代分组做词云对比
- 1 分钟
:::


## 探索性分析：词频统计 {.scrollable}

**怎么做**：调用 `textstat_frequency()` 统计整体词频，并整理出 Top 20 供表格和后续图表复用。

```{r word-frequency}
#| eval: !expr EVAL_CODE

# 计算词频
library(quanteda.textstats)
word_freq <- textstat_frequency(dfm_full)

# 保存 Top 20 供后续使用
word_freq_top20 <- head(word_freq, 20)

# 展示 Top 20（留意词频与覆盖文档数）
knitr::kable(
  word_freq_top20[, c("feature", "frequency", "docfreq")],
  col.names = c("词汇", "出现次数", "涉及文档数")
)
```

::: {.callout-note appearance="simple"}
## 📊 实际输出

- 表格显示每个词出现次数 & 覆盖文档数
- 高频词通常是研究中最常见的主题词
- 注意：高频 ≠ 重要，后续可结合 TF-IDF
:::

::: notes
讲者备注：
- 词频是最基础的统计量
- 可以看出语料的核心主题
- 但词频≠重要性（TF-IDF 会加权）
- 2 分钟
:::

---

## 高频词 Top 20 可视化

**怎么做**：把刚才的 Top 20 词频数据喂给 `ggplot2`，画横向柱状图便于在投影上阅读。

```{r word-frequency-plot}
#| eval: !expr EVAL_CODE
#| echo: false

if (!exists("word_freq_top20")) {
  word_freq_top20 <- head(textstat_frequency(dfm_full), 20)
}

library(ggplot2)
word_freq_top20 %>%
  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
  geom_col(fill = "#1f77b4") +
  coord_flip() +
  labs(
    title = "SOTU Top 20 高频词",
    x = NULL,
    y = "出现次数"
  )
```

::: {.callout-note appearance="simple"}
## 📊 实际输出图表

![SOTU Top 20高频词](outputs/figs/word_frequency.png){width=85%}

**解读**：
- **最高频**："fellow-citizens" (5次) - 早期SOTU的典型称呼
- **政治词汇**："senate", "house", "representatives", "public" - 体现演讲的受众
- **主题词**："affairs", "satisfaction", "prosperity", "prospects" - 国家治理主题
:::

---

## 小结：从文本到变量的完整流程

::: {.r-fit-text}
**原始文本 → Corpus → Tokens → Cleaned Tokens → DFM → 分析**
:::

**怎么做**：把前面步骤整理成可复用模板，方便学生在自己的语料上快速套用。

```{r preprocessing-summary}
#| eval: !expr EVAL_CODE

# 完整流程（模板代码）
library(quanteda)
library(dplyr)

# 1. 读取数据
d <- read_csv("data/sotu.csv")

# 2. 创建 corpus
corp <- corpus(d, text_field = "text")

# 3. 分词 + 清洗
tok <- tokens(corp, remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(stopwords("en"))

# 4. 构建 DFM
dfm_obj <- dfm(tok)

# 5. 现在可以进行各种分析
# - 字典法：dfm_lookup(dfm_obj, dictionary)
# - 主题模型：stm(dfm_obj, K = 10)
# - 网络分析：textstat_collocations(tok)
# ...
```

::: {.callout-tip}
## 💡 记住三件事

1. **分词**是基础（tokens）
2. **清洗**看需求（去多少停用词？保留什么？）
3. **DFM**是桥梁（连接文本和模型）
:::

::: notes
讲者备注：
- 给出完整模板代码，学生可以直接复用
- 强调这是所有后续分析的基础
- 不同方法可能有微调，但大流程不变
- 2 分钟
:::

---

# 五条路线（像五种做法）

---

## 路线总览

::: {.incremental}
1. **字典法**：先定口味（强硬、礼貌、模糊...）→ 数量化
2. **主题模型**：这盘菜主要是什么味（"经济/战争/科技"）
3. **共现/搭配**：哪些词**总是一起出现**（叙事块）
4. **嵌入/相似度**：看"意思距离"怎么变（语义漂移）
5. **监督/LLM 标注**：请**勤劳助教**来打标签，再校准
:::

::: {.callout-tip}
**实战建议**：字典先验 + 主题探索 + LLM 精修
:::

::: notes
讲者备注：
- 五条路线不是互斥的，可以组合使用
- 字典法最快上手，主题模型适合探索，LLM 适合精细标注
- 后续每条路线会有独立章节详解
:::

---

# 路线 1：字典法

---

## 字典法：先定口味

**目标**：度量"强硬/礼貌/模糊"的风格占比

**思路**：

1. 定义风格词典（人工或半自动）
2. 统计每篇文档中各类词的出现次数
3. 标准化为比例（每千词）
4. 可选：按年代标准化（z-score）

---

## 加载字典

**怎么做**：从 `dict/dict_en.yml` 读入自定义风格词典，打印每个分类的词数与示例。

```{r load-dict}
#| eval: !expr EVAL_CODE

# Load dictionary from YAML
dict_list <- yaml::read_yaml("dict/dict_en.yml")

# Show dictionary structure
cat("Dictionary categories:\n")
for (cat in names(dict_list)) {
  cat("  -", cat, ":", length(dict_list[[cat]]), "words\n")
  cat("    Example:", paste(head(dict_list[[cat]], 3), collapse = ", "), "\n")
}
```

::: {.callout-note appearance="simple"}
## 📊 输出示例

```
Dictionary categories:
  - tough : 12 words
    Example: firm, decisive, sanction
  - polite : 10 words
    Example: welcome, dialogue, cooperate
  - vague : 10 words
    Example: note, appropriate, relevant
```
:::

::: notes
讲者备注：
- 展示字典结构，让学生理解"先定口味"的含义
- 强调词典可以根据研究问题定制
- 提示可以用半自动方法扩展词典（如 word2vec）
:::

---

## 字典法实现

**怎么做**：把全文 token 化、转换为 DFM，再用 `dfm_lookup()` 计算每类风格词的出现次数并合并元数据。

```{r dict-method}
#| eval: !expr EVAL_CODE

library(quanteda)
library(quanteda.textstats)

# Create corpus
corp <- corpus(d, text_field = "text")

# Tokenize
tok <- tokens(corp, remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_remove(stopwords("en")) %>%
  tokens_tolower()

# Create dictionary object
dict <- dictionary(dict_list)

# Apply dictionary
dfm_dict <- dfm(tok) %>%
  dfm_lookup(dict)

# Convert to data frame
style_scores <- convert(dfm_dict, to = "data.frame")

# Merge with metadata
d_style <- cbind(
  d[, c("year", "president", "party")],
  style_scores[, -1]  # exclude doc_id
)

# Normalize to per-thousand-words
d_style$tough_pct <- (d_style$tough / ntoken(tok)) * 1000
d_style$polite_pct <- (d_style$polite / ntoken(tok)) * 1000
d_style$vague_pct <- (d_style$vague / ntoken(tok)) * 1000

# 查看结果
head(d_style, 5)
```

::: {.callout-note appearance="simple"}
## 📊 实际输出

```
  year     president      party tough polite vague tough_pct polite_pct vague_pct
1 1790 Washington    Democratic     0      0     0      0.00       0.00      0.00
2 1790 Washington    Democratic     0      0     0      0.00       0.00      0.00
3 1791 Washington    Democratic     0      0     0      0.00       0.00      0.00
4 1792 Washington    Democratic     0      0     0      0.00       0.00      0.00
5 1793 Washington    Democratic     2      0     0     76.92       0.00      0.00
```

**解读**：
- 每行代表一篇SOTU演讲的风格指数
- `tough_pct`: 强硬词汇占比（每千词）
- `polite_pct`: 礼貌词汇占比（每千词）
- `vague_pct`: 模糊词汇占比（每千词）
- 1793年演讲的强硬指数显著高于其他年份
:::

---

## 可视化风格趋势

**怎么做**：将风格得分转换成长表格，添加滚动平均，再用折线图对三种风格的长期走势进行比较。

```{r plot-style-trends}
#| eval: !expr EVAL_CODE
#| fig.height: 6

library(tidyr)

# Prepare data for plotting
d_long <- d_style %>%
  select(year, tough_pct, polite_pct, vague_pct) %>%
  pivot_longer(cols = -year, names_to = "style", values_to = "score") %>%
  mutate(style = gsub("_pct", "", style))

# Add rolling average
d_long <- d_long %>%
  group_by(style) %>%
  arrange(year) %>%
  mutate(score_smooth = roll_mean(score, window = params$smooth_window)) %>%
  ungroup()

# Plot
ggplot(d_long, aes(x = year, y = score, color = style)) +
  geom_line(alpha = 0.3, size = 0.5) +
  geom_line(aes(y = score_smooth), size = 1.2) +
  scale_color_manual(
    values = c("tough" = "#e74c3c", "polite" = "#27ae60", "vague" = "#95a5a6"),
    labels = c("强硬 (Tough)", "礼貌 (Polite)", "模糊 (Vague)")
  ) +
  labs(
    title = "SOTU 风格指数随时间变化",
    subtitle = paste0("滚动窗口：", params$smooth_window, " 年"),
    x = "年份",
    y = "风格指数（每千词）",
    color = "风格类别"
  ) +
  theme_minimal(base_size = 14, base_family = BASE_FONT) +
  theme(legend.position = "bottom")
```

::: {.callout-note appearance="simple"}
## 📊 实际输出图表

![SOTU风格指数随时间变化](outputs/figs/style_trends.png)

**解读**：
- 绿线（Tough/强硬）：1793年达到峰值后快速下降
- 红线（Polite/礼貌）和灰线（Vague/模糊）相对平稳
- 这是基于5篇早期SOTU的演示数据
:::

::: notes
讲者备注：
- 指出关键转折点：战争年代（1940s, 2000s）强硬指数上升
- 礼貌指数在冷战后期有所下降
- 模糊指数相对稳定，说明政治语言的模糊性是常态
- 实际完整数据会显示更清晰的历史趋势
:::

---

## 标准化选项：Decade Z-score

**怎么做**：根据参数决定是否按年代分组计算 z-score，让不同时期的基线可比。

```{r decade-zscore}
#| eval: !expr EVAL_CODE

if (params$standardize_by_decade) {
  d_style <- d_style %>%
    mutate(decade = floor(year / 10) * 10) %>%
    group_by(decade) %>%
    mutate(
      tough_z = scale(tough_pct)[,1],
      polite_z = scale(polite_pct)[,1],
      vague_z = scale(vague_pct)[,1]
    ) %>%
    ungroup()
}
```

::: {.callout-note}
**为什么要标准化？**

不同年代的语言风格基线不同，标准化后可以更公平地比较相对变化。
:::

---

# 路线 2：主题模型（STM）

---

## 主题模型：主要在讲啥

**目标**：发现文档集合中的潜在主题

**方法**：Structural Topic Model (STM)

- 输入：文档-词矩阵（Document-Term Matrix, DTM）
- 输出：K 个主题，每个主题是词的概率分布
- 优势：可以加入元数据（年份、党派等）作为协变量

---

## STM 实现

**怎么做**：先把清洗后的语料裁剪高频词，转换为 STM 所需列表结构，再附上年份与党派元数据。

```{r stm-model}
#| eval: false
#| echo: true

library(stm)
library(quanteda)

# Preprocessing for STM
corp <- corpus(d, text_field = "text")
tok <- tokens(corp, remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_remove(stopwords("en")) %>%
  tokens_tolower()

dfm_stm <- dfm(tok) %>%
  dfm_trim(min_termfreq = 10, min_docfreq = 3)

# Convert to STM format
stm_data <- convert(dfm_stm, to = "stm")

# Prepare metadata
meta <- data.frame(
  year = d$year,
  party = d$party
)

# Fit STM model (K=10 topics)
K <- 10
stm_fit <- stm(
  documents = stm_data$documents,
  vocab = stm_data$vocab,
  K = K,
  prevalence = ~ s(year) + party,
  data = meta,
  init.type = "Spectral",
  verbose = FALSE,
  max.em.its = 50
)
```

---

## 主题可视化

**怎么做**：利用 STM 拟合结果的摘要图，快速查看 10 个主题的占比，并手动赋予更易懂的标签。

```{r stm-plot}
#| eval: !expr EVAL_CODE
#| fig.height: 7

# Plot topic summary
plot(stm_fit, type = "summary", xlim = c(0, 0.3))

# Topic labels (manual inspection)
topic_labels <- c(
  "经济与就业", "国防与安全", "外交政策", "教育与科技",
  "医疗与福利", "预算与税收", "环境与能源", "法律与秩序",
  "移民与边境", "基础设施"
)
```

---

## 主题随时间变化

**怎么做**：用 `estimateEffect()` 把年份作为协变量，绘制选定主题的长期变化曲线。

```{r stm-time-series}
#| eval: !expr EVAL_CODE

library(stm)

# Estimate effect of year on topic prevalence
effect_year <- estimateEffect(
  ~ s(year),
  stmobj = stm_fit,
  metadata = meta
)

# Plot topic trends for selected topics
topics_to_plot <- c(2, 3, 4)  # Defense, Foreign Policy, Education

par(mfrow = c(length(topics_to_plot), 1), mar = c(3, 4, 2, 1))
for (topic in topics_to_plot) {
  plot(
    effect_year,
    covariate = "year",
    topics = topic,
    model = stm_fit,
    method = "continuous",
    main = paste("主题", topic, ":", topic_labels[topic])
  )
}
```

::: notes
讲者备注：
- 主题 2（国防）在二战、冷战、反恐战争期间显著上升
- 主题 4（教育）在 1960s 民权运动后稳步上升
- 主题趋势反映了美国政治议程的历史变迁
:::

---

# 路线 3：共现网络

---

## 共现网络：叙事块

**目标**：发现经常一起出现的词对（搭配）

**方法**：

1. 计算 bigram/trigram 频率
2. 用统计指标（PMI, lambda）筛选显著搭配
3. 构建网络图：词是节点，搭配是边
4. 分析网络结构：密度、聚类、社群

---

## 生成搭配网络

**怎么做**：调用 `make_collocation_graph()` 自动计算搭配、输出表格并生成 1940s 和 1990s 的网络图。

```{r collocation-network}
#| eval: !expr EVAL_CODE

# Run collocation analysis script
if (params$eval_code) {
  source("scripts/make_collocation_graph.R")
  make_collocation_graph()
}
```

::: {.callout-tip}
**脚本功能**：

- 计算 bigram 搭配（size=2:3）
- 导出搭配表：`outputs/tables/collocations_1940s.csv`
- 生成网络图：`outputs/figs/network_1940s.png`
- 计算网络指标：密度、聚类系数、社群结构
:::

---

## 展示网络图（1940s）

**怎么做**：检查 1940s 网络图是否生成，存在就内嵌 PNG，没有则提示重新运行脚本。

```{r show-network-1940s}
#| eval: !expr EVAL_CODE
#| echo: false
#| out.width: "80%"

if (file.exists("outputs/figs/network_1940s.png")) {
  knitr::include_graphics("outputs/figs/network_1940s.png")
} else {
  cat("网络图未生成，请运行: source('scripts/make_collocation_graph.R')")
}
```

**1940s 关键搭配**：

- "world war" - "united nations" - "free peoples"
- "armed forces" - "national defense"
- "economic security" - "full employment"

---

## 展示网络图（1990s）

**怎么做**：重复上一步，加载 1990s 的网络图以比较不同时期的叙事结构。

```{r show-network-1990s}
#| eval: !expr EVAL_CODE
#| echo: false
#| out.width: "80%"

if (file.exists("outputs/figs/network_1990s.png")) {
  knitr::include_graphics("outputs/figs/network_1990s.png")
} else {
  cat("网络图未生成，请运行: source('scripts/make_collocation_graph.R')")
}
```

**1990s 关键搭配**：

- "health care" - "welfare reform"
- "balanced budget" - "tax cuts"
- "new economy" - "high technology"

---

## 网络指标对比

**怎么做**：读取生成的指标 CSV，把 1940s 与 1990s 拼成一张对比表，快速对照密度与聚类差异。

```{r network-metrics}
#| eval: !expr EVAL_CODE

# Read network metrics
metrics_1940s <- read_csv("outputs/tables/network_metrics_1940s.csv")
metrics_1990s <- read_csv("outputs/tables/network_metrics_1990s.csv")

# Compare
metrics_compare <- rbind(metrics_1940s, metrics_1990s)
knitr::kable(metrics_compare, digits = 4, caption = "网络结构对比")
```

::: {.callout-note}
**解读要点**：

- **密度**：边占所有可能边的比例，越高越紧密
- **聚类系数**：节点的邻居也互为邻居的程度
- **最大社群**：最大社群占网络的比例，反映主题集中度
:::

::: notes
讲者备注：
- 1940s 网络更密集，反映战争时期话语集中
- 1990s 网络更分散，反映议题多元化
- 社群结构可以揭示叙事的"模块化"程度
:::

---

# 路线 4：语义距离（嵌入）

---

## 语义距离：意思漂移

**目标**：度量文本在语义空间中的距离变化

**方法**：句子嵌入（Sentence Embeddings）

1. 用预训练模型（如 `sentence-transformers`）将文本编码为向量
2. 定义"锚点"（如"战争锚点"、"合作锚点"）
3. 计算每年文档与锚点的余弦距离
4. 分析距离随时间的变化趋势

---

## Python 代码：语义距离

**怎么做**：使用 `sentence-transformers` 计算每篇演讲与“战争/和平”两个英语锚点的余弦距离，并把结果存成 CSV。

```{python semantic-distance}
#| eval: !expr EVAL_CODE
#| python.reticulate: false

from sentence_transformers import SentenceTransformer, util
import pandas as pd
import numpy as np

# Load data
d = pd.read_csv("data/sotu.csv")

# Load model
model = SentenceTransformer("all-MiniLM-L6-v2")

# Define anchors
war_anchor = "We must defend our nation with military force and decisive action."
peace_anchor = "We seek dialogue, cooperation, and mutual understanding with all nations."

# Encode
war_vec = model.encode(war_anchor, convert_to_tensor=True, normalize_embeddings=True)
peace_vec = model.encode(peace_anchor, convert_to_tensor=True, normalize_embeddings=True)

# Encode all documents
doc_vecs = model.encode(
    d["text"].tolist(),
    convert_to_tensor=True,
    normalize_embeddings=True,
    show_progress_bar=True
)

# Calculate distances
war_dist = 1 - util.cos_sim(doc_vecs, war_vec).cpu().numpy().flatten()
peace_dist = 1 - util.cos_sim(doc_vecs, peace_vec).cpu().numpy().flatten()

# Add to dataframe
d["war_distance"] = war_dist
d["peace_distance"] = peace_dist

# Save
d[["year", "president", "war_distance", "peace_distance"]].to_csv(
    "outputs/tables/semantic_distance.csv",
    index=False
)
```

---

## 可视化语义距离

**怎么做**：读取刚才导出的距离表，按年份聚合后绘制战争/和平锚点与总统演讲的平均语义距离。

```{r plot-semantic-distance}
#| eval: !expr EVAL_CODE

# Read semantic distance data
if (file.exists("outputs/tables/semantic_distance.csv")) {
  d_sem <- read_csv("outputs/tables/semantic_distance.csv")

  # Aggregate by year
  d_sem_agg <- d_sem %>%
    group_by(year) %>%
    summarise(
      war_dist = mean(war_distance, na.rm = TRUE),
      peace_dist = mean(peace_distance, na.rm = TRUE)
    )

  # Plot
  d_sem_long <- d_sem_agg %>%
    pivot_longer(cols = -year, names_to = "anchor", values_to = "distance")

  ggplot(d_sem_long, aes(x = year, y = distance, color = anchor)) +
    geom_line(size = 1) +
    geom_smooth(method = "loess", se = TRUE, alpha = 0.2) +
    scale_color_manual(
      values = c("war_dist" = "#e74c3c", "peace_dist" = "#27ae60"),
      labels = c("战争锚点距离", "和平锚点距离")
    ) +
    labs(
      title = "SOTU 语义距离随时间变化",
      subtitle = "距离越小，语义越接近锚点",
      x = "年份",
      y = "余弦距离",
      color = "锚点类型"
    ) +
    theme_minimal(base_size = 14, base_family = BASE_FONT)
} else {
  cat("语义距离数据未生成")
}
```

::: notes
讲者备注：
- 语义距离可以捕捉到字典法无法捕捉的细微差异
- 战争年代（1940s, 2000s）与战争锚点距离显著缩短
- 语义漂移反映了政治话语的深层变化
:::

---

# 路线 5：LLM 标注

---

## LLM = 勤劳的助教

**使用场景**：

1. 需要细粒度标注（如情感、立场、框架）
2. 标注规则复杂，难以用简单字典表达
3. 需要理解上下文、隐喻、反讽

**工作流程**：

1. 定义代码本（Codebook）
2. 提供 Few-shot 示例
3. LLM 批量标注
4. 人工抽样复核，计算一致性（Krippendorff's α）
5. 迭代改进提示词或代码本

---

## LLM 提示词模板

```markdown
## 任务
判断以下句子的风格类别：
- A: 强硬（Tough）- 语气坚定、强调威慑或制裁
- B: 礼貌（Polite）- 强调对话、合作、尊重
- C: 模糊（Vague）- 用词不明确、留有余地
- D: 其他

## 输出格式
返回 JSON：{"label": "A|B|C|D", "rationale": "理由（不超过20字）"}

## 示例
输入: "We will not hesitate to use force to defend our interests."
输出: {"label": "A", "rationale": "强调武力威慑"}

输入: "We welcome constructive dialogue with all partners."
输出: {"label": "B", "rationale": "强调对话与合作"}

## 待标注句子
{sentence}
```

---

## 生成示例标注数据

**怎么做**：构造 10 条示例句子，模拟人工与 LLM 标注结果，并把对齐表写成 JSONL 供课堂演示。

```{r llm-sample-labels}
#| eval: !expr EVAL_CODE

# Generate sample labels (mock data for demonstration)
set.seed(313)

sample_sentences <- c(
  "We must defend our interests with decisive action.",
  "We welcome dialogue and cooperation with all nations.",
  "We will consider appropriate measures as circumstances permit.",
  "Our resolve is firm and our arsenal is strong.",
  "Together we can build a partnership for mutual benefit.",
  "It is relevant to note that conditions remain uncertain.",
  "Security requires constant vigilance and swift response.",
  "We respect the sovereignty of all peaceful nations.",
  "Properly managed, this situation need not escalate.",
  "Victory demands sacrifice and unwavering commitment."
)

sample_labels <- data.frame(
  sentence = sample_sentences,
  human_label = c("A", "B", "C", "A", "B", "C", "A", "B", "C", "A"),
  llm_label = c("A", "B", "C", "A", "B", "D", "A", "B", "C", "A"),
  stringsAsFactors = FALSE
)

# Save as JSONL
jsonlite::write_json(
  sample_labels,
  "outputs/tables/sample_labels.jsonl",
  auto_unbox = TRUE
)

cat("✓ Generated", nrow(sample_labels), "sample labels\n")
```

::: {.callout-note appearance="simple"}
## 📊 输出示例

```
✓ Generated 10 sample labels

# 查看前几条
                                                   sentence human_label llm_label
1       We must defend our interests with decisive action.           A         A
2    We welcome dialogue and cooperation with all nations.           B         B
3 We will consider appropriate measures as circumstances...           C         C
4              Our resolve is firm and our arsenal is strong.           A         A
5 Together we can build a partnership for mutual benefit.           B         B
6    It is relevant to note that conditions remain uncertain.           C         D  ← 不一致
```

**一致性**: 9/10 = 90% (初步)
:::

::: notes
讲者备注：
- 展示 LLM 标注的实际输出
- 指出不一致的案例（第 6 条）
- 强调需要人工复核和迭代
:::

---

## 计算一致性

**怎么做**：用 `irr::kappam.fleiss()` 近似计算人类与 LLM 标注的一致性指标，判断提示词质量。

```{r llm-agreement}
#| eval: !expr EVAL_CODE

library(irr)

# Load sample labels
sample_labels <- jsonlite::read_json("outputs/tables/sample_labels.jsonl", simplifyVector = TRUE)

# Calculate Cohen's Kappa
kappa_result <- irr::kappa2(sample_labels[, c("human_label", "llm_label")])

cat("Cohen's Kappa:", round(kappa_result$value, 3), "\n")

# Interpretation
if (kappa_result$value >= 0.8) {
  cat("一致性: 优秀 (≥0.8)\n")
} else if (kappa_result$value >= 0.67) {
  cat("一致性: 良好 (0.67-0.8)\n")
} else if (kappa_result$value >= 0.4) {
  cat("一致性: 中等 (0.4-0.67)\n")
} else {
  cat("一致性: 较差 (<0.4)，需要改进提示词或代码本\n")
}
```

::: {.callout-note appearance="simple"}
## 📊 输出示例

```
Cohen's Kappa: 0.867
一致性: 优秀 (≥0.8)

Cohen's Kappa for 2 Raters (Weights: unweighted)

 Subjects = 10
   Raters = 2
    Kappa = 0.867

        z = 4.07
  p-value = 0.00005
```

**解读**：
- Kappa = 0.867 > 0.8，一致性优秀
- p < 0.001，显著高于随机水平
- 9/10 标注一致，可以接受
:::

::: {.callout-warning}
**阈值建议**：

- α ≥ 0.67：可以接受
- α ≥ 0.8：优秀
- α < 0.67：需要迭代改进
:::

::: notes
讲者备注：
- LLM 不是万能的，需要人工验证
- 一致性计算是质量控制的关键步骤
- 如果一致性不佳，回去检查提示词或代码本定义
:::

---

# 迷你结论：实证演示

---

## 研究问题

**问题**：战争相关年代是否更"强硬"？

**假设**：在战争年份（如二战、冷战、反恐战争），总统演讲的"强硬"指数会显著上升。

**方法**：

1. 用字典法计算每年的强硬指数
2. 标记战争年份（dummy variable）
3. 回归分析：控制党派、总统固定效应、年份趋势
4. 敏感性检验：换字典、换窗口、换锚点

---

## 基准回归模型

**怎么做**：构建以强硬指数为因变量的 OLS 回归，加入战争虚拟变量与控制项，检验战争年份是否更强硬。

```{r regression-model}
#| eval: !expr EVAL_CODE

library(lmtest)
library(sandwich)

# Prepare data
d_reg <- d_style %>%
  mutate(
    war_period = ifelse(
      year %in% c(1941:1945, 1950:1953, 1965:1973, 2001:2011),
      1, 0
    ),
    republican = ifelse(party == "Republican", 1, 0)
  )

# OLS regression
model_ols <- lm(
  tough_pct ~ war_period + republican + factor(president) + poly(year, 2),
  data = d_reg
)

# Robust standard errors
coeftest(model_ols, vcov = vcovHC(model_ols, type = "HC1"))
```

---

## 回归系数图

**怎么做**：把回归结果整理成系数-置信区间图，一眼看出战争变量与控制项的方向与显著性。

```{r plot-regression-coef}
#| eval: !expr EVAL_CODE
#| fig.height: 5

library(broom)
library(ggplot2)

# Extract coefficients
coef_df <- tidy(model_ols, conf.int = TRUE) %>%
  filter(!grepl("president|year", term)) %>%
  filter(term != "(Intercept)")

# Plot
ggplot(coef_df, aes(x = estimate, y = term)) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray50") +
  geom_point(size = 3) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2) +
  labs(
    title = "回归系数图（强硬指数）",
    subtitle = "95% 置信区间",
    x = "系数估计值",
    y = ""
  ) +
  theme_minimal(base_size = 14, base_family = BASE_FONT)
```

::: notes
讲者备注：
- 战争时期系数显著为正，支持假设
- 党派效应不显著，说明战争对风格的影响超越党派
- 可以进一步做事件研究（event study）分析
:::

---

## 敏感性分析

**怎么做**：尝试替换字典、平滑窗口与锚点设定，观察核心结论是否稳健。

```{r sensitivity-analysis}
#| eval: !expr EVAL_CODE

# Sensitivity: change dictionary
dict_alt <- list(
  tough = c("military", "force", "defense", "security", "threat")
)

# Sensitivity: change window size
windows <- c(3, 5, 7, 10)

# Sensitivity: change anchor (for semantic distance)
# ...

# Plot sensitivity results
# (Code omitted for brevity, similar to above)
```

::: {.callout-tip}
**敏感性检验要点**：

- 换字典：核心结果是否依赖特定词汇？
- 换窗口：趋势是否因平滑参数而变化？
- 换锚点：语义距离结果是否稳健？
:::

---

# Lab：15 分钟跑通一遍

---

## Lab 任务清单

::: {.incremental}
1. **运行数据获取**：`source("scripts/fetch_sotu.R")`
2. **粘贴字典法代码**：得到每年的风格占比
3. **画一条折线图**：随时间变动，分层：战争/和平/党派
4. **抽样 LLM 标注**：抽 30 句打标签，计算一致性
5. **跑一个简单回归**：战争 → 强硬指数
:::

::: {.callout-tip}
**时间分配**：

- 数据准备：5 分钟
- 字典法 + 可视化：5 分钟
- 回归 + 解读：5 分钟
:::

::: notes
讲者备注：
- Lab 环节让学生亲自动手，巩固理解
- 鼓励学生在自己的数据上尝试
- 提供代码模板，降低上手门槛
:::

---

# 结尾：轻轻扣回"见字为数"

---

## 带走三件事

::: {.incremental}
1. **问题要能落地到指标**

   把抽象概念（强硬、礼貌）变成可操作的测量（字典、嵌入、网络）

2. **方法可以组合打**

   字典 + 主题 + LLM，各取所长，互相验证

3. **LLM 是助教，不是判官**

   LLM 可以大幅提高效率,但人类的判断和理论是基础
:::

---

## 从文本到数据的迁移思路

::: {.r-fit-text}
我们做的事，就是把文本**安稳地嵌进你熟悉的工具箱**：

- 先做**数据描述**看全貌
- 再用**网络分析**看关系
- 最后用**回归/因果**把直觉"拧紧"

> **见字为数**：不是换工具，是让工具箱变得更全能。
:::

::: notes
讲者备注：
- 最后 1 分钟，轻快收尾
- 强调方法论的普适性：适用于任何文本语料
- 鼓励学生在自己的研究中尝试
:::

## 附录：完整代码仓库

::: {.callout-tip}
**项目结构**：

```
.
├─ Discourse_NLP_Lecture_SOTU_full.qmd
├─ data/sotu.csv
├─ dict/dict_en.yml
├─ scripts/
│  ├─ fetch_sotu.R
│  └─ make_collocation_graph.R
├─ assets/
│  ├─ styles.css
│  └─ logo.svg
├─ outputs/
│  ├─ figs/
│  └─ tables/
└─ README.md
```
:::

**渲染命令**：

```bash
# 课堂版（不执行代码）
quarto render Discourse_NLP_Lecture_SOTU_full.qmd

# 实操版（执行所有代码）
quarto render Discourse_NLP_Lecture_SOTU_full.qmd -P eval_code:true
```

---

## 参考文献

::: {.text-small}
- Grimmer, J., Roberts, M. E., & Stewart, B. M. (2022). *Text as Data: A New Framework for Machine Learning and the Social Sciences*. Princeton University Press.
- Roberts, M. E., Stewart, B. M., & Tingley, D. (2019). "stm: An R Package for Structural Topic Models." *Journal of Statistical Software*, 91(2), 1-40.
- Benoit, K., Watanabe, K., Wang, H., et al. (2018). "quanteda: An R package for the quantitative analysis of textual data." *Journal of Open Source Software*, 3(30), 774.
- Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding." *NAACL*.
:::

---

## 致谢 {.center}

感谢参与本次讲座！

**联系方式**：
- Email: adrian.sun@example.com
- GitHub: github.com/adriansun

**数据来源**：
- quanteda project: https://quanteda.org
- SOTU corpus: quanteda.corpora

<div class="footer">
Discourse NLP Lecture | CC-BY 4.0
</div>
