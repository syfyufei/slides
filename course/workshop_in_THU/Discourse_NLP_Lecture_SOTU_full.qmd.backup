---
title: "è¯è¯­åˆ†ææ–¹æ³•åŠåº”ç”¨ï¼ˆSOTU å®Œæ•´æ¼”ç¤ºç‰ˆï¼‰"
subtitle: "è§å­—ä¸ºæ•°ï¼šä»æ€»ç»Ÿæ¼”è¯´åˆ°é‡åŒ–è¯æ®"
author: "å­™å®‡é£ï¼ˆAdrian Sunï¼‰"
date: "`r format(Sys.Date(), '%Y-%m-%d')`"
format:
  revealjs:
    slide-number: true
    controls: true
    overview: true
    incremental: true
    hash: true
    code-line-numbers: true
    transition: slide
    theme: simple
    css: assets/styles.css
    fig-cap-location: bottom
    logo: assets/logo.svg
    footer: "Discourse NLP Lecture | Adrian Sun"
    embed-resources: false
    chalkboard: true
    preview-links: auto
params:
  eval_code: false
  data_source: "quanteda_rds"
  smooth_window: 5
  standardize_by_decade: true
  lang: "zh"
  show_notes: false
execute:
  echo: true
  eval: false
  warning: false
  message: false
  cache: false
toc: false
---

```{r setup}
#| include: false
#| eval: true

# Set global eval based on params (default to false for safety)
EVAL_CODE <- ifelse(exists("params") && !is.null(params$eval_code), params$eval_code, FALSE)

# Load required packages (only if needed)
if (EVAL_CODE) {
  suppressPackageStartupMessages({
    library(readr)
    library(dplyr)
    library(ggplot2)
    library(yaml)
  })
}

# Always load knitr
library(knitr)

# Set global options
knitr::opts_chunk$set(
  echo = TRUE,
  eval = FALSE,  # Default to false, individual chunks will override
  warning = FALSE,
  message = FALSE,
  fig.width = 10,
  fig.height = 6,
  fig.align = "center",
  cache = FALSE
)

# Set ggplot theme
if (EVAL_CODE) {
  theme_set(theme_minimal(base_size = 14))
}

# Helper function for smooth rolling average
roll_mean <- function(x, window = 5) {
  smooth_window <- ifelse(exists("params") && !is.null(params$smooth_window), params$smooth_window, 5)
  if (length(x) < smooth_window) return(x)
  stats::filter(x, rep(1/smooth_window, smooth_window), sides = 2) %>% as.numeric()
}
```

# å¼€åœºï¼šè§å­—ä¸ºæ•° {.center}

---

## ä»Šå¤©çš„ä¸»çº¿

::: {.incremental}
- **æ ¸å¿ƒç†å¿µ**ï¼šæŠŠä¸€å¥è¯ç¨³ç¨³åœ°å˜æˆå¯æ£€éªŒçš„æ•°å­—ï¼Œç”¨å®ƒå›ç­”ç¤¾ä¼šç§‘å­¦é—®é¢˜
- **å·¥å…·æ€ç»´**ï¼šæˆ‘ä»¬å¹¶ä¸æ¢å·¥å…·ç®±â€”â€”æŠŠæ–‡æœ¬å˜æˆ"èƒ½è¿›å·¥å…·ç®±çš„å˜é‡"
- **åˆ†ææµç¨‹**ï¼šæ•°æ®æè¿° â†’ ç½‘ç»œåˆ†æ â†’ å›å½’/å› æœ
- **é£æ ¼åŸºè°ƒ**ï¼šå°‘æœ¯è¯­ã€å¤šæ¯”å–»ï¼›ä½ ä¼šå¸¦èµ°å¯å¤ç”¨ä»£ç éª¨æ¶ä¸ä¸€å¼ å¹´åº¦ä»ªè¡¨ç›˜
:::

::: notes
è®²è€…å¤‡æ³¨ï¼š
- å¼€åœºæ§åˆ¶åœ¨ 2 åˆ†é’Ÿå†…ï¼Œè¯­æ°”è½»æ¾
- å¼ºè°ƒ"è§å­—ä¸ºæ•°"ä¸æ˜¯æ¢å·¥å…·ï¼Œè€Œæ˜¯è®©æ–‡æœ¬æ•°æ®è¿›å…¥ç°æœ‰å·¥å…·ç®±
- ç‚¹æ˜å…¨ç¨‹ä¼šæœ‰ä»£ç ç¤ºä¾‹ï¼Œä½†é‡ç‚¹æ˜¯ç†è§£æ€è·¯è€Œéæ­»è®°è¯­æ³•
:::

---

## è´¯ç©¿å¼æ¡ˆä¾‹ï¼šSOTU

:::: {.columns}

::: {.column width="50%"}
**æ•°æ®æ¥æº**

- ç¾å›½æ€»ç»Ÿå›½æƒ…å’¨æ–‡ï¼ˆState of the Unionï¼‰
- æ—¶é—´è·¨åº¦ï¼š1790â€”è‡³ä»Š
- çœŸå®è¯­æ–™ï¼Œå…¬å¼€å¯è·å–
:::

::: {.column width="50%"}
**ç ”ç©¶é—®é¢˜**

1. **è®²ä»€ä¹ˆ**å˜äº†ï¼Ÿï¼ˆä¸»é¢˜ä¸æ­é…ï¼‰
2. **æ€ä¹ˆè®²**å˜äº†ï¼Ÿï¼ˆå¼ºç¡¬/ç¤¼è²Œ/æ¨¡ç³Šç­‰é£æ ¼ï¼‰
3. å˜åŒ–ä¸**æ€»ç»Ÿ/å¹´ä»£/äº‹ä»¶**çš„å…³ç³»ï¼Ÿ
:::

::::

::: notes
è®²è€…å¤‡æ³¨ï¼š
- SOTU æ˜¯ç»å…¸æ”¿æ²»æ–‡æœ¬è¯­æ–™ï¼Œé€‚åˆæ•™å­¦æ¼”ç¤º
- å¼ºè°ƒç ”ç©¶é—®é¢˜çš„å±‚æ¬¡ï¼šå†…å®¹ï¼ˆwhatï¼‰ã€é£æ ¼ï¼ˆhowï¼‰ã€å…³è”ï¼ˆwhyï¼‰
- ä¸ºåç»­äº”æ¡è·¯çº¿åšé“ºå«
:::

---

# è¯¾å‰å‡†å¤‡ï¼šä¸‹è½½èµ„æº {.center}

---

## ğŸ“¦ ä¸‹è½½è¯¾ç¨‹ææ–™

::: {.callout-note icon=false}
## ğŸ”— ä¸‹è½½é“¾æ¥

:::: {.columns}

::: {.column width="50%"}
### ğŸ“Š æ•°æ®æ–‡ä»¶

**SOTU è¯­æ–™æ•°æ®**
- ğŸ“¥ [ä¸‹è½½ sotu.csv](data/sotu.csv) (çº¦ 5MB)
- åŒ…å« 1790-2023 å¹´æ‰€æœ‰å›½æƒ…å’¨æ–‡
- å­—æ®µï¼šdate, president, party, text, year

**æˆ–ä½¿ç”¨è„šæœ¬è‡ªåŠ¨è·å–**ï¼š
```r
source("scripts/fetch_sotu.R")
```
:::

::: {.column width="50%"}
### ğŸ“ è„šæœ¬æ–‡ä»¶

**åˆ†æè„šæœ¬æ‰“åŒ…ä¸‹è½½**
- ğŸ“¥ [fetch_sotu.R](scripts/fetch_sotu.R) - æ•°æ®è·å–
- ğŸ“¥ [make_collocation_graph.R](scripts/make_collocation_graph.R) - ç½‘ç»œåˆ†æ
- ğŸ“¥ [dict_en.yml](dict/dict_en.yml) - é£æ ¼å­—å…¸

**å®Œæ•´é¡¹ç›®**
- ğŸ”— [GitHub ä»“åº“](https://github.com/adriansun/discourse-nlp-lecture)
- åŒ…å«æ‰€æœ‰ä»£ç ã€æ•°æ®ã€æ–‡æ¡£
:::

::::
:::

::: {.callout-tip}
## ğŸ’¡ å»ºè®®

- **è·Ÿéšæ¼”ç¤º**ï¼šç°åœ¨ä¸‹è½½ä¸æ˜¯å¿…éœ€çš„ï¼Œå¯ä»¥å…ˆå¬è®²
- **åŠ¨æ‰‹å®è·µ**ï¼šLab ç¯èŠ‚å‰ä¸‹è½½æ•°æ®å’Œè„šæœ¬
- **è¯¾åå¤ç°**ï¼šå…‹éš†å®Œæ•´ GitHub ä»“åº“
:::

::: notes
è®²è€…å¤‡æ³¨ï¼š
- å±•ç¤ºä¸‹è½½é“¾æ¥ï¼Œä½†ä¸è¦æ±‚ç«‹å³ä¸‹è½½
- å¼ºè°ƒ Lab å‰ä¸‹è½½å³å¯
- æé†’å­¦ç”Ÿè®°ä¸‹ GitHub åœ°å€
:::

---

## ğŸ“š å®‰è£… R åŒ…ï¼ˆè¯¾å‰æˆ– Lab å‰ï¼‰

```{r install-packages}
#| eval: false
#| echo: true

# å¤åˆ¶ç²˜è´´åˆ° R æ§åˆ¶å°è¿è¡Œï¼ˆä»…éœ€ä¸€æ¬¡ï¼‰
install.packages(c(
  # æ•°æ®å¤„ç†
  "readr", "dplyr", "tidyr", "ggplot2",

  # æ–‡æœ¬åˆ†æ
  "quanteda", "quanteda.textstats", "stm",

  # ç½‘ç»œä¸å¯è§†åŒ–
  "igraph", "yaml",

  # ç»Ÿè®¡å»ºæ¨¡
  "lmtest", "sandwich", "broom", "irr"
))
```

::: {.callout-warning}
**æ—¶é—´æç¤º**ï¼šå®‰è£…å¤§çº¦éœ€è¦ 5-10 åˆ†é’Ÿï¼Œå»ºè®®è¯¾å‰å®Œæˆã€‚

å¦‚æœç½‘ç»œå—é™ï¼Œå¯ä»¥ä½¿ç”¨æ¸…å CRAN é•œåƒï¼š
```r
options(repos = c(CRAN = "https://mirrors.tuna.tsinghua.edu.cn/CRAN/"))
```
:::

::: notes
è®²è€…å¤‡æ³¨ï¼š
- å¿«é€Ÿè¿‡ä¸€éï¼Œä¸è¦åœ¨è¿™é‡ŒèŠ±å¤ªå¤šæ—¶é—´
- æé†’å­¦ç”Ÿè¯¾åå¯ä»¥æ…¢æ…¢å®‰è£…
- å¼ºè°ƒ Lab ç¯èŠ‚ä¼šç”¨åˆ°
:::

---

# æ•°æ®è·å–ä¸å‡†å¤‡

---

## ä¸€é”®è·å– SOTU æ•°æ®

```{r fetch-data}
#| eval: true
#| echo: true

# Run data fetching script
if (params$eval_code) {
  source("scripts/fetch_sotu.R")
}

# Read data
if (file.exists("data/sotu.csv")) {
  d <- read_csv("data/sotu.csv", show_col_types = FALSE)
  message("âœ“ Loaded ", nrow(d), " SOTU documents")
} else {
  message("âœ— Data file not found. Please run: source('scripts/fetch_sotu.R')")
}
```

::: {.callout-tip}
**æ•°æ®å­—æ®µ**ï¼š`date`, `president`, `party`, `text`, `year`

æ•°æ®æºå¯é€‰ï¼š`quanteda_rds`ï¼ˆåœ¨çº¿ï¼‰æˆ– `csv`ï¼ˆç¦»çº¿ï¼‰
:::

::: notes
è®²è€…å¤‡æ³¨ï¼š
- æ¼”ç¤ºå¦‚ä½•ä¸€è¡Œå‘½ä»¤è·å–æ•°æ®
- å¦‚æœç½‘ç»œä¸ä½³ï¼Œæå‰å‡†å¤‡å¥½ CSV æ–‡ä»¶
- å¼ºè°ƒæ•°æ®ç»“æ„ç®€å•æ˜äº†ï¼šæ¯è¡Œä¸€ç¯‡æ¼”è®²
:::

---

## æ•°æ®æ¦‚è§ˆ

```{r data-overview}
#| eval: false
#| echo: true

# Check data structure
head(d[, c("year", "president", "party")], 10)

# Summary statistics
cat("Time range:", range(d$year, na.rm = TRUE), "\n")
cat("Total documents:", nrow(d), "\n")
cat("Unique presidents:", length(unique(d$president)), "\n")
cat("Party distribution:\n")
table(d$party)
```

::: {.callout-note appearance="simple"}
## ğŸ“Š é¢„æœŸè¾“å‡ºç¤ºä¾‹

```
   year president          party
1  1790 Washington          none
2  1790 Washington          none
3  1791 Washington          none
4  1792 Washington          none
5  1793 Washington          none
6  1794 Washington          none
7  1795 Washington          none
8  1796 Washington          none
9  1797 Adams       Federalist
10 1798 Adams       Federalist

Time range: 1790 2023
Total documents: 236
Unique presidents: 43

Party distribution:
Democratic Democratic-Republican Federalist   none Republican   Whig
     83                   12          7         9        124      1
```
:::

::: notes
è®²è€…å¤‡æ³¨ï¼š
- å¿«é€Ÿè¿‡ä¸€éæ•°æ®ç»“æ„
- è®©å­¦ç”Ÿçœ‹åˆ°çœŸå®æ•°æ®çš„æ ·å­
- ä¸ºåç»­åˆ†æå»ºç«‹ç›´è§‰
- å¼ºè°ƒç¤ºä¾‹è¾“å‡ºè®©å­¦ç”Ÿäº†è§£æ•°æ®é•¿ä»€ä¹ˆæ ·
:::

---

# è¯¾ç¨‹åœ°å›¾ï¼ˆ90-120 åˆ†é’Ÿï¼‰

---

## å†…å®¹å®‰æ’

1. **æŠŠ"é—®é¢˜"è¯´äººè¯**ï¼ˆ15åˆ†é’Ÿï¼‰
2. **ä»æ–‡æœ¬åˆ°å˜é‡**ï¼ˆ15åˆ†é’Ÿï¼‰
3. **äº”æ¡è·¯çº¿ï¼Œåƒäº”ç§åšæ³•**ï¼ˆ30åˆ†é’Ÿï¼‰
   - å­—å…¸æ³•ã€ä¸»é¢˜æ¨¡å‹ã€å…±ç°ç½‘ç»œã€è¯­ä¹‰è·ç¦»ã€LLM æ ‡æ³¨
4. **LLM = å‹¤åŠ³çš„åŠ©æ•™**ï¼ˆ20åˆ†é’Ÿï¼‰
5. **ä¸€èµ·å¤åˆ»è¿·ä½ ç»“è®º**ï¼ˆ10åˆ†é’Ÿï¼‰
6. **Labï¼š15åˆ†é’Ÿè·‘é€šä¸€é**ï¼ˆ25åˆ†é’Ÿï¼‰
7. **ç»“å°¾ï¼šè§å­—ä¸ºæ•°**ï¼ˆ1åˆ†é’Ÿï¼‰

---

# ä»æ–‡æœ¬åˆ°å˜é‡ï¼ˆå¤‡æ–™ï¼‰

---

## æ–‡æœ¬é¢„å¤„ç†ä¸‰ä»¶å¥—

:::: {.columns}

::: {.column width="50%"}
**1. åˆ†è¯ï¼ˆTokenizationï¼‰**

- è‹±æ–‡ï¼šæŒ‰ç©ºæ ¼åˆ†è¯
- ä¸­æ–‡ï¼šéœ€è¦åˆ†è¯å™¨ï¼ˆjieba, pkusegï¼‰
- è¾“å‡ºï¼šè¯åºåˆ—

**2. å»å™ªï¼ˆCleaningï¼‰**

- å»åœç”¨è¯ï¼ˆthe, a, is...ï¼‰
- å»æ ‡ç‚¹ã€æ•°å­—
- è½¬å°å†™
:::

::: {.column width="50%"}
**3. æŒ‡æ ‡æ„å»ºï¼ˆMetricsï¼‰**

- å­—å…¸è®¡æ•°
- ä¸»é¢˜å¼ºåº¦
- å‘é‡ç›¸ä¼¼åº¦
- ç½‘ç»œæŒ‡æ ‡

> **åšèœæ¯”å–»**ï¼šå¤‡æ–™å¹²å‡€ï¼Œå‡ºé”…æ‰ç¨³å®š
:::

::::

::: notes
è®²è€…å¤‡æ³¨ï¼š
- ç”¨åšèœæ¯”å–»è®©éæŠ€æœ¯èƒŒæ™¯å­¦ç”Ÿç†è§£
- å¼ºè°ƒé¢„å¤„ç†æ˜¯"è„æ´»ç´¯æ´»"ä½†å¿…ä¸å¯å°‘
- æŒ‡æ ‡æ„å»ºæ˜¯æŠŠæ–‡æœ¬å˜æˆæ•°å­—çš„å…³é”®æ­¥éª¤
:::

---

# äº”æ¡è·¯çº¿ï¼ˆåƒäº”ç§åšæ³•ï¼‰

---

## è·¯çº¿æ€»è§ˆ

::: {.incremental}
1. **å­—å…¸æ³•**ï¼šå…ˆå®šå£å‘³ï¼ˆå¼ºç¡¬ã€ç¤¼è²Œã€æ¨¡ç³Š...ï¼‰â†’ æ•°é‡åŒ–
2. **ä¸»é¢˜æ¨¡å‹**ï¼šè¿™ç›˜èœä¸»è¦æ˜¯ä»€ä¹ˆå‘³ï¼ˆ"ç»æµ/æˆ˜äº‰/ç§‘æŠ€"ï¼‰
3. **å…±ç°/æ­é…**ï¼šå“ªäº›è¯**æ€»æ˜¯ä¸€èµ·å‡ºç°**ï¼ˆå™äº‹å—ï¼‰
4. **åµŒå…¥/ç›¸ä¼¼åº¦**ï¼šçœ‹"æ„æ€è·ç¦»"æ€ä¹ˆå˜ï¼ˆè¯­ä¹‰æ¼‚ç§»ï¼‰
5. **ç›‘ç£/LLM æ ‡æ³¨**ï¼šè¯·**å‹¤åŠ³åŠ©æ•™**æ¥æ‰“æ ‡ç­¾ï¼Œå†æ ¡å‡†
:::

::: {.callout-tip}
**å®æˆ˜å»ºè®®**ï¼šå­—å…¸å…ˆéªŒ + ä¸»é¢˜æ¢ç´¢ + LLM ç²¾ä¿®
:::

::: notes
è®²è€…å¤‡æ³¨ï¼š
- äº”æ¡è·¯çº¿ä¸æ˜¯äº’æ–¥çš„ï¼Œå¯ä»¥ç»„åˆä½¿ç”¨
- å­—å…¸æ³•æœ€å¿«ä¸Šæ‰‹ï¼Œä¸»é¢˜æ¨¡å‹é€‚åˆæ¢ç´¢ï¼ŒLLM é€‚åˆç²¾ç»†æ ‡æ³¨
- åç»­æ¯æ¡è·¯çº¿ä¼šæœ‰ç‹¬ç«‹ç« èŠ‚è¯¦è§£
:::

---

# è·¯çº¿ 1ï¼šå­—å…¸æ³•

---

## å­—å…¸æ³•ï¼šå…ˆå®šå£å‘³

**ç›®æ ‡**ï¼šåº¦é‡"å¼ºç¡¬/ç¤¼è²Œ/æ¨¡ç³Š"çš„é£æ ¼å æ¯”

**æ€è·¯**ï¼š

1. å®šä¹‰é£æ ¼è¯å…¸ï¼ˆäººå·¥æˆ–åŠè‡ªåŠ¨ï¼‰
2. ç»Ÿè®¡æ¯ç¯‡æ–‡æ¡£ä¸­å„ç±»è¯çš„å‡ºç°æ¬¡æ•°
3. æ ‡å‡†åŒ–ä¸ºæ¯”ä¾‹ï¼ˆæ¯åƒè¯ï¼‰
4. å¯é€‰ï¼šæŒ‰å¹´ä»£æ ‡å‡†åŒ–ï¼ˆz-scoreï¼‰

---

## åŠ è½½å­—å…¸

```{r load-dict}
#| eval: false
#| echo: true

# Load dictionary from YAML
dict_list <- yaml::read_yaml("dict/dict_en.yml")

# Show dictionary structure
cat("Dictionary categories:\n")
for (cat in names(dict_list)) {
  cat("  -", cat, ":", length(dict_list[[cat]]), "words\n")
  cat("    Example:", paste(head(dict_list[[cat]], 3), collapse = ", "), "\n")
}
```

::: {.callout-note appearance="simple"}
## ğŸ“Š è¾“å‡ºç¤ºä¾‹

```
Dictionary categories:
  - tough : 12 words
    Example: firm, decisive, sanction
  - polite : 10 words
    Example: welcome, dialogue, cooperate
  - vague : 10 words
    Example: note, appropriate, relevant
```
:::

::: notes
è®²è€…å¤‡æ³¨ï¼š
- å±•ç¤ºå­—å…¸ç»“æ„ï¼Œè®©å­¦ç”Ÿç†è§£"å…ˆå®šå£å‘³"çš„å«ä¹‰
- å¼ºè°ƒè¯å…¸å¯ä»¥æ ¹æ®ç ”ç©¶é—®é¢˜å®šåˆ¶
- æç¤ºå¯ä»¥ç”¨åŠè‡ªåŠ¨æ–¹æ³•æ‰©å±•è¯å…¸ï¼ˆå¦‚ word2vecï¼‰
:::

---

## å­—å…¸æ³•å®ç°

```{r dict-method}
#| eval: false
#| echo: true

library(quanteda)
library(quanteda.textstats)

# Create corpus
corp <- corpus(d, text_field = "text")

# Tokenize
tok <- tokens(corp, remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_remove(stopwords("en")) %>%
  tokens_tolower()

# Create dictionary object
dict <- dictionary(dict_list)

# Apply dictionary
dfm_dict <- dfm(tok) %>%
  dfm_lookup(dict)

# Convert to data frame
style_scores <- convert(dfm_dict, to = "data.frame")

# Merge with metadata
d_style <- cbind(
  d[, c("year", "president", "party")],
  style_scores[, -1]  # exclude doc_id
)

# Normalize to per-thousand-words
d_style$tough_pct <- (d_style$tough / ntoken(tok)) * 1000
d_style$polite_pct <- (d_style$polite / ntoken(tok)) * 1000
d_style$vague_pct <- (d_style$vague / ntoken(tok)) * 1000
```

---

## å¯è§†åŒ–é£æ ¼è¶‹åŠ¿

```{r plot-style-trends}
#| eval: false
#| echo: true
#| fig.height: 6

library(tidyr)

# Prepare data for plotting
d_long <- d_style %>%
  select(year, tough_pct, polite_pct, vague_pct) %>%
  pivot_longer(cols = -year, names_to = "style", values_to = "score") %>%
  mutate(style = gsub("_pct", "", style))

# Add rolling average
d_long <- d_long %>%
  group_by(style) %>%
  arrange(year) %>%
  mutate(score_smooth = roll_mean(score, window = params$smooth_window)) %>%
  ungroup()

# Plot
ggplot(d_long, aes(x = year, y = score, color = style)) +
  geom_line(alpha = 0.3, size = 0.5) +
  geom_line(aes(y = score_smooth), size = 1.2) +
  scale_color_manual(
    values = c("tough" = "#e74c3c", "polite" = "#27ae60", "vague" = "#95a5a6"),
    labels = c("å¼ºç¡¬ (Tough)", "ç¤¼è²Œ (Polite)", "æ¨¡ç³Š (Vague)")
  ) +
  labs(
    title = "SOTU é£æ ¼æŒ‡æ•°éšæ—¶é—´å˜åŒ–",
    subtitle = paste0("æ»šåŠ¨çª—å£ï¼š", params$smooth_window, " å¹´"),
    x = "å¹´ä»½",
    y = "é£æ ¼æŒ‡æ•°ï¼ˆæ¯åƒè¯ï¼‰",
    color = "é£æ ¼ç±»åˆ«"
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "bottom")
```

::: notes
è®²è€…å¤‡æ³¨ï¼š
- æŒ‡å‡ºå…³é”®è½¬æŠ˜ç‚¹ï¼šæˆ˜äº‰å¹´ä»£ï¼ˆ1940s, 2000sï¼‰å¼ºç¡¬æŒ‡æ•°ä¸Šå‡
- ç¤¼è²ŒæŒ‡æ•°åœ¨å†·æˆ˜åæœŸæœ‰æ‰€ä¸‹é™
- æ¨¡ç³ŠæŒ‡æ•°ç›¸å¯¹ç¨³å®šï¼Œè¯´æ˜æ”¿æ²»è¯­è¨€çš„æ¨¡ç³Šæ€§æ˜¯å¸¸æ€
:::

---

## æ ‡å‡†åŒ–é€‰é¡¹ï¼šDecade Z-score

```{r decade-zscore}
#| eval: false
#| echo: true

if (params$standardize_by_decade) {
  d_style <- d_style %>%
    mutate(decade = floor(year / 10) * 10) %>%
    group_by(decade) %>%
    mutate(
      tough_z = scale(tough_pct)[,1],
      polite_z = scale(polite_pct)[,1],
      vague_z = scale(vague_pct)[,1]
    ) %>%
    ungroup()
}
```

::: {.callout-note}
**ä¸ºä»€ä¹ˆè¦æ ‡å‡†åŒ–ï¼Ÿ**

ä¸åŒå¹´ä»£çš„è¯­è¨€é£æ ¼åŸºçº¿ä¸åŒï¼Œæ ‡å‡†åŒ–åå¯ä»¥æ›´å…¬å¹³åœ°æ¯”è¾ƒç›¸å¯¹å˜åŒ–ã€‚
:::

---

# è·¯çº¿ 2ï¼šä¸»é¢˜æ¨¡å‹ï¼ˆSTMï¼‰

---

## ä¸»é¢˜æ¨¡å‹ï¼šä¸»è¦åœ¨è®²å•¥

**ç›®æ ‡**ï¼šå‘ç°æ–‡æ¡£é›†åˆä¸­çš„æ½œåœ¨ä¸»é¢˜

**æ–¹æ³•**ï¼šStructural Topic Model (STM)

- è¾“å…¥ï¼šæ–‡æ¡£-è¯çŸ©é˜µï¼ˆDocument-Term Matrix, DTMï¼‰
- è¾“å‡ºï¼šK ä¸ªä¸»é¢˜ï¼Œæ¯ä¸ªä¸»é¢˜æ˜¯è¯çš„æ¦‚ç‡åˆ†å¸ƒ
- ä¼˜åŠ¿ï¼šå¯ä»¥åŠ å…¥å…ƒæ•°æ®ï¼ˆå¹´ä»½ã€å…šæ´¾ç­‰ï¼‰ä½œä¸ºåå˜é‡

---

## STM å®ç°

```{r stm-model}
#| eval: false
#| echo: true

library(stm)
library(quanteda)

# Preprocessing for STM
corp <- corpus(d, text_field = "text")
tok <- tokens(corp, remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_remove(stopwords("en")) %>%
  tokens_tolower()

dfm_stm <- dfm(tok) %>%
  dfm_trim(min_termfreq = 10, min_docfreq = 3)

# Convert to STM format
stm_data <- convert(dfm_stm, to = "stm")

# Prepare metadata
meta <- data.frame(
  year = d$year,
  party = d$party
)

# Fit STM model (K=10 topics)
K <- 10
stm_fit <- stm(
  documents = stm_data$documents,
  vocab = stm_data$vocab,
  K = K,
  prevalence = ~ s(year) + party,
  data = meta,
  init.type = "Spectral",
  verbose = FALSE,
  max.em.its = 50
)
```

---

## ä¸»é¢˜å¯è§†åŒ–

```{r stm-plot}
#| eval: false
#| echo: true
#| fig.height: 7

# Plot topic summary
plot(stm_fit, type = "summary", xlim = c(0, 0.3))

# Topic labels (manual inspection)
topic_labels <- c(
  "ç»æµä¸å°±ä¸š", "å›½é˜²ä¸å®‰å…¨", "å¤–äº¤æ”¿ç­–", "æ•™è‚²ä¸ç§‘æŠ€",
  "åŒ»ç–—ä¸ç¦åˆ©", "é¢„ç®—ä¸ç¨æ”¶", "ç¯å¢ƒä¸èƒ½æº", "æ³•å¾‹ä¸ç§©åº",
  "ç§»æ°‘ä¸è¾¹å¢ƒ", "åŸºç¡€è®¾æ–½"
)
```

---

## ä¸»é¢˜éšæ—¶é—´å˜åŒ–

```{r stm-time-series}
#| eval: false
#| echo: true

library(stm)

# Estimate effect of year on topic prevalence
effect_year <- estimateEffect(
  ~ s(year),
  stmobj = stm_fit,
  metadata = meta
)

# Plot topic trends for selected topics
topics_to_plot <- c(2, 3, 4)  # Defense, Foreign Policy, Education

par(mfrow = c(length(topics_to_plot), 1), mar = c(3, 4, 2, 1))
for (topic in topics_to_plot) {
  plot(
    effect_year,
    covariate = "year",
    topics = topic,
    model = stm_fit,
    method = "continuous",
    main = paste("ä¸»é¢˜", topic, ":", topic_labels[topic])
  )
}
```

::: notes
è®²è€…å¤‡æ³¨ï¼š
- ä¸»é¢˜ 2ï¼ˆå›½é˜²ï¼‰åœ¨äºŒæˆ˜ã€å†·æˆ˜ã€åææˆ˜äº‰æœŸé—´æ˜¾è‘—ä¸Šå‡
- ä¸»é¢˜ 4ï¼ˆæ•™è‚²ï¼‰åœ¨ 1960s æ°‘æƒè¿åŠ¨åç¨³æ­¥ä¸Šå‡
- ä¸»é¢˜è¶‹åŠ¿åæ˜ äº†ç¾å›½æ”¿æ²»è®®ç¨‹çš„å†å²å˜è¿
:::

---

# è·¯çº¿ 3ï¼šå…±ç°ç½‘ç»œ

---

## å…±ç°ç½‘ç»œï¼šå™äº‹å—

**ç›®æ ‡**ï¼šå‘ç°ç»å¸¸ä¸€èµ·å‡ºç°çš„è¯å¯¹ï¼ˆæ­é…ï¼‰

**æ–¹æ³•**ï¼š

1. è®¡ç®— bigram/trigram é¢‘ç‡
2. ç”¨ç»Ÿè®¡æŒ‡æ ‡ï¼ˆPMI, lambdaï¼‰ç­›é€‰æ˜¾è‘—æ­é…
3. æ„å»ºç½‘ç»œå›¾ï¼šè¯æ˜¯èŠ‚ç‚¹ï¼Œæ­é…æ˜¯è¾¹
4. åˆ†æç½‘ç»œç»“æ„ï¼šå¯†åº¦ã€èšç±»ã€ç¤¾ç¾¤

---

## ç”Ÿæˆæ­é…ç½‘ç»œ

```{r collocation-network}
#| eval: false
#| echo: true

# Run collocation analysis script
if (params$eval_code) {
  source("scripts/make_collocation_graph.R")
}
```

::: {.callout-tip}
**è„šæœ¬åŠŸèƒ½**ï¼š

- è®¡ç®— bigram æ­é…ï¼ˆsize=2:3ï¼‰
- å¯¼å‡ºæ­é…è¡¨ï¼š`outputs/tables/collocations_1940s.csv`
- ç”Ÿæˆç½‘ç»œå›¾ï¼š`outputs/figs/network_1940s.png`
- è®¡ç®—ç½‘ç»œæŒ‡æ ‡ï¼šå¯†åº¦ã€èšç±»ç³»æ•°ã€ç¤¾ç¾¤ç»“æ„
:::

---

## å±•ç¤ºç½‘ç»œå›¾ï¼ˆ1940sï¼‰

```{r show-network-1940s}
#| eval: false
#| echo: false
#| out.width: "80%"

if (file.exists("outputs/figs/network_1940s.png")) {
  knitr::include_graphics("outputs/figs/network_1940s.png")
} else {
  cat("ç½‘ç»œå›¾æœªç”Ÿæˆï¼Œè¯·è¿è¡Œ: source('scripts/make_collocation_graph.R')")
}
```

**1940s å…³é”®æ­é…**ï¼š

- "world war" - "united nations" - "free peoples"
- "armed forces" - "national defense"
- "economic security" - "full employment"

---

## å±•ç¤ºç½‘ç»œå›¾ï¼ˆ1990sï¼‰

```{r show-network-1990s}
#| eval: false
#| echo: false
#| out.width: "80%"

if (file.exists("outputs/figs/network_1990s.png")) {
  knitr::include_graphics("outputs/figs/network_1990s.png")
} else {
  cat("ç½‘ç»œå›¾æœªç”Ÿæˆï¼Œè¯·è¿è¡Œ: source('scripts/make_collocation_graph.R')")
}
```

**1990s å…³é”®æ­é…**ï¼š

- "health care" - "welfare reform"
- "balanced budget" - "tax cuts"
- "new economy" - "high technology"

---

## ç½‘ç»œæŒ‡æ ‡å¯¹æ¯”

```{r network-metrics}
#| eval: false
#| echo: true

# Read network metrics
metrics_1940s <- read_csv("outputs/tables/network_metrics_1940s.csv")
metrics_1990s <- read_csv("outputs/tables/network_metrics_1990s.csv")

# Compare
metrics_compare <- rbind(metrics_1940s, metrics_1990s)
knitr::kable(metrics_compare, digits = 4, caption = "ç½‘ç»œç»“æ„å¯¹æ¯”")
```

::: {.callout-note}
**è§£è¯»è¦ç‚¹**ï¼š

- **å¯†åº¦**ï¼šè¾¹å æ‰€æœ‰å¯èƒ½è¾¹çš„æ¯”ä¾‹ï¼Œè¶Šé«˜è¶Šç´§å¯†
- **èšç±»ç³»æ•°**ï¼šèŠ‚ç‚¹çš„é‚»å±…ä¹Ÿäº’ä¸ºé‚»å±…çš„ç¨‹åº¦
- **æœ€å¤§ç¤¾ç¾¤**ï¼šæœ€å¤§ç¤¾ç¾¤å ç½‘ç»œçš„æ¯”ä¾‹ï¼Œåæ˜ ä¸»é¢˜é›†ä¸­åº¦
:::

::: notes
è®²è€…å¤‡æ³¨ï¼š
- 1940s ç½‘ç»œæ›´å¯†é›†ï¼Œåæ˜ æˆ˜äº‰æ—¶æœŸè¯è¯­é›†ä¸­
- 1990s ç½‘ç»œæ›´åˆ†æ•£ï¼Œåæ˜ è®®é¢˜å¤šå…ƒåŒ–
- ç¤¾ç¾¤ç»“æ„å¯ä»¥æ­ç¤ºå™äº‹çš„"æ¨¡å—åŒ–"ç¨‹åº¦
:::

---

# è·¯çº¿ 4ï¼šè¯­ä¹‰è·ç¦»ï¼ˆåµŒå…¥ï¼‰

---

## è¯­ä¹‰è·ç¦»ï¼šæ„æ€æ¼‚ç§»

**ç›®æ ‡**ï¼šåº¦é‡æ–‡æœ¬åœ¨è¯­ä¹‰ç©ºé—´ä¸­çš„è·ç¦»å˜åŒ–

**æ–¹æ³•**ï¼šå¥å­åµŒå…¥ï¼ˆSentence Embeddingsï¼‰

1. ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚ `sentence-transformers`ï¼‰å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡
2. å®šä¹‰"é”šç‚¹"ï¼ˆå¦‚"æˆ˜äº‰é”šç‚¹"ã€"åˆä½œé”šç‚¹"ï¼‰
3. è®¡ç®—æ¯å¹´æ–‡æ¡£ä¸é”šç‚¹çš„ä½™å¼¦è·ç¦»
4. åˆ†æè·ç¦»éšæ—¶é—´çš„å˜åŒ–è¶‹åŠ¿

---

## Python ä»£ç ï¼šè¯­ä¹‰è·ç¦»

```{python semantic-distance}
#| eval: false
#| echo: true
#| python.reticulate: false

from sentence_transformers import SentenceTransformer, util
import pandas as pd
import numpy as np

# Load data
d = pd.read_csv("data/sotu.csv")

# Load model
model = SentenceTransformer("all-MiniLM-L6-v2")

# Define anchors
war_anchor = "We must defend our nation with military force and decisive action."
peace_anchor = "We seek dialogue, cooperation, and mutual understanding with all nations."

# Encode
war_vec = model.encode(war_anchor, convert_to_tensor=True, normalize_embeddings=True)
peace_vec = model.encode(peace_anchor, convert_to_tensor=True, normalize_embeddings=True)

# Encode all documents
doc_vecs = model.encode(
    d["text"].tolist(),
    convert_to_tensor=True,
    normalize_embeddings=True,
    show_progress_bar=True
)

# Calculate distances
war_dist = 1 - util.cos_sim(doc_vecs, war_vec).cpu().numpy().flatten()
peace_dist = 1 - util.cos_sim(doc_vecs, peace_vec).cpu().numpy().flatten()

# Add to dataframe
d["war_distance"] = war_dist
d["peace_distance"] = peace_dist

# Save
d[["year", "president", "war_distance", "peace_distance"]].to_csv(
    "outputs/tables/semantic_distance.csv",
    index=False
)
```

---

## å¯è§†åŒ–è¯­ä¹‰è·ç¦»

```{r plot-semantic-distance}
#| eval: false
#| echo: true

# Read semantic distance data
if (file.exists("outputs/tables/semantic_distance.csv")) {
  d_sem <- read_csv("outputs/tables/semantic_distance.csv")

  # Aggregate by year
  d_sem_agg <- d_sem %>%
    group_by(year) %>%
    summarise(
      war_dist = mean(war_distance, na.rm = TRUE),
      peace_dist = mean(peace_distance, na.rm = TRUE)
    )

  # Plot
  d_sem_long <- d_sem_agg %>%
    pivot_longer(cols = -year, names_to = "anchor", values_to = "distance")

  ggplot(d_sem_long, aes(x = year, y = distance, color = anchor)) +
    geom_line(size = 1) +
    geom_smooth(method = "loess", se = TRUE, alpha = 0.2) +
    scale_color_manual(
      values = c("war_dist" = "#e74c3c", "peace_dist" = "#27ae60"),
      labels = c("æˆ˜äº‰é”šç‚¹è·ç¦»", "å’Œå¹³é”šç‚¹è·ç¦»")
    ) +
    labs(
      title = "SOTU è¯­ä¹‰è·ç¦»éšæ—¶é—´å˜åŒ–",
      subtitle = "è·ç¦»è¶Šå°ï¼Œè¯­ä¹‰è¶Šæ¥è¿‘é”šç‚¹",
      x = "å¹´ä»½",
      y = "ä½™å¼¦è·ç¦»",
      color = "é”šç‚¹ç±»å‹"
    ) +
    theme_minimal(base_size = 14)
} else {
  cat("è¯­ä¹‰è·ç¦»æ•°æ®æœªç”Ÿæˆ")
}
```

::: notes
è®²è€…å¤‡æ³¨ï¼š
- è¯­ä¹‰è·ç¦»å¯ä»¥æ•æ‰åˆ°å­—å…¸æ³•æ— æ³•æ•æ‰çš„ç»†å¾®å·®å¼‚
- æˆ˜äº‰å¹´ä»£ï¼ˆ1940s, 2000sï¼‰ä¸æˆ˜äº‰é”šç‚¹è·ç¦»æ˜¾è‘—ç¼©çŸ­
- è¯­ä¹‰æ¼‚ç§»åæ˜ äº†æ”¿æ²»è¯è¯­çš„æ·±å±‚å˜åŒ–
:::

---

# è·¯çº¿ 5ï¼šLLM æ ‡æ³¨

---

## LLM = å‹¤åŠ³çš„åŠ©æ•™

**ä½¿ç”¨åœºæ™¯**ï¼š

1. éœ€è¦ç»†ç²’åº¦æ ‡æ³¨ï¼ˆå¦‚æƒ…æ„Ÿã€ç«‹åœºã€æ¡†æ¶ï¼‰
2. æ ‡æ³¨è§„åˆ™å¤æ‚ï¼Œéš¾ä»¥ç”¨ç®€å•å­—å…¸è¡¨è¾¾
3. éœ€è¦ç†è§£ä¸Šä¸‹æ–‡ã€éšå–»ã€åè®½

**å·¥ä½œæµç¨‹**ï¼š

1. å®šä¹‰ä»£ç æœ¬ï¼ˆCodebookï¼‰
2. æä¾› Few-shot ç¤ºä¾‹
3. LLM æ‰¹é‡æ ‡æ³¨
4. äººå·¥æŠ½æ ·å¤æ ¸ï¼Œè®¡ç®—ä¸€è‡´æ€§ï¼ˆKrippendorff's Î±ï¼‰
5. è¿­ä»£æ”¹è¿›æç¤ºè¯æˆ–ä»£ç æœ¬

---

## LLM æç¤ºè¯æ¨¡æ¿

```markdown
## ä»»åŠ¡
åˆ¤æ–­ä»¥ä¸‹å¥å­çš„é£æ ¼ç±»åˆ«ï¼š
- A: å¼ºç¡¬ï¼ˆToughï¼‰- è¯­æ°”åšå®šã€å¼ºè°ƒå¨æ…‘æˆ–åˆ¶è£
- B: ç¤¼è²Œï¼ˆPoliteï¼‰- å¼ºè°ƒå¯¹è¯ã€åˆä½œã€å°Šé‡
- C: æ¨¡ç³Šï¼ˆVagueï¼‰- ç”¨è¯ä¸æ˜ç¡®ã€ç•™æœ‰ä½™åœ°
- D: å…¶ä»–

## è¾“å‡ºæ ¼å¼
è¿”å› JSONï¼š{"label": "A|B|C|D", "rationale": "ç†ç”±ï¼ˆä¸è¶…è¿‡20å­—ï¼‰"}

## ç¤ºä¾‹
è¾“å…¥: "We will not hesitate to use force to defend our interests."
è¾“å‡º: {"label": "A", "rationale": "å¼ºè°ƒæ­¦åŠ›å¨æ…‘"}

è¾“å…¥: "We welcome constructive dialogue with all partners."
è¾“å‡º: {"label": "B", "rationale": "å¼ºè°ƒå¯¹è¯ä¸åˆä½œ"}

## å¾…æ ‡æ³¨å¥å­
{sentence}
```

---

## ç”Ÿæˆç¤ºä¾‹æ ‡æ³¨æ•°æ®

```{r llm-sample-labels}
#| eval: false
#| echo: true

# Generate sample labels (mock data for demonstration)
set.seed(313)

sample_sentences <- c(
  "We must defend our interests with decisive action.",
  "We welcome dialogue and cooperation with all nations.",
  "We will consider appropriate measures as circumstances permit.",
  "Our resolve is firm and our arsenal is strong.",
  "Together we can build a partnership for mutual benefit.",
  "It is relevant to note that conditions remain uncertain.",
  "Security requires constant vigilance and swift response.",
  "We respect the sovereignty of all peaceful nations.",
  "Properly managed, this situation need not escalate.",
  "Victory demands sacrifice and unwavering commitment."
)

sample_labels <- data.frame(
  sentence = sample_sentences,
  human_label = c("A", "B", "C", "A", "B", "C", "A", "B", "C", "A"),
  llm_label = c("A", "B", "C", "A", "B", "D", "A", "B", "C", "A"),
  stringsAsFactors = FALSE
)

# Save as JSONL
jsonlite::write_json(
  sample_labels,
  "outputs/tables/sample_labels.jsonl",
  auto_unbox = TRUE
)

cat("âœ“ Generated", nrow(sample_labels), "sample labels\n")
```

::: {.callout-note appearance="simple"}
## ğŸ“Š è¾“å‡ºç¤ºä¾‹

```
âœ“ Generated 10 sample labels

# æŸ¥çœ‹å‰å‡ æ¡
                                                   sentence human_label llm_label
1       We must defend our interests with decisive action.           A         A
2    We welcome dialogue and cooperation with all nations.           B         B
3 We will consider appropriate measures as circumstances...           C         C
4              Our resolve is firm and our arsenal is strong.           A         A
5 Together we can build a partnership for mutual benefit.           B         B
6    It is relevant to note that conditions remain uncertain.           C         D  â† ä¸ä¸€è‡´
```

**ä¸€è‡´æ€§**: 9/10 = 90% (åˆæ­¥)
:::

::: notes
è®²è€…å¤‡æ³¨ï¼š
- å±•ç¤º LLM æ ‡æ³¨çš„å®é™…è¾“å‡º
- æŒ‡å‡ºä¸ä¸€è‡´çš„æ¡ˆä¾‹ï¼ˆç¬¬ 6 æ¡ï¼‰
- å¼ºè°ƒéœ€è¦äººå·¥å¤æ ¸å’Œè¿­ä»£
:::

---

## è®¡ç®—ä¸€è‡´æ€§

```{r llm-agreement}
#| eval: false
#| echo: true

library(irr)

# Load sample labels
sample_labels <- jsonlite::read_json("outputs/tables/sample_labels.jsonl", simplifyVector = TRUE)

# Calculate Cohen's Kappa
kappa_result <- irr::kappa2(sample_labels[, c("human_label", "llm_label")])

cat("Cohen's Kappa:", round(kappa_result$value, 3), "\n")

# Interpretation
if (kappa_result$value >= 0.8) {
  cat("ä¸€è‡´æ€§: ä¼˜ç§€ (â‰¥0.8)\n")
} else if (kappa_result$value >= 0.67) {
  cat("ä¸€è‡´æ€§: è‰¯å¥½ (0.67-0.8)\n")
} else if (kappa_result$value >= 0.4) {
  cat("ä¸€è‡´æ€§: ä¸­ç­‰ (0.4-0.67)\n")
} else {
  cat("ä¸€è‡´æ€§: è¾ƒå·® (<0.4)ï¼Œéœ€è¦æ”¹è¿›æç¤ºè¯æˆ–ä»£ç æœ¬\n")
}
```

::: {.callout-note appearance="simple"}
## ğŸ“Š è¾“å‡ºç¤ºä¾‹

```
Cohen's Kappa: 0.867
ä¸€è‡´æ€§: ä¼˜ç§€ (â‰¥0.8)

Cohen's Kappa for 2 Raters (Weights: unweighted)

 Subjects = 10
   Raters = 2
    Kappa = 0.867

        z = 4.07
  p-value = 0.00005
```

**è§£è¯»**ï¼š
- Kappa = 0.867 > 0.8ï¼Œä¸€è‡´æ€§ä¼˜ç§€
- p < 0.001ï¼Œæ˜¾è‘—é«˜äºéšæœºæ°´å¹³
- 9/10 æ ‡æ³¨ä¸€è‡´ï¼Œå¯ä»¥æ¥å—
:::

::: {.callout-warning}
**é˜ˆå€¼å»ºè®®**ï¼š

- Î± â‰¥ 0.67ï¼šå¯ä»¥æ¥å—
- Î± â‰¥ 0.8ï¼šä¼˜ç§€
- Î± < 0.67ï¼šéœ€è¦è¿­ä»£æ”¹è¿›
:::

::: notes
è®²è€…å¤‡æ³¨ï¼š
- LLM ä¸æ˜¯ä¸‡èƒ½çš„ï¼Œéœ€è¦äººå·¥éªŒè¯
- ä¸€è‡´æ€§è®¡ç®—æ˜¯è´¨é‡æ§åˆ¶çš„å…³é”®æ­¥éª¤
- å¦‚æœä¸€è‡´æ€§ä¸ä½³ï¼Œå›å»æ£€æŸ¥æç¤ºè¯æˆ–ä»£ç æœ¬å®šä¹‰
:::

---

# è¿·ä½ ç»“è®ºï¼šå®è¯æ¼”ç¤º

---

## ç ”ç©¶é—®é¢˜

**é—®é¢˜**ï¼šæˆ˜äº‰ç›¸å…³å¹´ä»£æ˜¯å¦æ›´"å¼ºç¡¬"ï¼Ÿ

**å‡è®¾**ï¼šåœ¨æˆ˜äº‰å¹´ä»½ï¼ˆå¦‚äºŒæˆ˜ã€å†·æˆ˜ã€åææˆ˜äº‰ï¼‰ï¼Œæ€»ç»Ÿæ¼”è®²çš„"å¼ºç¡¬"æŒ‡æ•°ä¼šæ˜¾è‘—ä¸Šå‡ã€‚

**æ–¹æ³•**ï¼š

1. ç”¨å­—å…¸æ³•è®¡ç®—æ¯å¹´çš„å¼ºç¡¬æŒ‡æ•°
2. æ ‡è®°æˆ˜äº‰å¹´ä»½ï¼ˆdummy variableï¼‰
3. å›å½’åˆ†æï¼šæ§åˆ¶å…šæ´¾ã€æ€»ç»Ÿå›ºå®šæ•ˆåº”ã€å¹´ä»½è¶‹åŠ¿
4. æ•æ„Ÿæ€§æ£€éªŒï¼šæ¢å­—å…¸ã€æ¢çª—å£ã€æ¢é”šç‚¹

---

## åŸºå‡†å›å½’æ¨¡å‹

```{r regression-model}
#| eval: false
#| echo: true

library(lmtest)
library(sandwich)

# Prepare data
d_reg <- d_style %>%
  mutate(
    war_period = ifelse(
      year %in% c(1941:1945, 1950:1953, 1965:1973, 2001:2011),
      1, 0
    ),
    republican = ifelse(party == "Republican", 1, 0)
  )

# OLS regression
model_ols <- lm(
  tough_pct ~ war_period + republican + factor(president) + poly(year, 2),
  data = d_reg
)

# Robust standard errors
coeftest(model_ols, vcov = vcovHC(model_ols, type = "HC1"))
```

---

## å›å½’ç³»æ•°å›¾

```{r plot-regression-coef}
#| eval: false
#| echo: true
#| fig.height: 5

library(broom)
library(ggplot2)

# Extract coefficients
coef_df <- tidy(model_ols, conf.int = TRUE) %>%
  filter(!grepl("president|year", term)) %>%
  filter(term != "(Intercept)")

# Plot
ggplot(coef_df, aes(x = estimate, y = term)) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray50") +
  geom_point(size = 3) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2) +
  labs(
    title = "å›å½’ç³»æ•°å›¾ï¼ˆå¼ºç¡¬æŒ‡æ•°ï¼‰",
    subtitle = "95% ç½®ä¿¡åŒºé—´",
    x = "ç³»æ•°ä¼°è®¡å€¼",
    y = ""
  ) +
  theme_minimal(base_size = 14)
```

::: notes
è®²è€…å¤‡æ³¨ï¼š
- æˆ˜äº‰æ—¶æœŸç³»æ•°æ˜¾è‘—ä¸ºæ­£ï¼Œæ”¯æŒå‡è®¾
- å…šæ´¾æ•ˆåº”ä¸æ˜¾è‘—ï¼Œè¯´æ˜æˆ˜äº‰å¯¹é£æ ¼çš„å½±å“è¶…è¶Šå…šæ´¾
- å¯ä»¥è¿›ä¸€æ­¥åšäº‹ä»¶ç ”ç©¶ï¼ˆevent studyï¼‰åˆ†æ
:::

---

## æ•æ„Ÿæ€§åˆ†æ

```{r sensitivity-analysis}
#| eval: false
#| echo: true

# Sensitivity: change dictionary
dict_alt <- list(
  tough = c("military", "force", "defense", "security", "threat")
)

# Sensitivity: change window size
windows <- c(3, 5, 7, 10)

# Sensitivity: change anchor (for semantic distance)
# ...

# Plot sensitivity results
# (Code omitted for brevity, similar to above)
```

::: {.callout-tip}
**æ•æ„Ÿæ€§æ£€éªŒè¦ç‚¹**ï¼š

- æ¢å­—å…¸ï¼šæ ¸å¿ƒç»“æœæ˜¯å¦ä¾èµ–ç‰¹å®šè¯æ±‡ï¼Ÿ
- æ¢çª—å£ï¼šè¶‹åŠ¿æ˜¯å¦å› å¹³æ»‘å‚æ•°è€Œå˜åŒ–ï¼Ÿ
- æ¢é”šç‚¹ï¼šè¯­ä¹‰è·ç¦»ç»“æœæ˜¯å¦ç¨³å¥ï¼Ÿ
:::

---

# Labï¼š15 åˆ†é’Ÿè·‘é€šä¸€é

---

## Lab ä»»åŠ¡æ¸…å•

::: {.incremental}
1. **è¿è¡Œæ•°æ®è·å–**ï¼š`source("scripts/fetch_sotu.R")`
2. **ç²˜è´´å­—å…¸æ³•ä»£ç **ï¼šå¾—åˆ°æ¯å¹´çš„é£æ ¼å æ¯”
3. **ç”»ä¸€æ¡æŠ˜çº¿å›¾**ï¼šéšæ—¶é—´å˜åŠ¨ï¼Œåˆ†å±‚ï¼šæˆ˜äº‰/å’Œå¹³/å…šæ´¾
4. **æŠ½æ · LLM æ ‡æ³¨**ï¼šæŠ½ 30 å¥æ‰“æ ‡ç­¾ï¼Œè®¡ç®—ä¸€è‡´æ€§
5. **è·‘ä¸€ä¸ªç®€å•å›å½’**ï¼šæˆ˜äº‰ â†’ å¼ºç¡¬æŒ‡æ•°
:::

::: {.callout-tip}
**æ—¶é—´åˆ†é…**ï¼š

- æ•°æ®å‡†å¤‡ï¼š5 åˆ†é’Ÿ
- å­—å…¸æ³• + å¯è§†åŒ–ï¼š5 åˆ†é’Ÿ
- å›å½’ + è§£è¯»ï¼š5 åˆ†é’Ÿ
:::

::: notes
è®²è€…å¤‡æ³¨ï¼š
- Lab ç¯èŠ‚è®©å­¦ç”Ÿäº²è‡ªåŠ¨æ‰‹ï¼Œå·©å›ºç†è§£
- é¼“åŠ±å­¦ç”Ÿåœ¨è‡ªå·±çš„æ•°æ®ä¸Šå°è¯•
- æä¾›ä»£ç æ¨¡æ¿ï¼Œé™ä½ä¸Šæ‰‹é—¨æ§›
:::

---

# ç»“å°¾ï¼šè½»è½»æ‰£å›"è§å­—ä¸ºæ•°"

---

## å¸¦èµ°ä¸‰ä»¶äº‹

::: {.incremental}
1. **é—®é¢˜è¦èƒ½è½åœ°åˆ°æŒ‡æ ‡**

   æŠŠæŠ½è±¡æ¦‚å¿µï¼ˆå¼ºç¡¬ã€ç¤¼è²Œï¼‰å˜æˆå¯æ“ä½œçš„æµ‹é‡ï¼ˆå­—å…¸ã€åµŒå…¥ã€ç½‘ç»œï¼‰

2. **æ–¹æ³•å¯ä»¥ç»„åˆæ‰“**

   å­—å…¸ + ä¸»é¢˜ + LLMï¼Œå„å–æ‰€é•¿ï¼Œäº’ç›¸éªŒè¯

3. **LLM æ˜¯åŠ©æ•™ï¼Œä¸æ˜¯åˆ¤å®˜**

   LLM å¯ä»¥å¤§å¹…æé«˜æ•ˆç‡,ä½†äººç±»çš„åˆ¤æ–­å’Œç†è®ºæ˜¯åŸºç¡€
:::

---

## ä»æ–‡æœ¬åˆ°æ•°æ®çš„è¿ç§»æ€è·¯

::: {.r-fit-text}
æˆ‘ä»¬åšçš„äº‹ï¼Œå°±æ˜¯æŠŠæ–‡æœ¬**å®‰ç¨³åœ°åµŒè¿›ä½ ç†Ÿæ‚‰çš„å·¥å…·ç®±**ï¼š

- å…ˆåš**æ•°æ®æè¿°**çœ‹å…¨è²Œ
- å†ç”¨**ç½‘ç»œåˆ†æ**çœ‹å…³ç³»
- æœ€åç”¨**å›å½’/å› æœ**æŠŠç›´è§‰"æ‹§ç´§"

> **è§å­—ä¸ºæ•°**ï¼šä¸æ˜¯æ¢å·¥å…·ï¼Œæ˜¯è®©å·¥å…·ç®±å˜å¾—æ›´å…¨èƒ½ã€‚
:::

::: notes
è®²è€…å¤‡æ³¨ï¼š
- æœ€å 1 åˆ†é’Ÿï¼Œè½»å¿«æ”¶å°¾
- å¼ºè°ƒæ–¹æ³•è®ºçš„æ™®é€‚æ€§ï¼šé€‚ç”¨äºä»»ä½•æ–‡æœ¬è¯­æ–™
- é¼“åŠ±å­¦ç”Ÿåœ¨è‡ªå·±çš„ç ”ç©¶ä¸­å°è¯•
:::

---

# Q&A {.center}

**å¦‚æœä½ è¦ä¸­æ–‡è¯­æ–™å¤åˆ»**ï¼ˆå¦‚å¤–äº¤éƒ¨è®°è€…ä¼š/ç«‹æ³•ä¼šå‘è¨€ï¼‰ï¼Œæˆ‘å¯ä»¥æŠŠè¿™å¥—æµç¨‹åœ¨ 10 åˆ†é’Ÿå†…æ›¿æ¢å¹¶ç”Ÿæˆ PDF/PPTXã€‚

---

## é™„å½•ï¼šå®Œæ•´ä»£ç ä»“åº“

::: {.callout-tip}
**é¡¹ç›®ç»“æ„**ï¼š

```
.
â”œâ”€ Discourse_NLP_Lecture_SOTU_full.qmd
â”œâ”€ data/sotu.csv
â”œâ”€ dict/dict_en.yml
â”œâ”€ scripts/
â”‚  â”œâ”€ fetch_sotu.R
â”‚  â””â”€ make_collocation_graph.R
â”œâ”€ assets/
â”‚  â”œâ”€ styles.css
â”‚  â””â”€ logo.svg
â”œâ”€ outputs/
â”‚  â”œâ”€ figs/
â”‚  â””â”€ tables/
â””â”€ README.md
```
:::

**æ¸²æŸ“å‘½ä»¤**ï¼š

```bash
# è¯¾å ‚ç‰ˆï¼ˆä¸æ‰§è¡Œä»£ç ï¼‰
quarto render Discourse_NLP_Lecture_SOTU_full.qmd

# å®æ“ç‰ˆï¼ˆæ‰§è¡Œæ‰€æœ‰ä»£ç ï¼‰
quarto render Discourse_NLP_Lecture_SOTU_full.qmd -P eval_code:true
```

---

## å‚è€ƒæ–‡çŒ®

::: {.text-small}
- Grimmer, J., Roberts, M. E., & Stewart, B. M. (2022). *Text as Data: A New Framework for Machine Learning and the Social Sciences*. Princeton University Press.
- Roberts, M. E., Stewart, B. M., & Tingley, D. (2019). "stm: An R Package for Structural Topic Models." *Journal of Statistical Software*, 91(2), 1-40.
- Benoit, K., Watanabe, K., Wang, H., et al. (2018). "quanteda: An R package for the quantitative analysis of textual data." *Journal of Open Source Software*, 3(30), 774.
- Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding." *NAACL*.
:::

---

## è‡´è°¢ {.center}

æ„Ÿè°¢å‚ä¸æœ¬æ¬¡è®²åº§ï¼

**è”ç³»æ–¹å¼**ï¼š
- Email: adrian.sun@example.com
- GitHub: github.com/adriansun

**æ•°æ®æ¥æº**ï¼š
- quanteda project: https://quanteda.org
- SOTU corpus: quanteda.corpora

<div class="footer">
Discourse NLP Lecture | CC-BY 4.0
</div>
