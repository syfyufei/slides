---
title: "网络爬虫进阶"
subtitle: "R workshop Lecture6"
author: "Sun Yufei"
institute: "Department of Political Science, Tsinghua University"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    self_contained: yes
    lib_dir: libs
    css:
      - default
      - zh-CN_custom.css
      - styles.css
    
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    nature:
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
      ratio: 21:9


---

## Outline

- 初窥门径：爬虫基础知识

- 利刃出鞘：我们的爬虫工具箱

- 小试牛刀：如何使用八爪鱼来抓取网页新闻信息

- 小试牛刀：如何使用R抓取政府网页


---
class: inverse, center, middle

# 初窥门径：爬虫基础知识

---

## 什么是爬虫

在广袤的互联网中，有这样一种"爬虫生物"，穿梭于万维网中，将承载信息的网页吞食，然后交由搜索引擎进行转化，吸收，并最终"孵化"出结构化的数据，供人快速查找，展示。

这种"生物"，其名曰"网络蜘蛛"（又被称为网页蜘蛛，网络机器人）。网络蜘蛛虽以数据为食，但是数据的生产者-网站，也需要借助爬虫的帮助，将网页提交给搜索引擎。

```{r out.width = "30%", echo = FALSE}
knitr::include_graphics("http://www.chipscoco.com/zb_users/upload/2021/02/202102051612492579435384.jpg")
```
---

## 为什么要爬虫

- 社会科学研究需要的数据更加多元

- 但数据源拒绝给我们结构化的数据查询方式或API

---

## 爬虫能干什么？

## 爬虫的典型案例：搜索引擎

搜索引擎是Web时代用户使用互联网的入口和指南。

网络爬虫是搜索引擎系统中十分重要的组成部分，它负责从互联网中搜集网页，采集信息，这些网页信息用于建立索引从而为搜索引擎提供支持，它决定着整个引擎系统的内容是否丰富，信息是否即时，因此其性能的优劣直接影响着搜索引擎的效果。

```{r out.width = "50%", echo = FALSE}
knitr::include_graphics("https://piaosanlang.gitbooks.io/spiders/content/photos/01-engine.png")
```

---

## 网络基础知识

- 网络访问是个什么样的过程

- URI和URL：我们想要的东西存在哪里？

- Hypertext：我们想要的东西以什么方式存在

- HTTP和HTTPS：我们想要的东西以什么样的方式传输

- HTTP请求过程：我们如何告诉别人我们想要什么

---

## 网络访问是个什么样的过程

> “网站是把个人计算机连上网络的过程，爬虫就是通过网络到别人计算机下载数据”

爬虫是模拟用户在浏览器或者某个应用上的操作，把操作的过程、实现自动化的程序。
当我们在浏览器中输入一个url后回车，后台会发生什么？

简单来说这段过程发生了以下四个步骤：

- 查找域名对应的IP地址。

- 向IP对应的服务器发送请求。

- 服务器响应请求，发回网页内容。

- 浏览器解析网页内容。

---

## 网络访问是个什么样的过程

```{r out.width = "90%", echo = FALSE}
knitr::include_graphics("https://piaosanlang.gitbooks.io/spiders/content/photos/01-webdns.jpg")
```

---

## URI和URL：我们想要的东西存在哪里？

Universar Resource Locator(URL)，统一资源定位符

Uniform Resource Identifier(URI)，统一资源标志符

--

URI = URN + URL

--

URN:只命名资源而不指定如何定位资源，比如:

isbn:1203102348

它只是指定了一本书的ISBN，可以唯一标识这本书，但是没有指定到哪里定位这本书

---

## URL

> https://www.baidu.com/

> https://github.com/syfyufei/Rworkshop2022/blob/main/WebCrawler/WebCrawler.html

> http://166.111.105.29:8787/


url的基本格式：

- schema://host[:port#]/path/…/

- schema： 协议（例如：http，https，ftp）

- host：服务器的IP或域名

- port：服务器的端口

- path：访问资源的路径

---

## Hypertext：我们想要的东西以什么方式存在

超文字（Hypertext）：浏览器里看到的网页就是Hypertext解析而成的，其网页源代码是一系列的HTML代码，里面包含了一系列的标签，比如：

- img 标签显示图片
- p 标签指定显示段落
- a标签指定链接

--

.navy[学会看源码啦，下次遇到某些网页禁止复制网页内容，你想到了解决办法吗？]

https://blog.csdn.net/apsw6825/article/details/123993027?spm=1001.2100.3001.7377&utm_medium=distribute.pc_feed_blog_category.none-task-blog-classify_tag-5-123993027-null-null.nonecase&depth_1-utm_source=distribute.pc_feed_blog_category.none-task-blog-classify_tag-5-123993027-null-null.nonecase

---

## HTTP和HTTPS：我们想要的东西以什么样的方式传输

文本传输协议：

- http

- https

- ftp

- sftp


HTTP（Hyper Text Transfer Protocol)超文本传输协议，用于从网络传输超文本数据到本地浏览器的传输协议，能保证高效而准确的传送超文本文档。由万维网协会（World Wide Web Consortium）和Internet工作小组IETF（Internet Engineering Task Force）共同合作和制定的规范。

HTTPS（Hyper Test Transfer protocol over .red[Secure Socket Layer]）是以安全为目标的HTTP通道，简单讲是HTTP的安全版，即HTTP下加入SSL层，简称为HTTPS，安全基础是SSL，因此通过它传输的内容都是经过SSL加密。

---

## 请求和响应：我们如何告诉别人我们想要什么

在浏览器中输入一个url，回车之后便可以在浏览器中观察到页面内容，这个过程是浏览器向网站所在的服务器发送了一个请求，网站的服务器接收到这个请求后进行处理和解析，然后将对应的响应传回给浏览器。

- General(总览)

- Response Headers（响应头）

- **Request Headers（请求头）**

---

## 请求

- 请求方法（Request Method)

- 请求的网址（Request URL）

- 请求头 （Request Headers)

---

## 请求方法

GET:

在浏览器中直接输入URL并回车，便发起了一个GET请求，请求的参数会包含在URL中


POST:

POST请求大多数在表单提交时发起，例如：对于一个登录表单，输入用户名和密码后，点击“登录”按钮，这通常会发起一个POST请求，起数据通常以表单的形式传输，而不会体现在URL中。


GET和POST的区别：

- GET请求中的参数包含在URL中，数据可以在URL中看到，而POST请求的URL不会包含这些数据，数据都是通过表单的形式传输的，会包含在请求体中

- GET请求提交的数据最多只有1024字节，而POST请求没有限制

---

## 请求的网址

请求的网址：即统一资源定位符URL，可以唯一确定我们想要请求的资源


## 请求头

用来说明服务器要使用的附加信息，比较重要的信息有Cookie、User-Agent

Cookie：也常用复数形式Cookies，这是网站为了辨别用户进行会话跟踪而存储在本地的数据，它的主要功能是维持当前访问会话。例如：我们输入用户名和密码成功登陆到某个网站后，服务器会用会话保存登陆状态信息，后面我们每次刷新或请求该站点的其它页面时，会发现都是登陆状态，这就是Cookie的功劳。Cookies里面有信息标识了我们所对应的服务器的会话，每次在请求该站点时，都会在请求头长加上Cookies并将其发送给服务器，服务器通过Cookies识别出是我们自己，并且查出当前状态是登陆状态，所以返回结果就是登陆之后才能看到的网页内容。

User-Agent：简称UA，它是一个特殊的字符串头，可以使服务器识别客户使用的操作系统及版本、浏览器及版本信息等。在做爬虫是加上此信息，可以将爬虫伪装成浏览器，如果不加，将会是默认的爬虫的UA，很有可能会被识别为爬虫。

---

## 响应

响应状态码表示服务器的响应状态：

200	正常

301	本网页永久性转移达到另一个地址

400	请求出现语法错误

403	客户端未能获得授权

404	在指定位置不存在所申请的资源

500	服务器遇到了意料不到的情况

503	服务器由于维护或者负载过重未能应答

---

class: inverse, center, middle

# 利刃出鞘：我们的爬虫工具箱

---

## 利刃出鞘：我们的爬虫工具箱

- 零代码爬虫工具：“八爪鱼”

- 编程语言们

- 网络抓包工具

---
class: inverse, center, middle

# 小试牛刀：如何使用“八爪鱼”抓取政府网页

---

## 如何解析一个爬虫项目

- 人怎么操作？

对于网络爬虫，我们的目标是模仿人类用户访问网站的行为。在实际操作中，人类用户会通过浏览器浏览网页、点击链接、填写表单等方式与网站交互。因此，在设计网络爬虫时，我们需要模拟这些行为，使我们的爬虫更像一个真实用户。

> “混在群众队伍中才是最安全的”

这句话意味着，让爬虫尽量模拟正常用户的行为，这样就不容易被网站检测到。

---

- 模仿人

为了让爬虫更像一个真实用户，我们需要注意以下几点：

  - 设置User-Agent：User-Agent是HTTP请求头中的一个字段，它告诉服务器发起请求的客户端信息。设置合适的User-Agent可以让爬虫看起来像一个正常浏览器。
  
  - 处理JavaScript：许多网站使用JavaScript动态加载内容，因此爬虫需要能够执行JavaScript。可以使用像Selenium这样的工具实现。
  
  - 限制爬取速度：避免在短时间内发送大量请求，这样可以减轻目标服务器的负担，并降低被检测到的风险。可以通过设置延迟或使用代理IP轮换实现。

---

- 有什么反爬机制？

网站可能会采取以下一些反爬机制：

  - 检测User-Agent：如果User-Agent看起来不像一个正常浏览器，服务器可能会拒绝请求。
  
  - 检测请求速度：如果在短时间内接收到大量请求，服务器可能会认为这是一个爬虫。
  
  - 验证码：一些网站可能会要求用户输入验证码以确认其身份。
  
  - 使用Cookie和Session：网站可能会使用Cookie和Session来跟踪用户，并阻止没有合法Cookie或Session的请求。

---

- 我们应该如何突破这些反爬机制？

为了突破这些反爬机制，我们可以采取以下措施：

  - 设置合适的User-Agent：使用常见浏览器的User-Agent，或者使用库（如fake_useragent）来生成随机User-Agent。
  
  - 限制爬取速度：设置请求之间的延迟，或者使用代理IP轮换。
  
  - 处理验证码：对于简单的验证码，可以使用OCR（光学字符识别）技术识别。对于复杂的验证码，可以考虑使用第三方验证码识别服务。
  
  - 管理Cookie和Session：使用库（如requests）来自动处理Cookie和Session。

---

- 有没有好方法？

有一些更高级的方法可以提高爬虫的效果和效率：

  - 使用分布式爬虫：通过多台计算机或多个线程同时运行爬虫，可以提高爬取速度。但要注意遵守目标网站的爬虫政策，避免给服务器带来过大负担。
  
  - 使用代理池：维护一个代理IP池，轮换使用代理IP来发起请求，可以降低被检测到的风险。
  
  - 使用机器学习技术：对于复杂的网站结构，可以使用机器学习技术（如自然语言处理、计算机视觉等）来提取所需的信息。
  
  - 使用API：许多网站提供API接口，可以直接通过API获取数据。这比爬取网页更高效，也更容易维护。但要注意遵守API使用规定，避免触发API限制。

总之，要建立一个高效且稳定的爬虫，需要根据目标网站的特点和反爬策略采取相应的方法。同时，始终要遵守网站的爬虫政策和法律法规，尊重网站的版权和隐私。



---

## 小试牛刀：如何使用“八爪鱼”抓取数据

---

## 小试牛刀：如何使用R抓取政府网页

- 构造URLs池子

- 循环进行

> http://sousuo.gov.cn/column/31421/0.htm?

---

## 构造URLs池子

```{r}

# 安装必要的包
if (!requireNamespace("rvest", quietly = TRUE)) {
  install.packages("rvest")
}
if (!requireNamespace("tidyverse", quietly = TRUE)) {
  install.packages("tidyverse")
}

# 加载库
library(rvest)
library(tidyverse)

# 初始化空的数据框
news_data <- tibble(title = character(), content = character())

# 定义爬取的总页数
total_pages <- 1979

# 循环爬取每一页的新闻
for (page in 0:total_pages) {
  # 构建每一页的网址
  url <- paste0("http://sousuo.gov.cn/column/31421/", page, ".htm?")

  # 读取页面内容
  page_content <- read_html(url)

  # 提取新闻链接
  news_links <- page_content %>% html_nodes(".listTxt a") %>% html_attr("href")

  # 循环爬取每一个新闻链接
  for (link in news_links) {
    # 读取新闻页面
    news_page <- read_html(link)

    # 提取新闻标题和内容
    title <- news_page %>% html_nodes("h1") %>% html_text(trim = TRUE)
    content <- news_page %>% html_nodes("#UCAP-CONTENT") %>% html_text(trim = TRUE)

    # 将新闻数据添加到数据框中
    news_data <- news_data %>% add_row(title = title, content = content)
  }

  # 输出进度
  cat("已完成第", page + 1, "页，共", total_pages + 1, "页\n")
}

# 输出爬取到的新闻数据
print(news_data)

```



```{r, eval = FALSE}

# http://selectorgadget.com/

urls<- data.frame(1:141)
names(urls) <- "url"
for (n in 1:141) {
  urls$url[n] <- paste0("http://www.ngd.org.cn/xwzx/ywdt/index", n, ".htm")
  print(n)
}
dpnews_CPAWDP <- data.frame()

for (n in 1:length(urls$url)) {
  partlink <- read_html(urls$url[n]) %>% 
  html_nodes(".gp-ellipsis a") %>% 
  html_attrs()
  linksP1 <- data.frame(unlist(partlink))
  dpnews_CPAWDP <- rbind(dpnews_CPAWDP, linksP1)
  print(n)
}

dpnews_CPAWDP$url <- paste0("http://www.ngd.org.cn/xwzx/ywdt/", dpnews_CPAWDP$unlist.partlink.)

for (b in 1:length(dpnews_CPAWDP$url)) {
  html <- dpnews_CPAWDP$url[b]
  dpnews_CPAWDP$title[b] <- read_html(html) %>% 
             html_nodes("#shareTitle") %>% 
             html_text()
  dpnews_CPAWDP$author[b] <- read_html(html) %>% 
             html_nodes(".articleAuthor span:nth-child(1)") %>% 
             html_text()
  dpnews_CPAWDP$author[b] <- read_html(html) %>% 
             html_nodes(".articleAuthor span:nth-child(5)") %>% 
             html_text()
  dpnews_CPAWDP$content[b] <- read_html(html) %>% 
             html_nodes("span") %>% 
             html_text()
  print(b)
}

https://www.gov.mo/zh-hant/news-search/page/100/?category_id&entity_id&start_date&end_date

read_html("https://www.gov.mo/zh-hant/news-search/page/100/?category_id&entity_id&start_date&end_date") %>% 
  html_nodes(".links a") %>% 
  html_attrs()

```

```{r}
temp <- read_html(paste0("https://api.isuresults.eu/events/?page=", 1)) %>% 
  html_nodes("p") %>% 
  html_text()
url1 <- jsonlite::fromJSON(temp, simplifyVector = TRUE)[[4]]

for(a in 2:5){
  temp <- read_html(paste0("https://api.isuresults.eu/events/?page=", a)) %>% 
  html_nodes("p") %>% 
  html_text()
  url <- jsonlite::fromJSON(temp, simplifyVector = TRUE)[[4]]
  url1 <- bind_rows(url1, url)
  print(a)
}

data <- as.data.frame(url1$scheduleUrl)

write_csv2(data, "/Users/sunyufei/Desktop/temp.csv")

data <- read_csv("/Users/sunyufei/Desktop/temp.csv")

names(data) <- "scheduleUrl"

urls <- read_csv("/Users/sunyufei/Desktop/ISU Speed Skating Results - Live.csv")



for (b in 1:urls$colxs2_链接1) {
  url <- read_html(urls$colxs2_链接1[b]) %>% 
    html_nodes(".row-button:nth-child(2) .btn-skating") %>% 
    html_attr()
}

url1$scheduleUrl[1]

names(jsonlite::fromJSON(temp, simplifyVector = TRUE))
url2 <- jsonlite::fromJSON(temp, simplifyVector = TRUE)[[5]][[2]]

url <- paste0(url2$url[1], "results")

data <- data_frame(data$links)

data$`data$links`

data1 <- read.csv("/Users/sunyufei/Desktop/ISUSpeedSkatingResultsData1.csv")
data2 <- read.csv("/Users/sunyufei/Desktop/ISUSpeedSkatingResultsData2.csv")
data3 <- read.csv("/Users/sunyufei/Desktop/ISUSpeedSkatingResultsData3.csv")
data4 <- read.csv("/Users/sunyufei/Desktop/ISUSpeedSkatingResultsData4.csv")
data5 <- read.csv("/Users/sunyufei/Desktop/ISUSpeedSkatingResultsData5.csv")

data <- rbind(data1, data2, data3, data4, data5)

write.csv(data, "/Users/sunyufei/Desktop/ISUSpeedSkatingResultsData.csv")




```


```{r}

data <- readxl::read_excel("/Users/sunyufei/Desktop/ISUSpeedSkatingResultsData.xlsx") %>% 
  dplyr::filter(nchar(nationality) > 1)

for (a in 1:length(data$文本)) {
  data$event[a] <- unlist(strsplit(data$文本[a], "Referee", fixed= T))[1]
  print(a)
}

for (b in 1:length(data$文本)) {
  data$name[b] <- unlist(strsplit(data$字段[b], "\r"))[1]
  print(b)
}

for (c in 1:length(data$文本)) {
  data$age[c] <- unlist(strsplit(data$字段4[c], " | "))[2]
  print(c)
}

for (d in 1:length(data$文本)) {
  data$gender[d] <- unlist(strsplit(data$字段4[d], " | "))[4]
  print(d)
}

```


```{r}

library(rvest)

read_html("https://www2.ed.gov/policy/landing.jhtml?src=pn") %>% 
  html_nodes("#featuretable a") %>% 
  html_attrs()


```
---

## 大试牛刀：如何爬取微信公众号

## 项目解析：困难在哪？

- 微信公众号历史的所有文章（在封闭的系统内如何构造IP池子）

--

-如何获得永久链接

--

- 在网页打开是看不到评论的啊喂


---

## 网络抓包

互动环节：什么是抓包？

--

抓包就是将网络传输发送与接收的数据包进行截获、重发、编辑、转存等操作

```{r out.width = "60%", echo = FALSE}
knitr::include_graphics("https://pic1.zhimg.com/80/v2-dbea07ea287aee9383bf51fb65f82524_720w.jpg")
```

---

class: inverse, center, middle

# 🦀️蟹蟹🦀️

📧[sunyf20@mails.tsinghua.edu.cn](mailto:sunyf20@mails.tsinghua.edu.cn) 

🧑‍💻[Github: syfyufei](https://syfyufei.github.io/)

💻[Yufei Sun: github.com/syfyufei](https://github.com/syfyufei)



## 为什么要爬虫

- 社会科学研究需要的数据更加多元

::: {.callout-note}

# 问题时间到🙋

你能想到你的研究领域能够用到哪些需要爬取的数据？

:::

- 但数据源拒绝给我们结构化的数据查询方式或API

::: {.callout-note}

## API

你能想到回归分析的几个基本假设？

:::

---

## 爬虫能干什么？

## 爬虫的典型案例：搜索引擎

搜索引擎是Web时代用户使用互联网的入口和指南。

网络爬虫是搜索引擎系统中十分重要的组成部分，它负责从互联网中搜集网页，采集信息，这些网页信息用于建立索引从而为搜索引擎提供支持，它决定着整个引擎系统的内容是否丰富，信息是否即时，因此其性能的优劣直接影响着搜索引擎的效果。

```{r out.width = "50%", echo = FALSE}
knitr::include_graphics("https://piaosanlang.gitbooks.io/spiders/content/photos/01-engine.png")
```

---

## 网络基础知识

- 网络访问是个什么样的过程

- URI和URL：我们想要的东西存在哪里？

- Hypertext：我们想要的东西以什么方式存在

- HTTP和HTTPS：我们想要的东西以什么样的方式传输

- HTTP请求过程：我们如何告诉别人我们想要什么

---

## 网络访问是个什么样的过程

> “网站是把个人计算机连上网络的过程，爬虫就是通过网络到别人计算机下载数据”

爬虫是模拟用户在浏览器或者某个应用上的操作，把操作的过程、实现自动化的程序。
当我们在浏览器中输入一个url后回车，后台会发生什么？

简单来说这段过程发生了以下四个步骤：

- 查找域名对应的IP地址。

- 向IP对应的服务器发送请求。

- 服务器响应请求，发回网页内容。

- 浏览器解析网页内容。

---

## 网络访问是个什么样的过程

```{r out.width = "90%", echo = FALSE}
knitr::include_graphics("https://piaosanlang.gitbooks.io/spiders/content/photos/01-webdns.jpg")
```

---

## URI和URL：我们想要的东西存在哪里？

Universar Resource Locator(URL)，统一资源定位符

Uniform Resource Identifier(URI)，统一资源标志符

--

URI = URN + URL

--

URN:只命名资源而不指定如何定位资源，比如:

isbn:1203102348

它只是指定了一本书的ISBN，可以唯一标识这本书，但是没有指定到哪里定位这本书

---

## URL

> https://www.baidu.com/

> https://github.com/syfyufei/Rworkshop2022/blob/main/WebCrawler/WebCrawler.html

> http://166.111.105.29:8787/


url的基本格式：

- schema://host[:port#]/path/…/

- schema： 协议（例如：http，https，ftp）

- host：服务器的IP或域名

- port：服务器的端口

- path：访问资源的路径

---

## Hypertext：我们想要的东西以什么方式存在

超文字（Hypertext）：浏览器里看到的网页就是Hypertext解析而成的，其网页源代码是一系列的HTML代码，里面包含了一系列的标签，比如：

- img 标签显示图片
- p 标签指定显示段落
- a标签指定链接

--

.navy[学会看源码啦，下次遇到某些网页禁止复制网页内容，你想到了解决办法吗？]

https://blog.csdn.net/apsw6825/article/details/123993027?spm=1001.2100.3001.7377&utm_medium=distribute.pc_feed_blog_category.none-task-blog-classify_tag-5-123993027-null-null.nonecase&depth_1-utm_source=distribute.pc_feed_blog_category.none-task-blog-classify_tag-5-123993027-null-null.nonecase

---

## HTTP和HTTPS：我们想要的东西以什么样的方式传输

文本传输协议：

- http

- https

- ftp

- sftp


HTTP（Hyper Text Transfer Protocol)超文本传输协议，用于从网络传输超文本数据到本地浏览器的传输协议，能保证高效而准确的传送超文本文档。由万维网协会（World Wide Web Consortium）和Internet工作小组IETF（Internet Engineering Task Force）共同合作和制定的规范。

HTTPS（Hyper Test Transfer protocol over .red[Secure Socket Layer]）是以安全为目标的HTTP通道，简单讲是HTTP的安全版，即HTTP下加入SSL层，简称为HTTPS，安全基础是SSL，因此通过它传输的内容都是经过SSL加密。

---

## 请求和响应：我们如何告诉别人我们想要什么

在浏览器中输入一个url，回车之后便可以在浏览器中观察到页面内容，这个过程是浏览器向网站所在的服务器发送了一个请求，网站的服务器接收到这个请求后进行处理和解析，然后将对应的响应传回给浏览器。

- General(总览)

- Response Headers（响应头）

- **Request Headers（请求头）**

---

## 请求

- 请求方法（Request Method)

- 请求的网址（Request URL）

- 请求头 （Request Headers)

---

## 请求方法

GET:

在浏览器中直接输入URL并回车，便发起了一个GET请求，请求的参数会包含在URL中


POST:

POST请求大多数在表单提交时发起，例如：对于一个登录表单，输入用户名和密码后，点击“登录”按钮，这通常会发起一个POST请求，起数据通常以表单的形式传输，而不会体现在URL中。


GET和POST的区别：

- GET请求中的参数包含在URL中，数据可以在URL中看到，而POST请求的URL不会包含这些数据，数据都是通过表单的形式传输的，会包含在请求体中

- GET请求提交的数据最多只有1024字节，而POST请求没有限制

---

## 请求的网址

请求的网址：即统一资源定位符URL，可以唯一确定我们想要请求的资源


## 请求头

用来说明服务器要使用的附加信息，比较重要的信息有Cookie、User-Agent

Cookie：也常用复数形式Cookies，这是网站为了辨别用户进行会话跟踪而存储在本地的数据，它的主要功能是维持当前访问会话。例如：我们输入用户名和密码成功登陆到某个网站后，服务器会用会话保存登陆状态信息，后面我们每次刷新或请求该站点的其它页面时，会发现都是登陆状态，这就是Cookie的功劳。Cookies里面有信息标识了我们所对应的服务器的会话，每次在请求该站点时，都会在请求头长加上Cookies并将其发送给服务器，服务器通过Cookies识别出是我们自己，并且查出当前状态是登陆状态，所以返回结果就是登陆之后才能看到的网页内容。

User-Agent：简称UA，它是一个特殊的字符串头，可以使服务器识别客户使用的操作系统及版本、浏览器及版本信息等。在做爬虫是加上此信息，可以将爬虫伪装成浏览器，如果不加，将会是默认的爬虫的UA，很有可能会被识别为爬虫。

---

## 响应

响应状态码表示服务器的响应状态：

200	正常

301	本网页永久性转移达到另一个地址

400	请求出现语法错误

403	客户端未能获得授权

404	在指定位置不存在所申请的资源

500	服务器遇到了意料不到的情况

503	服务器由于维护或者负载过重未能应答

要做文本分析，首先得有文本，才能进行分析。
我们在上一章说到，目前对于社会科学中的计算文本分析来说，大多数一手数据都是来自于互联网产生的数据，所以在开始具体的技术部分之前，本书首先讲解如何从网页获取用于数据。

## 什么是网页抓取

网页抓取（Web Scraping）是指从互联网上获取信息的过程，通常通过自动程序（称为爬虫或蜘蛛）访问网站并提取所需的数据。
这些数据可以是文本、图像、链接等，用于各种目的，如分析、研究、展示或其他应用。

网页抓取的基本步骤包括：

1. 发现网页： 爬虫首先需要知道要抓取的网页的地址。这可以是一个单一的页面或整个网站。
1. 发送**请求**： 爬虫通过HTTP或HTTPS协议向目标服务器发送请求，请求页面的内容。
1. **下载**网页的HTML： 服务器接收到请求后，会返回相应的HTML页面。爬虫获取这个HTML页面的内容。
1. **解析**该HTML： 爬虫使用解析器（如Beautiful Soup、Scrapy等）来解析HTML，提取所需的数据。这可能涉及到识别标签、类、ID等HTML元素。
1. **提取**数据： 提取的数据通常被存储在数据结构中，如数据库、CSV文件或其他格式，以备后续分析或使用。

网页抓取的应用非常广泛，可以用于搜索引擎索引、数据挖掘、舆情分析、价格比较、自动化测试等领域。


## 使用R进行网页抓取的工具

在R中进行网页抓取常用的有三个工具： `rvest`，`RSelenium` 和 `Rcrawler`，
它们被设计用于不同类型的任务。

:::{.callout-tip}
**但是**，如果你发现你在爬取数据时较多地用到了`RSelenium` 和 `Rcrawler`，说明你现在进行的工作是比较复杂的，最好直接找第三方如八爪鱼等帮忙，或者直接通过API获取。
因为在研究工作中，获取数据其实是一个比较费力但是又没有任何实际能够写到纸面上的产出的事情，建议大家最好将时间留给后面的分析和建模过程。

当然，我们还是要对爬取数据的基本技术有一个了解和掌握。
:::

### [`rvest`](https://rvest.tidyverse.org/)

首先是`rvest`包，它是本章主要讲解的一个抓取数据的包，主要功能就是使我们能够轻松收割网页。
`rvest`结合了Python的 `BeautifulSoup` 和 `RoboBrowser`功能，使用CSS选择器（稍后详细说明），也可以与[`polite`](https://dmi3kno.github.io/polite/)兼容，“礼貌”地抓取多个页面。 


### [`RSelenium`](https://docs.ropensci.org/RSelenium/index.html)

`RSelenium`是R版本的 `Selenium` (Python)，它是一套专门用于Selenium 2.0的Selenium Remote WebDriver的R绑定，使用自动化浏览器、本地或远程进行操作，就像真实用户手动导航它们一样。

### [`Rcrawler`](https://github.com/salimk/Rcrawler)

`Rcrawler`可以自动遍历和解析网站的所有网页，并且通过单个命令一次性提取所有数据，也可以对数据进行爬取、检索和解析。
它的工作方式是探索网站的HTML结构，包括内部和外部链接。

像我们之前说的那样，`RSelenium`和`Rcrawler`是比较进阶的方法，有他们针对的独特的问题，尤其是对于一些复杂的网站，比如java类网站，需要用到他们去抓取。



### 使用API抓取网页

在可能的情况下，我们推荐尽量使用使用API获取数据，因为它通常会给你提供更可靠的数据，现在百度、高德、推特（X）、新浪等大网站已经有API可以去获取数据了，甚至，**许多流行的网站API已经有了R包！**
换句话说，你可以立即在R中使用它！

另外，我们也可以使用像 `httr`和`jsonlite`这样的包，向在线抓取工具（online scrapper） (例如， [Browserbear](https://api.browserbear.com/)， [八爪鱼](https://openapi.bazhuayu.com/zh-CN/))直接发出任务请求，方便快捷！

例如， [这里](https://www.browserbear.com/blog/web-scraping-with-r-an-introduction/)是这种方法的一个介绍。

换句话说，如果你发现你需要爬取的网站用本章讲的方法爬不下来，可以直接向外界求助。


在接下来的教程中，我们将主要关注`rvest`。
这些内容从Hadley Wickham、Mine Çetinkaya-Rundel和Garrett Grolemund的书籍，[*R for Data Science: Import, Tidy, Transform, Visualize, and Model Data*, 2nd edition, O'Reilly Media 2023](https://www.amazon.com/Data-Science-Transform-Visualize-Model/dp/1492097403)中获得了重要灵感，这是一个开发获取的书，大家有需要也可以去查看。

### 使用`rvest`进行网页抓取

在R中爬取数据使用最多的就是`rvest`，可以通过以下代码安装并调用。

```{r install, eval=FALSE}
if (!require(rvest)) install.packages("rvest")
library("rvest")
```

### 抓取过程

网页抓取是一个**模拟用户**在浏览器或应用程序上的行为以自动化该过程的过程：

什么意思呢？
我们浏览网页的时候，看似只是鼠标轻轻一点的过程，但是其实这“一点”背后计算机已经完成了至少五个步骤的工作：

1. 输入URL，比如www.amazon.com；
1. 把上述网址输入解析服务器DNS Server,查找与URL域名对应的IP地址；
1. 向对应IP地址的服务器，一般是HTTP或者HTTPS发送**请求**（request）；
1. 服务器**响应**（response）请求并返回网页内容；
1. 用户电脑上的浏览器**解析**网页内容。

![](images/scrap_dns.jpg)

也就是说，在浏览网页时，你作为一个中转站是连接了两方的，它们中无论哪个没有连接上，最后的数据都不会拿下来。
如果前面DNS服务器没连接上的话，就会出一个代码，比如最著名的404，说你这个网页打不开。
所以关于网页能不能打开这件事一般关系到DNS Server，所以有时候如果我们用R或者是用python去打开网站的时候，发现我们在浏览器上打得开，但你用R去打它就打不开了，因为R和python是机器发送请求，这个时候可能就要做一些认证。

### URL 

我们这里给大家简单讲解一下url，也就是链接，是怎么组成的。
url基本上分成下图这几个部分：

- `schema`: 例如，http，https，ftp，sftp
- `host`: 注册名（包括但不限于主机名例如www）或IP地址（一般是一串数字，如示例3）
- `port`: 如果是IP地址则需要给出port，也就是示例3`:`后面的数字59837
- `path`: 由斜杠（/）分隔的路径段序列

> 示例1：https://www.google.com/       
> 示例2：https://sammo3182.github.io     
> 示例3：http://166.111.105.34:59837/

![](images/scrap_url.png)
